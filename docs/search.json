[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Salvatore Davi’s Website",
    "section": "",
    "text": "Last Updated: Tuesday 02 11, 2025 at 15:02PM"
  },
  {
    "objectID": "index.html#current-graduate-student-studying-quantitative-methods-modeling-baruch-college.",
    "href": "index.html#current-graduate-student-studying-quantitative-methods-modeling-baruch-college.",
    "title": "Salvatore Davi’s Website",
    "section": "",
    "text": "Last Updated: Tuesday 02 11, 2025 at 15:02PM"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Analysis on NYC Payroll Optimization",
    "section": "",
    "text": "This report was comissioned by the Commission to Analyze Taxpayer spending. As an incoming technical analyst, I was appointed to investigate several areas of interest that pertained to the way the city handles it’s payroll bill. The following analysis seeks to uncover interesting trends that will aide city stakeholders in their decision making process on how to handle it’s finances. This is followed by several recommended policies that the city wants to undertake, and a third policy underwritten by the team on how New York City can get it’s spending under control.\nEric L. Adams is the current New York City mayor until 2025. Below, we examine his journey from being a borough president in FY 2014 until his ascendancy to become the Mayor of New York City in 2023. His current salary for being Mayor was $258,720 in 2024.\n\n\n\nMayor Adams Salary by Fiscal Year\n\n\nFiscal Year\nTotal Salary\nPosition\nAgency\n\n\n\n\n2014\n160000\nBorough President\nBorough President-Brooklyn\n\n\n2015\n160000\nBorough President\nBorough President-Brooklyn\n\n\n2016\n179200\nBorough President\nBorough President-Brooklyn\n\n\n2017\n179200\nBorough President\nBorough President-Brooklyn\n\n\n2018\n179200\nBorough President\nBorough President-Brooklyn\n\n\n2019\n179200\nBorough President\nBorough President-Brooklyn\n\n\n2020\n179200\nBorough President\nBorough President-Brooklyn\n\n\n2021\n179200\nBorough President\nBorough President-Brooklyn\n\n\n2022\n437950\nBorough President\nBorough President-Brooklyn\n\n\n2023\n258750\nMayor\nOffice Of The Mayor\n\n\n2024\n258750\nMayor\nOffice Of The Mayor"
  },
  {
    "objectID": "docs/mp01.html",
    "href": "docs/mp01.html",
    "title": "Untitled",
    "section": "",
    "text": "Reads a CSV file into a data frame\nnyc_payroll &lt;- read.csv(“data/mp01/nyc_payroll_export.csv”, header = TRUE, # Indicates that the first row contains column names stringsAsFactors = FALSE) # Keeps character columns as character type\nhead(nyc_payroll)\nlibrary(readr) library(dplyr) library(stringr) nyc_payroll &lt;- read.csv(“data/mp01/nyc_payroll_export.csv”) %&gt;% mutate( agency_name = str_to_title(agency_name), last_name = str_to_title(last_name), first_name = str_to_title(first_name), work_location_borough = str_to_title(‘work_location_borough’), title_description = str_to_title(title_description), leave_status_as_of_june_30 = str_to_title(leave_status_as_of_june_30) )\nhead(nyc_payroll)"
  },
  {
    "objectID": "docs/docs/mp01.html",
    "href": "docs/docs/mp01.html",
    "title": "Untitled",
    "section": "",
    "text": "Reads a CSV file into a data frame\nnyc_payroll &lt;- read.csv(“data/mp01/nyc_payroll_export.csv”, header = TRUE, # Indicates that the first row contains column names stringsAsFactors = FALSE) # Keeps character columns as character type\nhead(nyc_payroll)\nlibrary(readr) library(dplyr) library(stringr) nyc_payroll &lt;- read.csv(“data/mp01/nyc_payroll_export.csv”) %&gt;% mutate( agency_name = str_to_title(agency_name), last_name = str_to_title(last_name), first_name = str_to_title(first_name), work_location_borough = str_to_title(‘work_location_borough’), title_description = str_to_title(title_description), leave_status_as_of_june_30 = str_to_title(leave_status_as_of_june_30) )\nhead(nyc_payroll)\nglimpse(nyc_payroll)"
  },
  {
    "objectID": "mp01.html#policy-analysis",
    "href": "mp01.html#policy-analysis",
    "title": "Analysis on NYC Payroll",
    "section": "Policy Analysis",
    "text": "Policy Analysis"
  },
  {
    "objectID": "mp01.html#which-individual-worked-the-most-overtime-hours-in-this-data-set",
    "href": "mp01.html#which-individual-worked-the-most-overtime-hours-in-this-data-set",
    "title": "Analysis on NYC Payroll",
    "section": "",
    "text": "Show the code\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(kableExtra)\n\ntop_individual &lt;- nyc_payroll |&gt;\n  group_by(first_name, last_name, mid_init) |&gt;\n  summarize(\n    total_ot_hours = sum(ot_hours, na.rm = TRUE),\n    Position = first(title_description),\n    Leave_Status = first(leave_status_as_of_june_30),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(total_ot_hours)) |&gt;\n  slice(1:3)\n\nkable(top_individual, \n      caption = \"Top 3 Individuals with the Most Overtime Hours\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nTop 3 Individuals with the Most Overtime Hours\n\n\nfirst_name\nlast_name\nmid_init\ntotal_ot_hours\nPosition\nLeave_Status\n\n\n\n\nNA\nNA\nNA\n1878065.72\nRackets Investigator - Start &gt;4-24-08 No Abc\nCeased\n\n\nMd\nIslam\nS\n59436.11\nComputer Systems Manager\nCeased\n\n\nDavid\nRodriguez\nNA\n52421.33\nElection Worker\nActive\n\n\n\n\n\n\n\nWe next wanted to investigate which individuals have worked the most overtime hours. The view above produces the top 3 indiviuals with the most overtime hours throughout the entire dataset. Although we could have taken the average or grouped by year, the sum was the appropiate metric here as we wanted to measure how high employees have been able to clock in the amount of overtime hours this past decade. Interestingly, Islam, Md, has been able to recieve almost 60,000 hours, although now is not longer am employee. The highest employee with the most overtime hours is Rodriguez, David, an Election Worker, with 52,421 total overtime hours.\nOvertime is a particular area of interest, as hourly wages can become more expensive than salary, leaving room for inflated wages."
  },
  {
    "objectID": "mp01.html#which-agency-has-the-highest-average-total-annual-payroll-base-and-overtime-pay-per-employee",
    "href": "mp01.html#which-agency-has-the-highest-average-total-annual-payroll-base-and-overtime-pay-per-employee",
    "title": "Analysis on NYC Payroll",
    "section": "",
    "text": "The agency with the highest average total annual payroll is the Office Of Collective Bargaining. With 204 employees, they have an average gross total of 105,562.\n\n\nShow the code\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\n\nagency_avg_gross &lt;- nyc_payroll |&gt;\n  # Calculate total annual gross payroll for each employee\n  mutate(annual_gross_pay = regular_gross_paid + total_ot_paid + total_other_pay) |&gt;\n  # Aggregate data by agency name\n  group_by(agency_name) |&gt;\n  summarize(\n    avg_annual_gross = mean(annual_gross_pay, na.rm = TRUE),\n    total_employees = n(),\n    total_gross_pay = sum(annual_gross_pay, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(avg_annual_gross)) |&gt;\n  slice(1:5)\n\n# Optionally, format dollar values (if desired)\nagency_avg_gross &lt;- agency_avg_gross |&gt;\n  mutate(\n    avg_annual_gross = dollar(avg_annual_gross),\n    total_gross_pay = dollar(total_gross_pay)\n  )\n\nkable(agency_avg_gross, \n      caption = \"Top 5 Agencies by Average Annual Gross Payroll\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nTop 5 Agencies by Average Annual Gross Payroll\n\n\nagency_name\navg_annual_gross\ntotal_employees\ntotal_gross_pay\n\n\n\n\nOffice Of Collective Bargainin\n$105,563\n204\n$21,534,797\n\n\nFinancial Info Svcs Agency\n$105,437\n5121\n$539,941,389\n\n\nBronx Community Board #3\n$104,195\n21\n$2,188,093\n\n\nFire Department\n$100,285\n209272\n$20,986,772,536\n\n\nOffice Of The Actuary\n$98,543\n498\n$49,074,417"
  },
  {
    "objectID": "mp01.html#which-agency-has-the-most-employees-on-payroll-in-each-year",
    "href": "mp01.html#which-agency-has-the-most-employees-on-payroll-in-each-year",
    "title": "Analysis on NYC Payroll",
    "section": "",
    "text": "Show the code\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(ggplot2)\n\n# Calculate the top agency by employee count for each fiscal year\ntop_agency_each_year &lt;- nyc_payroll |&gt;\n  group_by(fiscal_year, agency_name) |&gt;\n  summarize(num_employees = n(), .groups = \"drop\") |&gt;\n  group_by(fiscal_year) |&gt;\n  slice_max(order_by = num_employees, n = 1, with_ties = FALSE) |&gt;\n  ungroup() |&gt;\n  arrange(desc(fiscal_year))\n\n# Display the table using kable\nkable(top_agency_each_year, \n      caption = \"Top Agency by Employee Count per Fiscal Year\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n# Create a bar graph for the same data\nggplot(top_agency_each_year, aes(x = factor(fiscal_year), y = num_employees, fill = agency_name)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Top Agency by Employee Count per Fiscal Year\",\n    x = \"Fiscal Year\",\n    y = \"Number of Employees\",\n    fill = \"Agency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe department of education contains the most amount of employees on payroll per year. The agency has topped over 100,000 employees the past decade."
  },
  {
    "objectID": "mp01.html#which-individual-in-what-year-had-the-single-highest-city-total-payroll-regular-and-overtime-combined",
    "href": "mp01.html#which-individual-in-what-year-had-the-single-highest-city-total-payroll-regular-and-overtime-combined",
    "title": "Analysis on NYC Payroll",
    "section": "",
    "text": "Show the code\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\n\nnyc_data &lt;- nyc_payroll |&gt;\n  mutate(gross_annual_pay = regular_gross_paid + total_ot_paid + total_other_pay)\n\nhighest_paid_employee &lt;-\n  nyc_data |&gt;\n  group_by(fiscal_year) |&gt;\n  slice_max(gross_annual_pay, n = 1) |&gt;\n  select(fiscal_year, title_description, agency_name, first_name, mid_init, last_name, gross_annual_pay) |&gt;\n  ungroup() |&gt;\n  slice_max(gross_annual_pay, n = 1)\n\n# Display the output as a nicely formatted table\nkable(highest_paid_employee, caption = \"Highest Paid Employee Overall\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nHighest Paid Employee Overall\n\n\nfiscal_year\ntitle_description\nagency_name\nfirst_name\nmid_init\nlast_name\ngross_annual_pay\n\n\n\n\n2024\nChief Marine Engineer\nDepartment Of Transportation\nMark\nK\nTettonis\n1689518\n\n\n\n\n\n\n\nIn 2024, MarkTettonis earned $1,689,518."
  },
  {
    "objectID": "mp01.html#which-job-title-has-the-highest-base-rate-of-pay",
    "href": "mp01.html#which-job-title-has-the-highest-base-rate-of-pay",
    "title": "Analysis on NYC Payroll",
    "section": "",
    "text": "Job Title with the Highest Average Total Pay (Representative Details)\n\n\nfirst_name\nmid_init\nlast_name\nagency_name\ntitle_description\ntotal_pay\n\n\n\n\nMarek\nM\nTyszkiewicz\nOffice Of The Actuary\nChief Actuary\n349723\n\n\n\n\n\n\n\nTo begin our analysis, we will look into which ‘job title’ contains the highest base rate of pay. Marek Tyszkiewicz, with the job title ‘Chief Actuary’, at the office of the actuary contains the highest total pay."
  },
  {
    "objectID": "mp01.html#which-agency-has-the-highest-overtime-usage-compared-to-regular-hours",
    "href": "mp01.html#which-agency-has-the-highest-overtime-usage-compared-to-regular-hours",
    "title": "Analysis on NYC Payroll",
    "section": "",
    "text": "The agency with the highest overtime usage is the NYPD.\n\n\nShow the code\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\n\nagency_ot_usage_absolute &lt;- nyc_payroll |&gt;\n  group_by(agency_name) |&gt;\n  summarize(\n    total_ot_hours = sum(ot_hours, na.rm = TRUE),\n    total_regular_hours = sum(regular_hours, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(total_ot_hours))\n\ntop_agency_ot_usage &lt;- agency_ot_usage_absolute |&gt;\n  slice(1)\n\nkable(top_agency_ot_usage, caption = \"Agency with the Highest Absolute Overtime Hours Usage:\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nAgency with the Highest Absolute Overtime Hours Usage:\n\n\nagency_name\ntotal_ot_hours\ntotal_regular_hours\n\n\n\n\nPolice Department\n149005878\n1145700945\n\n\n\n\n\n\n\nThe agency with the highest overtime usage is the NYPD. From 2014 to 2024 they have accrued 149 million total amount of overtime hours."
  },
  {
    "objectID": "mp01.html#what-is-the-average-salary-of-employees-who-work-outside-the-five-boroughs-that-is-whose-work_location_borough-is-not-one-of-the-five-counties.",
    "href": "mp01.html#what-is-the-average-salary-of-employees-who-work-outside-the-five-boroughs-that-is-whose-work_location_borough-is-not-one-of-the-five-counties.",
    "title": "NYC Payroll Policies",
    "section": "What is the average salary of employees who work outside the five boroughs? (That is, whose work_location_borough is not one of the five counties.)",
    "text": "What is the average salary of employees who work outside the five boroughs? (That is, whose work_location_borough is not one of the five counties.)\n\n\nShow the code\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\n\n# Use a different name for your data frame\nnyc_data &lt;- nyc_payroll\n\ntemp &lt;- nyc_data |&gt;\n  # Filter out rows where work_location_borough is one of the five boroughs or is NA\n  filter(!work_location_borough %in% c(\"Manhattan\", \"Queens\", \"Richmond\", \"Brooklyn\", \"Bronx\") &\n         !is.na(work_location_borough)) |&gt;\n  # Compute gross annual pay if not already computed\n  mutate(gross_annual_pay = regular_gross_paid + total_ot_paid + total_other_pay) |&gt;\n  group_by(work_location_borough) |&gt;\n  summarize(avg_salary = mean(gross_annual_pay, na.rm = TRUE), .groups = \"drop\") |&gt;\n  mutate(avg_salary = dollar(avg_salary))\n\nkable(temp |&gt; rename(\n  \"LOCATION OUTSIDE NYC\" = work_location_borough,\n  \"AVERAGE SALARY\" = avg_salary\n)) |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\nLOCATION OUTSIDE NYC\nAVERAGE SALARY\n\n\n\n\nWork_location_borough\n$50,041.10\n\n\n\n\n\n\n\nThe average salary of employees who work outside of the five boroughs is $50,041."
  },
  {
    "objectID": "mp01.html#how-much-has-the-citys-aggregate-payroll-grown-over-the-past-10-years",
    "href": "mp01.html#how-much-has-the-citys-aggregate-payroll-grown-over-the-past-10-years",
    "title": "Analysis on NYC Payroll",
    "section": "",
    "text": "Show the code\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(ggplot2)\n\nnyc_data &lt;- nyc_payroll |&gt; \n  mutate(gross_annual_pay = regular_gross_paid + total_ot_paid + total_other_pay)\n\naggregate_payroll &lt;- nyc_data |&gt;\n  group_by(fiscal_year) |&gt;\n  summarize(tot_payroll = sum(gross_annual_pay, na.rm = TRUE), .groups = \"drop\")\n\npayroll_2014 &lt;- aggregate_payroll |&gt; filter(fiscal_year == 2014) |&gt; pull(tot_payroll)\npayroll_2024 &lt;- aggregate_payroll |&gt; filter(fiscal_year == 2024) |&gt; pull(tot_payroll)\n\nggplot(aggregate_payroll, aes(x = fiscal_year, y = tot_payroll)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(\n    title = \"NYC Payroll Growth 2014-2024\",\n    x = \"Year\",\n    y = \"Payroll in $USD\"\n  ) +\n  scale_y_continuous(labels = label_dollar(scale = 1e-9, suffix = \"B\")) +\n  scale_x_continuous(breaks = seq(min(aggregate_payroll$fiscal_year), max(aggregate_payroll$fiscal_year), by = 1)) +\n  theme_minimal()"
  },
  {
    "objectID": "mp01.html#what-is-the-average-salary-of-employees-who-work-outside-the-five-boroughs",
    "href": "mp01.html#what-is-the-average-salary-of-employees-who-work-outside-the-five-boroughs",
    "title": "Analysis on NYC Payroll",
    "section": "",
    "text": "Show the code\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\n\n# Use a different name for your data frame\nnyc_data &lt;- nyc_payroll\n\ntemp &lt;- nyc_data |&gt;\n  # Filter out rows where work_location_borough is one of the five boroughs or is NA\n  filter(!work_location_borough %in% c(\"Manhattan\", \"Queens\", \"Richmond\", \"Brooklyn\", \"Bronx\") &\n         !is.na(work_location_borough)) |&gt;\n  # Compute gross annual pay if not already computed\n  mutate(gross_annual_pay = regular_gross_paid + total_ot_paid + total_other_pay) |&gt;\n  group_by(work_location_borough) |&gt;\n  summarize(avg_salary = mean(gross_annual_pay, na.rm = TRUE), .groups = \"drop\") |&gt;\n  mutate(avg_salary = dollar(avg_salary))\n\nkable(temp |&gt; rename(\n  \"LOCATION OUTSIDE NYC\" = work_location_borough,\n  \"AVERAGE SALARY\" = avg_salary\n)) |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\nLOCATION OUTSIDE NYC\nAVERAGE SALARY\n\n\n\n\nWork_location_borough\n$50,041.10\n\n\n\n\n\n\n\nThe average salary of employees who work outside of the five boroughs is $50,041."
  },
  {
    "objectID": "mp01.html#policy-i.",
    "href": "mp01.html#policy-i.",
    "title": "Analysis on NYC Payroll",
    "section": "Policy I.",
    "text": "Policy I.\nCATS is considering a policy that would ensure no subordinate employee earns more than the mayor. They have tasked us with analyzing the mayor’s total annual pay, identifying employees who exceed that amount, calculating the potential savings if these employees’ compensation were capped, and determining which job titles would be most affected by this policy.\n\n\nShow the code\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# 1. Compute the mayor's salary for each fiscal year.\nmayor_salary &lt;- nyc_payroll |&gt;\n  filter(\n    # Identify mayor records by checking if title contains \"mayor\" (case-insensitive)\n    # and/or the agency is \"Office Of The Mayor\".\n    str_detect(title_description, \"(?i)mayor\") & \n      (agency_name == \"Office Of The Mayor\" | title_description == \"Mayor\")\n  ) |&gt;\n  group_by(fiscal_year) |&gt;\n  summarize(Mayor_Salary = max(base_salary, na.rm = TRUE), .groups = \"drop\")\n\n# 2. Identify employees (subordinates) whose salary exceeds the mayor's salary in the same fiscal year.\nemployees_above_mayor &lt;- nyc_payroll |&gt;\n  # Exclude mayor records\n  filter(\n    !(title_description == \"Mayor\" | \n      (agency_name == \"Office Of The Mayor\" & str_detect(title_description, \"(?i)mayor\")))\n  ) |&gt;\n  left_join(mayor_salary, by = \"fiscal_year\") |&gt;\n  mutate(\n    Earns_More_Than_Mayor = if_else(base_salary &gt; Mayor_Salary, \"Yes\", \"No\"),\n    Excess_Amount = if_else(Earns_More_Than_Mayor == \"Yes\", base_salary - Mayor_Salary, 0)\n  ) |&gt;\n  filter(Earns_More_Than_Mayor == \"Yes\") |&gt;\n  select(fiscal_year, agency_name, title_description, base_salary, Mayor_Salary, Excess_Amount) |&gt;\n  arrange(desc(Excess_Amount))\n\nprint(\"Employees earning more than the mayor's salary by fiscal year:\")\nprint(employees_above_mayor)\n\n# 3. Determine total savings if these employees’ compensation were capped at the mayor’s salary.\ntotal_savings &lt;- employees_above_mayor |&gt;\n  summarize(Total_Savings = sum(Excess_Amount, na.rm = TRUE)) |&gt;\n  pull(Total_Savings)\n\ncat(\"Total Savings if compensation were capped at the mayor's salary:\", total_savings, \"\\n\")\n\n# 4. Identify which agencies and job titles bear the brunt of this policy.\ntop_positions &lt;- employees_above_mayor |&gt;\n  group_by(agency_name, title_description) |&gt;\n  summarize(\n    Count = n(),\n    Avg_Excess = mean(Excess_Amount, na.rm = TRUE),\n    Total_Excess = sum(Excess_Amount, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(Total_Excess)) |&gt;\n  slice(1:10)\n\nkable(top_positions, \n      col.names = c(\"Agency\", \"Position\", \"Count\", \"Average Excess\", \"Total Excess\"),\n      format.args = list(big.mark = \",\"),\n      caption = \"Top 10 Positions/Agencies Exceeding Mayor's Salary\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) |&gt;\n  column_spec(3:5, width = \"100px\")\n\n\n66 employees will be impacted by capping their salaries. Before we move into recommending a cap on all salaries to the Mayor’s, each position must be reviewed on a case by case basis. Although we know that we can save up $2.6 million, there are some critical city agencies such as the Comprotroller, Department of Education and New York City Police Department that could have talent gutted if the suggestion is followed.\nThe first department to scrutinize is the New York City Housing Authority, which shows over half a million in total excess. This agency, responsible for affordable housing for city residents, has a history of payment abuses; notably, NYCHA Chairman Gregory Russ resigned after receiving a salary exceeding $414,000 during his tenure."
  },
  {
    "objectID": "mp01.html#policy-i.-impact",
    "href": "mp01.html#policy-i.-impact",
    "title": "Analysis on NYC Payroll",
    "section": "Policy I. Impact:",
    "text": "Policy I. Impact:\n66 employees will be impacted by capping their salaries. Before we move into recommending a cap on all salaries to the Mayor’s, each position must be reviewed on a case by case basis. Although we know that we can save up $2.6 million, there are some critical city agencies such as the Comprotroller, Department of Education and New York City Police Department that could have talent gutted if the suggestion is followed.\nThe first department to scrutinize is the New York City Housing Authority, which shows over half a million in total excess. This agency, responsible for affordable housing for city residents, has a history of payment abuses; notably, NYCHA Chairman Gregory Russ resigned after receiving a salary exceeding $414,000 during his tenure."
  },
  {
    "objectID": "mp01.html#policy-ii.-reducing-overtime",
    "href": "mp01.html#policy-ii.-reducing-overtime",
    "title": "Analysis on NYC Payroll Optimization",
    "section": "Policy II. Reducing Overtime",
    "text": "Policy II. Reducing Overtime\nThe CATS comissioners have wanted to analyze the potential upside in hiring more employees to reduce the amount of overtime that is paid. It is theorized that the city can save money on total overtime pay with increasing the amount of regular hours work by hiring more employees.\n\n\nShow the code\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\n\n# recreating the nyc_payroll salary\nnyc_payroll_with_hr &lt;- nyc_payroll |&gt;\n  mutate(\n    hourly_rate = case_when(\n      pay_basis == \"per Annum\" ~ base_salary / 2000,    \n      pay_basis == \"per Hour\"  ~ base_salary,           \n      pay_basis == \"per Day\"   ~ base_salary / 8,\n      TRUE ~ NA_real_\n    )\n  )\n\n# overtime for employees with ot hrs\novertime_analysis &lt;- nyc_payroll_with_hr |&gt;\n  filter(!is.na(hourly_rate), ot_hours &gt; 0) |&gt;\n  group_by(agency_name, title_description) |&gt;\n  summarize(\n    total_ot_hours = sum(ot_hours, na.rm = TRUE),\n    total_savings = sum(ot_hours * hourly_rate * 0.5, na.rm = TRUE),\n    employees_needed = ceiling(total_ot_hours / 2000),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(employees_needed)) |&gt;\n  slice_head(n = 5)\n\n# kable\nkable(overtime_analysis,\n      col.names = c(\"Agency Name\", \"Title Description\", \"Total Overtime Hours\", \"Total Savings\", \"Employees Needed\"),\n      caption = \"Top 5 Overtime Analysis by Agency and Job Title\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nTop 5 Overtime Analysis by Agency and Job Title\n\n\nAgency Name\nTitle Description\nTotal Overtime Hours\nTotal Savings\nEmployees Needed\n\n\n\n\nPolice Department\nPolice Officer\n63670892\n1187266167\n31836\n\n\nFire Department\nFirefighter\n43536481\n864202905\n21769\n\n\nDepartment Of Correction\nCorrection Officer\n34095332\n650179743\n17048\n\n\nDepartment Of Sanitation\nSanitation Worker\n23098532\n415208226\n11550\n\n\nPolice Department\nP.o. Da Det Gr3\n16211740\n398057007\n8106\n\n\n\n\n\n\n\n\nOver 31,000 NYPD officers are needed to make up for the overtime hours that have been earned over the last 10 years. By hiring them, the city would be able to save over $1 Billion.\n\nAlso,\n\n21,679 Firemen.\n17,048 Correction Officers.\n11,550 Sanitation Workers.\n\nIn a recent report by the comptroller’s office, for the FY23 the city will spend over $472 million in NYPD overtime, $98 million above it’s budget."
  },
  {
    "objectID": "mp01.html#recommendations",
    "href": "mp01.html#recommendations",
    "title": "Analysis on NYC Payroll",
    "section": "Recommendations",
    "text": "Recommendations\n\nHire more NYPD officers\nBring overtime hours under control, by implementing a cap.\n\nIt’s hardly surprising that the top five positions and agencies with the most overtime hours are in essential roles. These mission-critical positions require careful evaluation by the city, as excessive overtime spending can deplete city resources, affect employee health, and increase the potential for fraud."
  },
  {
    "objectID": "mp01.html#policy-iii.",
    "href": "mp01.html#policy-iii.",
    "title": "Analysis on NYC Payroll",
    "section": "Policy III.",
    "text": "Policy III.\nIn the following policy, we will be looking into a strategy into streamlining the cities expenditures in payroll and work towards to balance the books.\n\n\nShow the code\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(ggplot2)\n\n# Rename the dataset to avoid conflicts\nnyc_data &lt;- nyc_payroll |&gt;\n  mutate(\n    total_pay = case_when(\n      pay_basis == \"per Hour\" ~ (regular_hours * base_salary) + ((ot_hours * 0.75) * (base_salary * 1.5)),\n      pay_basis == \"per Day\" ~ (base_salary * (regular_hours / 7.5)) + ((ot_hours * 0.75) * ((base_salary / 7.5) * 1.5)),\n      pay_basis == \"per Annum\" ~ base_salary + (((base_salary) / 1950) * (ot_hours * 0.75)),\n      pay_basis == \"Prorated Annual\" ~ regular_gross_paid,\n      TRUE ~ NA_real_\n    )\n  ) |&gt;\n  mutate(\n    hourly_rate = case_when(\n      pay_basis == \"per Hour\" ~ base_salary,\n      pay_basis == \"per Day\" ~ base_salary / 7.5,\n      pay_basis == \"per Annum\" ~ base_salary / 2000,\n      pay_basis == \"Prorated Annual\" ~ base_salary / 2000,\n      TRUE ~ NA_real_\n    )\n  )\n\n# Aggregate payroll data by agency and fiscal year, and calculate payroll savings as 2% of the total payroll.\ndata_transformed &lt;- nyc_data |&gt;\n  group_by(agency_name, fiscal_year) |&gt;\n  summarize(\n    total_employees = n(),\n    avg_payroll = mean(total_pay, na.rm = TRUE),\n    payroll_savings = 0.02 * (avg_payroll * total_employees),\n    .groups = \"drop\"\n  )\n\n# Summarize agency-level savings across years (top 10 agencies)\ntop_agencies_reduction &lt;- data_transformed |&gt;\n  group_by(agency_name) |&gt;\n  summarize(\n    total_payroll_savings = sum(payroll_savings, na.rm = TRUE),\n    avg_payroll_savings = mean(payroll_savings, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(total_payroll_savings)) |&gt;\n  slice_head(n = 10) |&gt;\n  mutate(avg_payroll_savings = dollar(avg_payroll_savings))\n\n# Summarize annual savings (if needed)\nannual_savings &lt;- data_transformed |&gt;\n  group_by(fiscal_year) |&gt;\n  summarize(total_savings = sum(payroll_savings, na.rm = TRUE), .groups = \"drop\") |&gt;\n  mutate(total_savings = dollar(total_savings))\n\n# Print the numerical output for reference\nprint(\"Top Agencies Reduction:\")\n\n\n[1] \"Top Agencies Reduction:\"\n\n\nShow the code\nprint(top_agencies_reduction)\n\n\n# A tibble: 10 × 3\n   agency_name                    total_payroll_savings avg_payroll_savings\n   &lt;chr&gt;                                          &lt;dbl&gt; &lt;chr&gt;              \n 1 Dept Of Ed Pedagogical                   2218302021. $201,663,820       \n 2 Police Department                        1049718696. $95,428,972        \n 3 Fire Department                           367076091. $33,370,554        \n 4 Department Of Correction                  229020421. $20,820,038        \n 5 Dept Of Ed Para Professionals             228797067. $20,799,733        \n 6 Department Of Education Admin             224719999. $20,429,091        \n 7 Department Of Sanitation                  200285840. $18,207,804        \n 8 Nyc Housing Authority                     191879408. $17,443,583        \n 9 Hra/Dept Of Social Services               186253379. $16,932,125        \n10 Dept Of Environment Protection            121486772. $11,044,252        \n\n\nShow the code\nprint(\"Annual Savings:\")\n\n\n[1] \"Annual Savings:\"\n\n\nShow the code\nprint(annual_savings)\n\n\n# A tibble: 11 × 2\n   fiscal_year total_savings\n         &lt;int&gt; &lt;chr&gt;        \n 1        2014 $434,976,733 \n 2        2015 $491,278,865 \n 3        2016 $511,118,138 \n 4        2017 $523,886,018 \n 5        2018 $548,739,573 \n 6        2019 $571,509,454 \n 7        2020 $604,393,190 \n 8        2021 $591,990,062 \n 9        2022 $648,409,787 \n10        2023 $617,324,250 \n11        2024 $638,163,386 \n\n\nShow the code\n# Create a horizontal bar graph for the top agencies by total payroll savings.\nggplot(top_agencies_reduction, aes(x = reorder(agency_name, total_payroll_savings), y = total_payroll_savings)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Payroll Savings: 2% Headcount & 25% Overtime Reduction\",\n    x = \"Agency\",\n    y = \"Total Payroll Savings\"\n  ) +\n  scale_y_continuous(labels = label_dollar()) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nIn the previous two policies we explored: 1. Capping Salaries. 2. Hiring employees to reduce overtime.\nWhile both of these policies hold merit and should be implemented in some capacity, the city urgently needs to rein in its rapidly escalating costs from the past decade."
  },
  {
    "objectID": "mp01.html#recommendation",
    "href": "mp01.html#recommendation",
    "title": "Analysis on NYC Payroll",
    "section": "Recommendation:",
    "text": "Recommendation:\n\nReduce the Overall Workforce Count by 2% This measure will help streamline operations and trim excess spending, positioning the city for a more sustainable future.\nCut Overall Overtime Hours by 25% By significantly reducing overtime, the city can directly curb inflated labor costs while also encouraging more efficient staffing practices.\n\nAlthough these initiatives may be unpopular, establishing firm spending limits is essential to stabilize and improve the city’s financial health. The top 3 city agencies that will be affected the most by these savings; Education, Police & Department can all submit their own reports on where cuts can be made without having to disrupt necessary functions.\nhttps://comptroller.nyc.gov/newsroom/nypd-overspending-on-overtime-grew-dramatically-in-recent-years/"
  },
  {
    "objectID": "mp01.html#policy-i.-recommendations",
    "href": "mp01.html#policy-i.-recommendations",
    "title": "Analysis on NYC Payroll Optimization",
    "section": "Policy I. Recommendations:",
    "text": "Policy I. Recommendations:\nRecommendation: Adopt this policy while continuing to assess each job and agency individually, focusing on their essential functions. Additionally, it is advisable to audit the largest agencies and consult independent city employees (those not directly employed by the agency in question) to explore cost-saving measures without the immediate need to impose salary caps. Once this is completed, proceed with implementing the cap."
  },
  {
    "objectID": "mp01.html#policy-ii.-recommendations",
    "href": "mp01.html#policy-ii.-recommendations",
    "title": "Analysis on NYC Payroll Optimization",
    "section": "Policy II. Recommendations:",
    "text": "Policy II. Recommendations:\n\nHire more NYPD officers, and for other ‘critical’ roles.\nBring overtime hours under control, by implementing a cap.\n\nIt’s hardly surprising that the top five positions and agencies with the most overtime hours are in essential roles. These mission-critical positions require careful evaluation by the city, as excessive overtime spending can deplete city resources, affect employee health, and increase the potential for fraud."
  },
  {
    "objectID": "mp01.html#policy-iii-recommendations",
    "href": "mp01.html#policy-iii-recommendations",
    "title": "Analysis on NYC Payroll Optimization",
    "section": "Policy III Recommendations:",
    "text": "Policy III Recommendations:\n\nReduce the Overall Workforce Count by 2%. This measure will help streamline operations and trim excess spending on headcount, positioning the city for a more sustainable future. This will average savings of over $600 million per year.\nCut Overall Overtime Hours by 25%. By significantly reducing overtime, the city can directly curb inflated labor costs while also encouraging more efficient staffing practices. This is also a push to hire more individuals for key positions to reduce the need for overtime.\n\nAlthough this policy may be unpopular, establishing firm spending limits is essential to stabilize and improve the city’s financial health. The top 3 city agencies that will be affected the most by these savings; Education, Police & Department can all submit their own reports on where cuts can be made without having to disrupt necessary functions. This policy is the preferred of the three."
  },
  {
    "objectID": "mp01.html#policy-iii.-special-recommendation",
    "href": "mp01.html#policy-iii.-special-recommendation",
    "title": "Analysis on NYC Payroll",
    "section": "Policy III. Special Recommendation",
    "text": "Policy III. Special Recommendation\nIn the following policy, we will be looking into a strategy into streamlining the cities expenditures in payroll and work towards to balance the books.\n\n\nShow the code\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(ggplot2)\n\n# Create the unified dataset with total_pay and hourly_rate\nnyc_data &lt;- nyc_payroll |&gt;\n  mutate(\n    total_pay = case_when(\n      pay_basis == \"per Hour\" ~ (regular_hours * base_salary) + ((ot_hours * 0.75) * (base_salary * 1.5)),\n      pay_basis == \"per Day\" ~ (base_salary * (regular_hours / 7.5)) + ((ot_hours * 0.75) * ((base_salary / 7.5) * 1.5)),\n      pay_basis == \"per Annum\" ~ base_salary + (((base_salary) / 1950) * (ot_hours * 0.75)),\n      pay_basis == \"Prorated Annual\" ~ regular_gross_paid,\n      TRUE ~ NA_real_\n    )\n  ) |&gt;\n  mutate(\n    hourly_rate = case_when(\n      pay_basis == \"per Hour\" ~ base_salary,\n      pay_basis == \"per Day\" ~ base_salary / 7.5,\n      pay_basis == \"per Annum\" ~ base_salary / 2000,\n      pay_basis == \"Prorated Annual\" ~ base_salary / 2000,\n      TRUE ~ NA_real_\n    )\n  )\n\n# Aggregate payroll data by agency and fiscal year; calculate payroll savings as 2% of total payroll.\ndata_transformed &lt;- nyc_data |&gt;\n  group_by(agency_name, fiscal_year) |&gt;\n  summarize(\n    total_employees = n(),\n    avg_payroll = mean(total_pay, na.rm = TRUE),\n    payroll_savings = 0.02 * (avg_payroll * total_employees),\n    .groups = \"drop\"\n  )\n\n# Summarize agency-level savings across years (top 10 agencies)\ntop_agencies_reduction &lt;- data_transformed |&gt;\n  group_by(agency_name) |&gt;\n  summarize(\n    total_payroll_savings = sum(payroll_savings, na.rm = TRUE),\n    avg_payroll_savings = mean(payroll_savings, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(total_payroll_savings)) |&gt;\n  slice_head(n = 10) |&gt;\n  mutate(avg_payroll_savings = dollar(avg_payroll_savings))\n\n# Summarize annual savings\nannual_savings &lt;- data_transformed |&gt;\n  group_by(fiscal_year) |&gt;\n  summarize(total_savings = sum(payroll_savings, na.rm = TRUE), .groups = \"drop\") |&gt;\n  mutate(total_savings = dollar(total_savings))\n\n# Display the top agencies table using kable\nkable(top_agencies_reduction, \n      caption = \"Top 10 Agencies by Payroll Savings (2% Headcount Reduction)\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nTop 10 Agencies by Payroll Savings (2% Headcount Reduction)\n\n\nagency_name\ntotal_payroll_savings\navg_payroll_savings\n\n\n\n\nDept Of Ed Pedagogical\n2218302021\n$201,663,820\n\n\nPolice Department\n1049718696\n$95,428,972\n\n\nFire Department\n367076091\n$33,370,554\n\n\nDepartment Of Correction\n229020421\n$20,820,038\n\n\nDept Of Ed Para Professionals\n228797067\n$20,799,733\n\n\nDepartment Of Education Admin\n224719999\n$20,429,091\n\n\nDepartment Of Sanitation\n200285840\n$18,207,804\n\n\nNyc Housing Authority\n191879408\n$17,443,583\n\n\nHra/Dept Of Social Services\n186253379\n$16,932,125\n\n\nDept Of Environment Protection\n121486772\n$11,044,252\n\n\n\n\n\n\n\nShow the code\n# Display the annual savings table using kable\nkable(annual_savings, \n      caption = \"Annual Payroll Savings (2% Headcount Reduction)\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nAnnual Payroll Savings (2% Headcount Reduction)\n\n\nfiscal_year\ntotal_savings\n\n\n\n\n2014\n$434,976,733\n\n\n2015\n$491,278,865\n\n\n2016\n$511,118,138\n\n\n2017\n$523,886,018\n\n\n2018\n$548,739,573\n\n\n2019\n$571,509,454\n\n\n2020\n$604,393,190\n\n\n2021\n$591,990,062\n\n\n2022\n$648,409,787\n\n\n2023\n$617,324,250\n\n\n2024\n$638,163,386\n\n\n\n\n\n\n\nShow the code\n# Create a horizontal bar graph for the top agencies by total payroll savings\nggplot(top_agencies_reduction, aes(x = reorder(agency_name, total_payroll_savings), y = total_payroll_savings)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Top 10 Agencies by Total Payroll Savings (2% Headcount Reduction)\",\n    x = \"Agency\",\n    y = \"Total Payroll Savings\"\n  ) +\n  scale_y_continuous(labels = label_dollar()) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nIn the previous two policies we explored:\n\nCapping Salaries.\nHiring employees to reduce overtime.\n\nWhile both of these policies hold merit and should be implemented in some capacity, the city urgently needs to rein in its rapidly escalating costs from the past decade."
  },
  {
    "objectID": "mp01.html#policy-i.-salary-cap",
    "href": "mp01.html#policy-i.-salary-cap",
    "title": "Analysis on NYC Payroll Optimization",
    "section": "Policy I. Salary Cap",
    "text": "Policy I. Salary Cap\nCATS is considering a policy that would ensure no subordinate employee earns more than the mayor. They have tasked us with analyzing the mayor’s total annual pay, identifying employees who exceed that amount, calculating the potential savings if these employees’ compensation were capped, and determining which job titles would be most affected by this policy.\n\n\nShow the code\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(knitr)\nlibrary(kableExtra)\n\n#GPT helped re-code and debug alot of the code in this cell\n\n# getting mayor salary\nleader_sal &lt;- nyc_payroll |&gt;\n  filter(str_detect(title_description, \"(?i)mayor\") &\n         (agency_name == \"Office Of The Mayor\" | title_description == \"Mayor\")) |&gt;\n  group_by(fiscal_year) |&gt;\n  summarize(leader_sal = max(base_salary, na.rm = TRUE), .groups = \"drop\")\n\n# Find employees earning more than mayor.\nsub_excess &lt;- nyc_payroll |&gt;\n  filter(!(title_description == \"Mayor\" |\n           (agency_name == \"Office Of The Mayor\" & str_detect(title_description, \"(?i)mayor\")))) |&gt;\n  left_join(leader_sal, by = \"fiscal_year\") |&gt;\n  mutate(excess = if_else(base_salary &gt; leader_sal, base_salary - leader_sal, 0)) |&gt;\n  filter(excess &gt; 0) |&gt;\n  select(fiscal_year, agency_name, title_description, base_salary, leader_sal, excess) |&gt;\n  arrange(desc(excess))\n\n# caluiclating total savings\ntotal_save &lt;- sub_excess |&gt;\n  summarize(total_save = sum(excess, na.rm = TRUE)) |&gt;\n  pull(total_save)\n\ncat(\"Total Savings if capped at the mayor's salary:\", total_save, \"\\n\")\n\n# 4. top 10 agencies above mayor salary\ntop_groups &lt;- sub_excess |&gt;\n  group_by(agency_name, title_description) |&gt;\n  summarize(\n    count = n(),\n    mean_excess = mean(excess, na.rm = TRUE),\n    total_excess = sum(excess, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(total_excess)) |&gt;\n  slice_head(n = 10)\n\n# kable\nkable(top_groups,\n      col.names = c(\"Agency\", \"Position\", \"Employee Count\", \"Mean Excess To Mayor Salary\", \"Total Excess To Mayor Salary\"),\n      caption = \"Top 10 Positions/Agencies Exceeding Mayor's Salary\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\n\nShow the code\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\n\n#\nagency_excess &lt;- sub_excess |&gt;\n  group_by(agency_name) |&gt;\n  summarize(total_excess = sum(excess, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(total_excess))\n\n#bar graph. Comparing agency and total excess\nggplot(agency_excess, aes(x = reorder(agency_name, total_excess), y = total_excess)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +  # Makes it horizontal for better readability\n  labs(\n    title = \"Total Excess to Mayor's Salary by Agency\",\n    x = \"Agency\",\n    y = \"Total Excess to Mayor's Salary\"\n  ) +\n  scale_y_continuous(labels = label_dollar()) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nAs we calculated in the beginning of the report, the mayor in 2024 made $258,750. At least 66 employees have made more than the mayor and will be impacted by capping their salaries. Before we move into recommending a cap on all salaries to the Mayor’s, each position must be reviewed on a case by case basis. Although we know that we can save up $2.6 million, there are some critical city agencies such as the Comptroller, Department of Education and New York City Police Department that could have talent gutted if the suggestion is followed.\nThe first department to scrutinize is the New York City Housing Authority, which shows over $800 million in total excess. This agency, responsible for affordable housing for city residents, has a history of payment abuses; notably, NYCHA Chairman Gregory Russ resigned last year after receiving a salary exceeding $414,000 during his tenure."
  },
  {
    "objectID": "mp01.html#introduction",
    "href": "mp01.html#introduction",
    "title": "Analysis on NYC Payroll Optimization",
    "section": "",
    "text": "This report was comissioned by the Commission to Analyze Taxpayer spending. As an incoming technical analyst, I was appointed to investigate several areas of interest that pertained to the way the city handles it’s payroll bill. The following analysis seeks to uncover interesting trends that will aide city stakeholders in their decision making process on how to handle it’s finances. This is followed by several recommended policies that the city wants to undertake, and a third policy underwritten by the team on how New York City can get it’s spending under control.\nEric L. Adams is the current New York City mayor until 2025. Below, we examine his journey from being a borough president in FY 2014 until his ascendancy to become the Mayor of New York City in 2023. His current salary for being Mayor was $258,720 in 2024.\n\n\n\nMayor Adams Salary by Fiscal Year\n\n\nFiscal Year\nTotal Salary\nPosition\nAgency\n\n\n\n\n2014\n160000\nBorough President\nBorough President-Brooklyn\n\n\n2015\n160000\nBorough President\nBorough President-Brooklyn\n\n\n2016\n179200\nBorough President\nBorough President-Brooklyn\n\n\n2017\n179200\nBorough President\nBorough President-Brooklyn\n\n\n2018\n179200\nBorough President\nBorough President-Brooklyn\n\n\n2019\n179200\nBorough President\nBorough President-Brooklyn\n\n\n2020\n179200\nBorough President\nBorough President-Brooklyn\n\n\n2021\n179200\nBorough President\nBorough President-Brooklyn\n\n\n2022\n437950\nBorough President\nBorough President-Brooklyn\n\n\n2023\n258750\nMayor\nOffice Of The Mayor\n\n\n2024\n258750\nMayor\nOffice Of The Mayor"
  },
  {
    "objectID": "mp01.html#policy-iii.-trimming-workforce-overtime-hours",
    "href": "mp01.html#policy-iii.-trimming-workforce-overtime-hours",
    "title": "Analysis on NYC Payroll Optimization",
    "section": "Policy III. Trimming Workforce & Overtime Hours",
    "text": "Policy III. Trimming Workforce & Overtime Hours\nIn the following policy, we will be looking into a strategy into streamlining the cities expenditures in payroll and work towards to balance the books.\n\n\nShow the code\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(ggplot2)\n\n# Create the unified dataset with total_pay and hourly_rate. ot hours are multiplied by .75 to signify a 25% reduction\nnyc_data &lt;- nyc_payroll |&gt;\n  mutate(\n    total_pay = case_when(\n      pay_basis == \"Prorated Annual\" ~ (regular_gross_paid),\n      pay_basis == \"per Hour\" ~ (regular_hours * base_salary) + ((ot_hours * 0.75) * (base_salary * 1.5)),\n      pay_basis == \"per Day\" ~ (base_salary * (regular_hours / 7.5)) + ((ot_hours * 0.75) * ((base_salary / 7.5) * 1.5)),\n      pay_basis == \"per Annum\" ~ (base_salary + ((base_salary) / 1950) * (ot_hours * 0.75)),\n      TRUE ~ as.numeric(NA)\n    )\n  ) \n\n#calculating 2% reduction savings in headcount\ndata_transformed &lt;- nyc_data |&gt;\n  group_by(agency_name, fiscal_year) |&gt;\n  summarize(\n    total_employees = n(),\n    avg_payroll = mean(total_pay, na.rm = TRUE),\n    payroll_savings = 0.02 * (avg_payroll * total_employees),\n    .groups = \"drop\"\n  )\n\n# top 10 agencies\ntop_agencies_reduction &lt;- data_transformed |&gt;\n  group_by(agency_name) |&gt;\n  summarize(\n    total_payroll_savings = sum(payroll_savings, na.rm = TRUE),\n    avg_payroll_savings = mean(payroll_savings, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(total_payroll_savings)) |&gt;\n  slice_head(n = 10) |&gt;\n  mutate(avg_payroll_savings = dollar(avg_payroll_savings))\n\n# Summarize annual savings\nannual_savings &lt;- data_transformed |&gt;\n  group_by(fiscal_year) |&gt;\n  summarize(total_savings = sum(payroll_savings, na.rm = TRUE), .groups = \"drop\") |&gt;\n  mutate(total_savings = dollar(total_savings))\n\nkable(top_agencies_reduction,\n      col.names = c(\"Agency Name\", \"Total Payroll Savings\", \"Average Payroll Savings\"),\n      caption = \"Top 10 Agencies by Payroll Savings (2% Headcount Reduction & 25% Overtime Hours Reduction)\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nTop 10 Agencies by Payroll Savings (2% Headcount Reduction & 25% Overtime Hours Reduction)\n\n\nAgency Name\nTotal Payroll Savings\nAverage Payroll Savings\n\n\n\n\nDept Of Ed Pedagogical\n2218302021\n$201,663,820\n\n\nPolice Department\n1049718696\n$95,428,972\n\n\nFire Department\n367076091\n$33,370,554\n\n\nDepartment Of Correction\n229020421\n$20,820,038\n\n\nDept Of Ed Para Professionals\n228797067\n$20,799,733\n\n\nDepartment Of Education Admin\n224719999\n$20,429,091\n\n\nDepartment Of Sanitation\n200285840\n$18,207,804\n\n\nNyc Housing Authority\n191879408\n$17,443,583\n\n\nHra/Dept Of Social Services\n186253379\n$16,932,125\n\n\nDept Of Environment Protection\n121486772\n$11,044,252\n\n\n\n\n\n\n\nShow the code\nkable(annual_savings,\n      col.names = c(\"Fiscal Year\", \"Total Payroll Savings\"),\n      caption = \"Annual Payroll Savings (2% Headcount & 25% Overtime Hours Reduction)\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE)\n\n\n\nAnnual Payroll Savings (2% Headcount & 25% Overtime Hours Reduction)\n\n\nFiscal Year\nTotal Payroll Savings\n\n\n\n\n2014\n$434,976,733\n\n\n2015\n$491,278,865\n\n\n2016\n$511,118,138\n\n\n2017\n$523,886,018\n\n\n2018\n$548,739,573\n\n\n2019\n$571,509,454\n\n\n2020\n$604,393,190\n\n\n2021\n$591,990,062\n\n\n2022\n$648,409,787\n\n\n2023\n$617,324,250\n\n\n2024\n$638,163,386\n\n\n\n\n\n\n\nShow the code\n#bar graph\nggplot(top_agencies_reduction, aes(x = reorder(agency_name, total_payroll_savings), y = total_payroll_savings)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Top 10 Agencies by Total Payroll Savings (2% Headcount & 25% Overtime Hours Reduction)\",\n    x = \"Agency\",\n    y = \"Total Payroll Savings\"\n  ) +\n  scale_y_continuous(labels = label_dollar()) +\n  theme_minimal()"
  },
  {
    "objectID": "mp01.html#key-issues",
    "href": "mp01.html#key-issues",
    "title": "Analysis on NYC Payroll Optimization",
    "section": "Key Issues",
    "text": "Key Issues\n\nMany employees earn more than the Mayor, our highest position in the city.\nFrom 2014 to 2024, the City’s payroll grew over $8 billion.\nMany agencies rely on overtime pay, hinting at staff shortages or avenues of abuse by certain individuals.\nA cost-control mechanism needs to be implemented.\n\nExternal Sources: 1. Github Co-Pilot\n\nGPT (debugging) For questions 5,7,8 & Policy I.\n\nhttps://comptroller.nyc.gov/newsroom/nypd-overspending-on-overtime-grew-dramatically-in-recent-years/"
  },
  {
    "objectID": "mp01.html#key-takeaways",
    "href": "mp01.html#key-takeaways",
    "title": "Analysis on NYC Payroll Optimization",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nMany employees earn more than the Mayor, our highest position in the city.\nFrom 2014 to 2024, the City’s payroll grew over $8 billion.\nMany agencies rely on overtime pay, hinting at staff shortages or avenues of abuse by certain individuals.\nA cost-control mechanism needs to be implemented.\n\n\nReferences\n\nGithub Co-Pilot\nGPT (debugging) For questions 7,8 & Policy I.\n\n\n\nSources\nhttps://comptroller.nyc.gov/newsroom/nypd-overspending-on-overtime-grew-dramatically-in-recent-years/"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Rolling In Green: GTA IV’s Award’s for Eco Transit",
    "section": "",
    "text": "Let’s Get Rolling!\n\n\nAs the newly minted Executive Director of the Green Transit Alliance for Investigation of Variance, I’m here to kick off the annual Rolling In Green awards—where sustainability meets a bit of sass! Our mission? To celebrate the transit agencies across the nation that continue to hustle hard for a cleaner, greener planet. So buckle up! Now, let’s shine the spotlight on those agencies who’ve not only gone green but have practically turned sustainability into an art form. Stay tuned for the green revolution on wheels!\nThe awards are:  1. Greenest Transit Agency  2. Most Emissions Avoiders  3. Smooth Sailors  4. Earth Destroyers \n\n\n\n\n\nThe water tastes fresh on this side\n\n\nThis award goes to the agency that is always fighting for our planet, the one that strives to streamline its workflow and push relentlessly toward sustainability. We honor the trailblazers who not only embrace green practices but also innovate to reduce their environmental impact. Their continuous effort to optimize operations and lower emissions sets a shining example for the transit industry, proving that eco-friendly progress is not only possible but essential for our future.\nSmall-Sized Transit Agency:  Agency - Hampton Jitney, Inc  State - New York  Green Score - 0.4  Emissions Per Mile - 0.12 CO₂ emissions\nMedium-sized Transit Agency:  Agency - City of Seattle, dba: Seattle Center Monorail  State - Washington  Green Score - 0.07  Emissions Per Mile - 0.077 CO₂ emissions \nLarge-sized Transit Agency:  Agency - Tri-County Metropolitan Transportation District of Oregon, dba: TriMet  State - Oregon  Green Score - 0.2  Emissions Per Mile - 0.124 CO₂ emissions\nThis award was determined by carefully analyzing the emissions performance of each agency. We computed the CO₂ emissions per UPT (Unlinked Passenger Trip) and per mile, providing a comprehensive view of their environmental impact. To ensure a fair comparison, we calculated a green score by taking the harmonic mean of these two emissions metrics. This method highlights agencies that consistently excel in reducing emissions both on a per-trip and per-mile basis, ultimately honoring those who lead the charge in sustainable transit solutions.\n\n\n\n\n\n\nKeep Running!\n\n\nThis award goes to the agency that has dodged more emissions than a cat avoids a bath. It doesn’t matter whether you’re a small-town hero or a big-city giant—what matters is the monumental strides each agency makes in avoiding emissions. In a world where we’re all searching for alternatives to car congestion and the endless honking, these agencies have carved out greener, faster routes to ease the burden on our communities. Their efforts not only improve our quality of life but also prove that fighting pollution can be as exhilarating as outrunning rush hour traffic. Congratulations to those who make our air cleaner and our commutes a little less chaotic!\nSmall-Sized Transit Agency:  Agency - Hampton Jitney, Inc  State - New York  Total Miles Accrued - 49,533,110  Lbs of CO2 avoided - 28,931,084 \nMedium-sized Transit Agency:  Agency - Hudson Transit Lines, Inc., dba: Short Line  State - New Jersey  Total Miles Accrued - 93,151,932  Lbs of CO2 avoided - 51,369,438 \nLarge-sized Transit Agency  Agency - MTA New York City Transit  State - New York  Total Miles Accrued - 47,956,268,290  Lbs of CO2 avoided - 29,455,493,069 \nWe computed the emissions avoided by each agency using a straightforward set of steps. First, we summed each agency’s total miles traveled and estimated their actual transit emissions based on a per-mile factor. Then, using US CAFE standards, we converted the miles into gallons—assuming an average of 25 miles per gallon—and calculated the hypothetical emissions if those miles were driven in an individual vehicle (with 19.6 lbs CO₂ per gallon). Finally, by subtracting the actual transit emissions from the hypothetical driving emissions, we arrived at the emissions avoided. This metric not only highlights the environmental impact of each agency’s operations but also underscores the significant strides made in reducing our carbon footprint through smart transit choices.\n\n\n\n\n\n\nRolling it in!\n\n\nYou know we love being green here! Renewable energy is as good as it can get, and we here are excited to present this award to the ones who use electricity the most. In a world where innovation meets sustainability, our award celebrates those transit agencies that spark change and set a high-voltage example for eco-friendly transportation. By harnessing the power of clean energy, these pioneers not only reduce carbon footprints but also lead the charge toward a more sustainable future. Their commitment to electrification is a testament to the potential of modern technology to transform everyday travel into a greener, more efficient experience. With high-speed trains buzzing on electric tracks, they’re powering progress one mile at a time. Congratulations to all the electrifying contenders—keep the current flowing!\nSmall-Sized Transit Agency:  Agency - Pennsylvania Department of Transportation  State - Pennsylvania  Percent Electric: 100%  Total Miles: 33,974,346 \nMedium-sized Transit Agency:  Agency - Northern Indiana Commuter Transportation District, dba: South Shore Line  State - Indiana  Percent Electric: 100%  Total Miles: 49,941,941 \nLarge-sized Transit Agency  Agency - Port Authority Trans-Hudson Corporation  State - New York  Percent Electric: 100%  Total Miles: 268,404,831 \nOur methodology was straightforward yet robust. We calculated the award score by dividing the total electricity used—comprising both battery and propulsion energy—by the total fuel sources consumed, ranging from diesel to battery power. This calculation provided us with a “percent electric score,” a clear indicator of how electric-powered a transit agency’s operations are.\nWe recognize that several agencies achieve a 100% carbon-neutral status by relying entirely on electric power. However, to add an extra layer of competitive rigor, we also considered travel mileage. For each category—small, medium, and large agencies—we selected the contenders that not only excel in electrification but also travel the most miles. This dual criterion ensures that our award honors those transit agencies that not only commit to sustainability but also demonstrate exceptional operational efficiency and impact.\n\n\n\n\n\n\nYea you…\n\n\nThis list might be naughtier than Santa’s, and we are definetely checking this twice. The Earth Destroyer award is the least sought after the attendees today, and the most exciting one for us here at the Green Transit Alliance. This distinction is not a celebration but rather a candid wake-up call for agencies that remain heavily reliant on diesel—the fuel source responsible for significant CO₂ emissions.\nSmall-Sized Transit Agency:  Agency - Seastreak LLC  State - New Jersey  Earth-Destroying Ratio - 2.48 gallons of diesel per UPT trip \nMedium-sized Transit Agency:  Agency - Virginia Railway Express  State - Virginia  Earth-Destroying Ratio - 2.48 gallons of diesel per UPT trip \nLarge-sized Transit Agency  Agency - Washington State Ferries  State - Washington  Earth-Destroying Ratio - 2.48 gallons of diesel per UPT trip \nThis award was calculated by taking diesel, which is the highest CO₂ emitting energy source, and divided it by UPT, a measure of trips taken by passengers across the nation. Take this as a measure to stride towards, and to continously improve on.\n\n\n\nAnd so, as we wrap up tonight’s celebration of high-octane eco-heroics, let’s give a standing (and maybe slightly electrified) ovation to our trailblazing transit agencies. Whether you’ve been zipping past emissions like a cat in a bathtub or powering the future one electric mile at a time, you’ve shown that fighting pollution can be as fun as outrunning rush hour—and almost as shocking as a surprise electric bill. Thank you for making our air cleaner, our commutes smoother, and our future a lot brighter. Keep dodging, keep rolling, and may your journeys always be greener than your punchlines!"
  },
  {
    "objectID": "mp02.html#the-earth-destroyers",
    "href": "mp02.html#the-earth-destroyers",
    "title": "Rolling In Green: GTA IV’s Award’s for Eco Transit",
    "section": "",
    "text": "Yea you…\n\n\nThis list might be naughtier than Santa’s, and we are definetely checking this twice. The Earth Destroyer award is the least sought after the attendees today, and the most exciting one for us here at the Green Transit Alliance. This distinction is not a celebration but rather a candid wake-up call for agencies that remain heavily reliant on diesel—the fuel source responsible for significant CO₂ emissions.\nSmall-Sized Transit Agency:  Agency - Seastreak LLC  State - New Jersey  Earth-Destroying Ratio - 2.48 gallons of diesel per UPT trip \nMedium-sized Transit Agency:  Agency - Virginia Railway Express  State - Virginia  Earth-Destroying Ratio - 2.48 gallons of diesel per UPT trip \nLarge-sized Transit Agency  Agency - Washington State Ferries  State - Washington  Earth-Destroying Ratio - 2.48 gallons of diesel per UPT trip \nThis award was calculated by taking diesel, which is the highest CO₂ emitting energy source, and divided it by UPT, a measure of trips taken by passengers across the nation. Take this as a measure to stride towards, and to continously improve on."
  },
  {
    "objectID": "mp02.html#the-electrocutioners",
    "href": "mp02.html#the-electrocutioners",
    "title": "Rolling In Green: GTA IV’s Award’s for Eco Transit",
    "section": "The Electrocutioners",
    "text": "The Electrocutioners\n\n\n\nRolling it in!\n\n\nYou know we love being green here! Renewable energy is as good as it can get, and we here are excited to present this award to the ones who use electricity the most. In a world where innovation meets sustainability, our award celebrates those transit agencies that spark change and set a high-voltage example for eco-friendly transportation. By harnessing the power of clean energy, these pioneers not only reduce carbon footprints but also lead the charge toward a more sustainable future. Their commitment to electrification is a testament to the potential of modern technology to transform everyday travel into a greener, more efficient experience. With high-speed trains buzzing on electric tracks, they’re powering progress one mile at a time. Congratulations to all the electrifying contenders—keep the current flowing!\nSmall-Sized Transit Agency: Agency - Pennsylvania Department of Transportation\nState - Pennsylvania Percent Electric: 100% Total Miles: 33,974,346\nMedium-sized Transit Agency: Agency - Northern Indiana Commuter Transportation District, dba: South Shore Line State - Indiana Percent Electric: 100% Total Miles: 49,941,941\nLarge-sized Transit Agency Agency - Port Authority Trans-Hudson Corporation State - New York Percent Electric: 100% Total Miles: 268,404,831\nOur methodology was straightforward yet robust. We calculated the award score by dividing the total electricity used—comprising both battery and propulsion energy—by the total fuel sources consumed, ranging from diesel to battery power. This calculation provided us with a “percent electric score,” a clear indicator of how electric-powered a transit agency’s operations are.\nWe recognize that several agencies achieve a 100% carbon-neutral status by relying entirely on electric power. However, to add an extra layer of competitive rigor, we also considered travel mileage. For each category—small, medium, and large agencies—we selected the contenders that not only excel in electrification but also travel the most miles. This dual criterion ensures that our award honors those transit agencies that not only commit to sustainability but also demonstrate exceptional operational efficiency and impact."
  },
  {
    "objectID": "mp02.html#the-smooth-sailors",
    "href": "mp02.html#the-smooth-sailors",
    "title": "Rolling In Green: GTA IV’s Award’s for Eco Transit",
    "section": "",
    "text": "Rolling it in!\n\n\nYou know we love being green here! Renewable energy is as good as it can get, and we here are excited to present this award to the ones who use electricity the most. In a world where innovation meets sustainability, our award celebrates those transit agencies that spark change and set a high-voltage example for eco-friendly transportation. By harnessing the power of clean energy, these pioneers not only reduce carbon footprints but also lead the charge toward a more sustainable future. Their commitment to electrification is a testament to the potential of modern technology to transform everyday travel into a greener, more efficient experience. With high-speed trains buzzing on electric tracks, they’re powering progress one mile at a time. Congratulations to all the electrifying contenders—keep the current flowing!\nSmall-Sized Transit Agency:  Agency - Pennsylvania Department of Transportation  State - Pennsylvania  Percent Electric: 100%  Total Miles: 33,974,346 \nMedium-sized Transit Agency:  Agency - Northern Indiana Commuter Transportation District, dba: South Shore Line  State - Indiana  Percent Electric: 100%  Total Miles: 49,941,941 \nLarge-sized Transit Agency  Agency - Port Authority Trans-Hudson Corporation  State - New York  Percent Electric: 100%  Total Miles: 268,404,831 \nOur methodology was straightforward yet robust. We calculated the award score by dividing the total electricity used—comprising both battery and propulsion energy—by the total fuel sources consumed, ranging from diesel to battery power. This calculation provided us with a “percent electric score,” a clear indicator of how electric-powered a transit agency’s operations are.\nWe recognize that several agencies achieve a 100% carbon-neutral status by relying entirely on electric power. However, to add an extra layer of competitive rigor, we also considered travel mileage. For each category—small, medium, and large agencies—we selected the contenders that not only excel in electrification but also travel the most miles. This dual criterion ensures that our award honors those transit agencies that not only commit to sustainability but also demonstrate exceptional operational efficiency and impact."
  },
  {
    "objectID": "mp02.html#greenest-transit-agency",
    "href": "mp02.html#greenest-transit-agency",
    "title": "Rolling In Green: GTA IV’s Award’s for Eco Transit",
    "section": "",
    "text": "The water tastes fresh on this side\n\n\nThis award goes to the agency that is always fighting for our planet, the one that strives to streamline its workflow and push relentlessly toward sustainability. We honor the trailblazers who not only embrace green practices but also innovate to reduce their environmental impact. Their continuous effort to optimize operations and lower emissions sets a shining example for the transit industry, proving that eco-friendly progress is not only possible but essential for our future.\nSmall-Sized Transit Agency:  Agency - Hampton Jitney, Inc  State - New York  Green Score - 0.4  Emissions Per Mile - 0.12 CO₂ emissions\nMedium-sized Transit Agency:  Agency - City of Seattle, dba: Seattle Center Monorail  State - Washington  Green Score - 0.07  Emissions Per Mile - 0.077 CO₂ emissions \nLarge-sized Transit Agency:  Agency - Tri-County Metropolitan Transportation District of Oregon, dba: TriMet  State - Oregon  Green Score - 0.2  Emissions Per Mile - 0.124 CO₂ emissions\nThis award was determined by carefully analyzing the emissions performance of each agency. We computed the CO₂ emissions per UPT (Unlinked Passenger Trip) and per mile, providing a comprehensive view of their environmental impact. To ensure a fair comparison, we calculated a green score by taking the harmonic mean of these two emissions metrics. This method highlights agencies that consistently excel in reducing emissions both on a per-trip and per-mile basis, ultimately honoring those who lead the charge in sustainable transit solutions."
  },
  {
    "objectID": "mp02.html#most-emissions-avoided",
    "href": "mp02.html#most-emissions-avoided",
    "title": "Rolling In Green: GTA IV’s Award’s for Eco Transit",
    "section": "",
    "text": "Keep Running!\n\n\nThis award goes to the agency that has dodged more emissions than a cat avoids a bath. It doesn’t matter whether you’re a small-town hero or a big-city giant—what matters is the monumental strides each agency makes in avoiding emissions. In a world where we’re all searching for alternatives to car congestion and the endless honking, these agencies have carved out greener, faster routes to ease the burden on our communities. Their efforts not only improve our quality of life but also prove that fighting pollution can be as exhilarating as outrunning rush hour traffic. Congratulations to those who make our air cleaner and our commutes a little less chaotic!\nSmall-Sized Transit Agency: Agency - Hampton Jitney, Inc State - New York Total Miles Accrued - 49,533,110 Lbs of CO2 avoided - 28,931,084\nMedium-sized Transit Agency: Agency - Hudson Transit Lines, Inc., dba: Short Line State - New Jersey Total Miles Accrued - 93,151,932 Lbs of CO2 avoided - 51,369,438\nLarge-sized Transit Agency Agency - MTA New York City Transit State - New York Total Miles Accrued - 47,956,268,290 Lbs of CO2 avoided - 29,455,493,069\nWe computed the emissions avoided by each agency using a straightforward set of steps. First, we summed each agency’s total miles traveled and estimated their actual transit emissions based on a per-mile factor. Then, using US CAFE standards, we converted the miles into gallons—assuming an average of 25 miles per gallon—and calculated the hypothetical emissions if those miles were driven in an individual vehicle (with 19.6 lbs CO₂ per gallon). Finally, by subtracting the actual transit emissions from the hypothetical driving emissions, we arrived at the emissions avoided. This metric not only highlights the environmental impact of each agency’s operations but also underscores the significant strides made in reducing our carbon footprint through smart transit choices."
  },
  {
    "objectID": "mp02.html#most-emissions-avoiders",
    "href": "mp02.html#most-emissions-avoiders",
    "title": "Rolling In Green: GTA IV’s Award’s for Eco Transit",
    "section": "",
    "text": "Keep Running!\n\n\nThis award goes to the agency that has dodged more emissions than a cat avoids a bath. It doesn’t matter whether you’re a small-town hero or a big-city giant—what matters is the monumental strides each agency makes in avoiding emissions. In a world where we’re all searching for alternatives to car congestion and the endless honking, these agencies have carved out greener, faster routes to ease the burden on our communities. Their efforts not only improve our quality of life but also prove that fighting pollution can be as exhilarating as outrunning rush hour traffic. Congratulations to those who make our air cleaner and our commutes a little less chaotic!\nSmall-Sized Transit Agency:  Agency - Hampton Jitney, Inc  State - New York  Total Miles Accrued - 49,533,110  Lbs of CO2 avoided - 28,931,084 \nMedium-sized Transit Agency:  Agency - Hudson Transit Lines, Inc., dba: Short Line  State - New Jersey  Total Miles Accrued - 93,151,932  Lbs of CO2 avoided - 51,369,438 \nLarge-sized Transit Agency  Agency - MTA New York City Transit  State - New York  Total Miles Accrued - 47,956,268,290  Lbs of CO2 avoided - 29,455,493,069 \nWe computed the emissions avoided by each agency using a straightforward set of steps. First, we summed each agency’s total miles traveled and estimated their actual transit emissions based on a per-mile factor. Then, using US CAFE standards, we converted the miles into gallons—assuming an average of 25 miles per gallon—and calculated the hypothetical emissions if those miles were driven in an individual vehicle (with 19.6 lbs CO₂ per gallon). Finally, by subtracting the actual transit emissions from the hypothetical driving emissions, we arrived at the emissions avoided. This metric not only highlights the environmental impact of each agency’s operations but also underscores the significant strides made in reducing our carbon footprint through smart transit choices."
  },
  {
    "objectID": "mp02.html#task-6---awards-calculations",
    "href": "mp02.html#task-6---awards-calculations",
    "title": "Rolling In Green: GTA IV’s Award’s for Eco Transit",
    "section": "Task 6 - Awards Calculations",
    "text": "Task 6 - Awards Calculations\n\n\nShow the code\n# Define new thresholds based on UPT quartiles.\nP25 &lt;- quantile(agency_modes |&gt; select(upt) |&gt; unique() |&gt; pull(upt), 0.25)\nP50 &lt;- quantile(agency_modes |&gt; select(upt) |&gt; unique() |&gt; pull(upt), 0.50)\nP75 &lt;- quantile(agency_modes |&gt; select(upt) |&gt; unique() |&gt; pull(upt), 0.75)\n\n# Mutating a new emissions per up and per mile columns\nagency_modes &lt;- agency_modes |&gt;\n  group_by(ntd_id) |&gt;\n  mutate(\n    emissions_per_upt  = sum(total_co2_emissions) / upt,\n    emissions_per_mile = sum(total_co2_emissions) / miles,\n    \n    #we will use the harmonic mean. Particuarly useful as in this case emissions_pet_upt and emissions_per_mile are like ratios, and are still related to one another. Such as total co2 emissions per upt for every mile.\n    green_score = round(2 * (emissions_per_upt * emissions_per_mile) / (emissions_per_upt + emissions_per_mile), 2),\n    \n    # Assign a size category based on UPT quartiles. I have an 'extra small' category but it won't be used for the awards\n    size = case_when(\n      upt &lt; P25 ~ \"Extra Small\",\n      upt &gt;= P25 & upt &lt; P50 ~ \"Small\",\n      upt &gt;= P50 & upt &lt; P75 ~ \"Medium\",\n      upt &gt;= P75 ~ \"Large\"\n    )\n  ) |&gt;\n  ungroup()\n\n\n\nGreenest Transit Agency\n\n\nShow the code\n# Greenest agency for Small agencies\ngreenest_small &lt;- agency_modes |&gt;\n  filter(size == \"Small\") |&gt;\n  filter(green_score == min(green_score, na.rm = TRUE)) |&gt;\n  select(agency, state, city, miles, primary_source, emissions_per_mile, green_score, size)\n\n# Greenest agency for Medium agencies\ngreenest_medium &lt;- agency_modes |&gt;\n  filter(size == \"Medium\") |&gt;\n  filter(green_score == min(green_score, na.rm = TRUE)) |&gt;\n  select(agency, state, city, miles, primary_source, emissions_per_mile, green_score, size)\n\n# Greenest agency for Large agencies\ngreenest_large &lt;- agency_modes |&gt;\n  filter(size == \"Large\") |&gt;\n  filter(green_score == min(green_score, na.rm = TRUE)) |&gt;\n  select(agency, state, city, miles, primary_source, emissions_per_mile, green_score, size)\n\nkable(greenest_small, caption = \"Greenest Small Transit Agency\", align = \"c\")\n\n\n\nGreenest Small Transit Agency\n\n\n\n\n\n\n\n\n\n\n\n\nagency\nstate\ncity\nmiles\nprimary_source\nemissions_per_mile\ngreen_score\nsize\n\n\n\n\nHampton Jitney, Inc.\nNew York\nCalverton\n49533110\nNatural gas\n0.1999243\n0.4\nSmall\n\n\n\n\n\nShow the code\nkable(greenest_medium, caption = \"Greenest Medium Transit Agency\", align = \"c\")\n\n\n\nGreenest Medium Transit Agency\n\n\n\n\n\n\n\n\n\n\n\n\nagency\nstate\ncity\nmiles\nprimary_source\nemissions_per_mile\ngreen_score\nsize\n\n\n\n\nCity of Seattle, dba: Seattle Center Monorail\nWashington\nSeattle\n1921479\nHydroelectric\n0.0768396\n0.07\nMedium\n\n\n\n\n\nShow the code\nkable(greenest_large, caption = \"Greenest Large Transit Agency\", align = \"c\")\n\n\n\nGreenest Large Transit Agency\n\n\n\n\n\n\n\n\n\n\n\n\nagency\nstate\ncity\nmiles\nprimary_source\nemissions_per_mile\ngreen_score\nsize\n\n\n\n\nTri-County Metropolitan Transportation District of Oregon, dba: TriMet\nOregon\nPortland\n231430380\nHydroelectric\n0.1247651\n0.2\nLarge\n\n\nTri-County Metropolitan Transportation District of Oregon, dba: TriMet\nOregon\nPortland\n231430380\nHydroelectric\n0.1247651\n0.2\nLarge\n\n\nTri-County Metropolitan Transportation District of Oregon, dba: TriMet\nOregon\nPortland\n231430380\nHydroelectric\n0.1247651\n0.2\nLarge\n\n\n\n\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(dplyr)\n\nagency_modes |&gt;\n  filter(size != \"Extra Small\") |&gt;\n  ggplot(aes(x = green_score, fill = size)) +\n  geom_density(alpha = 0.6) +\n  labs(\n    title = \"Distribution of Green Scores by Agency\",\n    x = \"Green Score (Lower is Greener)\",\n    y = \"Density\",\n    fill = \"Agency Size\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\n\nMost Emissions Avoiders\n\n\nShow the code\nlibrary(scales)\n\n# Define conversion constants from US CAFE standards\nmpg_value &lt;- 25            # Average miles per gallon\nfuel_emission_factor &lt;- 19.6  # lbs CO₂ emitted per gallon\n\n# Summarize each agency's performance: total miles and actual transit emissions.\n# We assume actual transit emissions are approximated by: total_miles * emissions_per_mile.\nagency_emissions_summary &lt;- agency_modes |&gt;\n  group_by(agency, state, size) |&gt;\n  summarise(\n    total_miles = sum(miles),\n    actual_transit_emissions = sum(miles * emissions_per_mile)\n  ) |&gt;\n  ungroup() |&gt;\n  mutate(\n    # Convert miles to gallons used based on average MPG\n    gallons_used = total_miles / mpg_value,\n    # Hypothetical emissions if each mile were driven in an individual vehicle\n    hypothetical_driving_emissions = gallons_used * fuel_emission_factor,\n    # Emissions avoided by using transit instead of driving\n    avoided_emissions = hypothetical_driving_emissions - actual_transit_emissions\n  )\n\n# For Small Agencies\nemissions_avoided_small &lt;- agency_emissions_summary |&gt;\n  filter(size == \"Small\") |&gt;\n  arrange(desc(avoided_emissions)) |&gt;\n  slice_head(n = 1)\n\n# Format numeric columns with commas for Small Agencies\nemissions_avoided_small_fmt &lt;- emissions_avoided_small |&gt;\n  mutate(\n    total_miles = comma(total_miles),\n    actual_transit_emissions = comma(actual_transit_emissions),\n    gallons_used = comma(gallons_used),\n    hypothetical_driving_emissions = comma(hypothetical_driving_emissions),\n    avoided_emissions = comma(avoided_emissions)\n  )\n\nkable(\n  emissions_avoided_small_fmt,\n  caption = \"Most Emissions Avoided: Small Agencies\",\n  align = \"c\"\n)\n\n\n\nMost Emissions Avoided: Small Agencies\n\n\n\n\n\n\n\n\n\n\n\n\nagency\nstate\nsize\ntotal_miles\nactual_transit_emissions\ngallons_used\nhypothetical_driving_emissions\navoided_emissions\n\n\n\n\nHampton Jitney, Inc.\nNew York\nSmall\n49,533,110\n9,902,875\n1,981,324\n38,833,958\n28,931,084\n\n\n\n\n\nShow the code\n# For Medium Agencies\nemissions_avoided_medium &lt;- agency_emissions_summary |&gt;\n  filter(size == \"Medium\") |&gt;\n  arrange(desc(avoided_emissions)) |&gt;\n  slice_head(n = 1)\n\n# Format numeric columns with commas for Medium Agencies\nemissions_avoided_medium_fmt &lt;- emissions_avoided_medium |&gt;\n  mutate(\n    total_miles = comma(total_miles),\n    actual_transit_emissions = comma(actual_transit_emissions),\n    gallons_used = comma(gallons_used),\n    hypothetical_driving_emissions = comma(hypothetical_driving_emissions),\n    avoided_emissions = comma(avoided_emissions)\n  )\n\nkable(\n  emissions_avoided_medium_fmt,\n  caption = \"Most Emissions Avoided: Medium Agencies\",\n  align = \"c\"\n)\n\n\n\nMost Emissions Avoided: Medium Agencies\n\n\n\n\n\n\n\n\n\n\n\n\nagency\nstate\nsize\ntotal_miles\nactual_transit_emissions\ngallons_used\nhypothetical_driving_emissions\navoided_emissions\n\n\n\n\nHudson Transit Lines, Inc., dba: Short Line\nNew Jersey\nMedium\n93,151,932\n21,721,677\n3,726,077\n73,031,115\n51,309,438\n\n\n\n\n\nShow the code\n# For Large Agencies\nemissions_avoided_large &lt;- agency_emissions_summary |&gt;\n  filter(size == \"Large\") |&gt;\n  arrange(desc(avoided_emissions)) |&gt;\n  slice_head(n = 1)\n\n# Format numeric columns with commas for Large Agencies\nemissions_avoided_large_fmt &lt;- emissions_avoided_large |&gt;\n  mutate(\n    total_miles = comma(total_miles),\n    actual_transit_emissions = comma(actual_transit_emissions),\n    gallons_used = comma(gallons_used),\n    hypothetical_driving_emissions = comma(hypothetical_driving_emissions),\n    avoided_emissions = comma(avoided_emissions)\n  )\n\nkable(\n  emissions_avoided_large_fmt,\n  caption = \"Most Emissions Avoided: Large Agencies\",\n  align = \"c\"\n)\n\n\n\nMost Emissions Avoided: Large Agencies\n\n\n\n\n\n\n\n\n\n\n\n\nagency\nstate\nsize\ntotal_miles\nactual_transit_emissions\ngallons_used\nhypothetical_driving_emissions\navoided_emissions\n\n\n\n\nMTA New York City Transit\nNew York\nLarge\n47,956,268,290\n8,142,221,270\n1,918,250,732\n37,597,714,339\n29,455,493,069\n\n\n\n\n\n\n\nThe Smooth Sailors\n\n\nShow the code\n# We are going to compute the electification award\n# Divide electric by all fuel sources\n\n# Summarize electrification metrics for each agency (including state) and total miles.\nagency_electrification &lt;- agency_modes |&gt;\n  group_by(agency, state, size) |&gt;\n  summarise(\n    total_electric = sum(electric_battery) + sum(electric_propulsion),\n    total_fuel = sum(biodiesel) + sum(cnaturalgas) + sum(diesel) + sum(gasoline) +\n                 sum(liqnatgas) + sum(liqpetgas) + sum(electric_battery) + sum(electric_propulsion),\n    total_miles = sum(miles)\n  ) |&gt;\n  mutate(\n    pct_electric = (total_electric / total_fuel) * 100\n  ) |&gt;\n  ungroup()\n\n# For Small Agencies: sort by pct_electric and total_miles, then take the top one.\nelectrified_small &lt;- agency_electrification |&gt;\n  filter(size == \"Small\") |&gt;\n  arrange(desc(pct_electric), desc(total_miles)) |&gt;\n  slice_head(n = 1)\n\nkable(\n  electrified_small,\n  caption = \"Agency with Highest Electrification – Small Agencies\",\n  align = \"c\"\n)\n\n\n\nAgency with Highest Electrification – Small Agencies\n\n\n\n\n\n\n\n\n\n\n\nagency\nstate\nsize\ntotal_electric\ntotal_fuel\ntotal_miles\npct_electric\n\n\n\n\nPennsylvania Department of Transportation\nPennsylvania\nSmall\n14204836\n14204836\n33974346\n100\n\n\n\n\n\nShow the code\n# For Medium Agencies\nelectrified_medium &lt;- agency_electrification |&gt;\n  filter(size == \"Medium\") |&gt;\n  arrange(desc(pct_electric), desc(total_miles)) |&gt;\n  slice_head(n = 1)\n\nkable(\n  electrified_medium,\n  caption = \"Agency with Highest Electrification – Medium Agencies\",\n  align = \"c\"\n)\n\n\n\nAgency with Highest Electrification – Medium Agencies\n\n\n\n\n\n\n\n\n\n\n\nagency\nstate\nsize\ntotal_electric\ntotal_fuel\ntotal_miles\npct_electric\n\n\n\n\nNorthern Indiana Commuter Transportation District, dba: South Shore Line\nIndiana\nMedium\n11106350\n11106350\n49941941\n100\n\n\n\n\n\nShow the code\n# For Large Agencies\nelectrified_large &lt;- agency_electrification |&gt;\n  filter(size == \"Large\") |&gt;\n  arrange(desc(pct_electric), desc(total_miles)) |&gt;\n  slice_head(n = 1)\n\nkable(\n  electrified_large,\n  caption = \"Agency with Highest Electrification – Large Agencies\",\n  align = \"c\"\n)\n\n\n\nAgency with Highest Electrification – Large Agencies\n\n\n\n\n\n\n\n\n\n\n\nagency\nstate\nsize\ntotal_electric\ntotal_fuel\ntotal_miles\npct_electric\n\n\n\n\nPort Authority Trans-Hudson Corporation\nNew York\nLarge\n98060829\n98060829\n268404831\n100\n\n\n\n\n\n\n\nThe Earth Destroyers\n\n\nShow the code\n# Summarize diesel usage and UPT\nagency_dirtiness &lt;- agency_modes |&gt;\n  group_by(agency, state, size) |&gt;\n  summarise(\n    total_diesel = sum(diesel),\n    total_upt = sum(upt)\n  ) |&gt;\n  mutate(\n    # Dirtiness score: higher means more diesel usage per UPT.\n    dirtiness = total_diesel / total_upt\n  ) |&gt;\n  ungroup()\n\n# Small Agencies\ndirtiest_small &lt;- agency_dirtiness |&gt;\n  filter(size == \"Small\") |&gt;\n  arrange(desc(dirtiness)) |&gt;\n  slice_head(n = 1)\n\nkable(\n  dirtiest_small,\n  caption = \"Dirtiest Transit Agency: Small Agencies\",\n  align = \"c\"\n)\n\n\n\nDirtiest Transit Agency: Small Agencies\n\n\n\n\n\n\n\n\n\n\nagency\nstate\nsize\ntotal_diesel\ntotal_upt\ndirtiness\n\n\n\n\nSeaStreak, LLC\nNew Jersey\nSmall\n2956636\n1192351\n2.479669\n\n\n\n\n\nShow the code\n# Medium Agencies\ndirtiest_medium &lt;- agency_dirtiness |&gt;\n  filter(size == \"Medium\") |&gt;\n  arrange(desc(dirtiness)) |&gt;\n  slice_head(n = 1)\n\nkable(\n  dirtiest_medium,\n  caption = \"Dirtiest Transit Agency: Medium Agencies\",\n  align = \"c\"\n)\n\n\n\nDirtiest Transit Agency: Medium Agencies\n\n\n\n\n\n\n\n\n\n\nagency\nstate\nsize\ntotal_diesel\ntotal_upt\ndirtiness\n\n\n\n\nVirginia Railway Express\nVirginia\nMedium\n1665975\n1466250\n1.136215\n\n\n\n\n\nShow the code\n# Large Agencies\ndirtiest_large &lt;- agency_dirtiness |&gt;\n  filter(size == \"Large\") |&gt;\n  arrange(desc(dirtiness)) |&gt;\n  slice_head(n = 1)\n\nkable(\n  dirtiest_large,\n  caption = \"Dirtiest Transit Agency: Large Agencies\",\n  align = \"c\"\n)\n\n\n\nDirtiest Transit Agency: Large Agencies\n\n\n\n\n\n\n\n\n\n\nagency\nstate\nsize\ntotal_diesel\ntotal_upt\ndirtiness\n\n\n\n\nWashington State Ferries\nWashington\nLarge\n14012522\n18241119\n0.7681832\n\n\n\n\n\n\n\nShow the code\nlibrary(ggplot2)\n\n\n# Combine the dirtiest agencies from small-large into one df\ntop_dirtiest &lt;- bind_rows(\n  dirtiest_small,\n  dirtiest_medium,\n  dirtiest_large\n)\nggplot(top_dirtiest, aes(x = reorder(agency, dirtiness), y = dirtiness, fill = size)) +\n  geom_bar(stat = \"identity\", width = 0.6) +\n  facet_wrap(~ size, scales = \"free_x\") +\n  labs(\n    title = \"Dirtiest Transit Agencies by Size Category\",\n    x = \"Agency\",\n    y = \"Dirtiness Score (Diesel per UPT)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )"
  },
  {
    "objectID": "mp02.html#conlusion-of-the-awards",
    "href": "mp02.html#conlusion-of-the-awards",
    "title": "Rolling In Green: GTA IV’s Award’s for Eco Transit",
    "section": "",
    "text": "And so, as we wrap up tonight’s celebration of high-octane eco-heroics, let’s give a standing (and maybe slightly electrified) ovation to our trailblazing transit agencies. Whether you’ve been zipping past emissions like a cat in a bathtub or powering the future one electric mile at a time, you’ve shown that fighting pollution can be as fun as outrunning rush hour—and almost as shocking as a surprise electric bill. Thank you for making our air cleaner, our commutes smoother, and our future a lot brighter. Keep dodging, keep rolling, and may your journeys always be greener than your punchlines!"
  },
  {
    "objectID": "mp02.html#conclusion-of-the-awards",
    "href": "mp02.html#conclusion-of-the-awards",
    "title": "Rolling In Green: GTA IV’s Award’s for Eco Transit",
    "section": "",
    "text": "And so, as we wrap up tonight’s celebration of high-octane eco-heroics, let’s give a standing (and maybe slightly electrified) ovation to our trailblazing transit agencies. Whether you’ve been zipping past emissions like a cat in a bathtub or powering the future one electric mile at a time, you’ve shown that fighting pollution can be as fun as outrunning rush hour—and almost as shocking as a surprise electric bill. Thank you for making our air cleaner, our commutes smoother, and our future a lot brighter. Keep dodging, keep rolling, and may your journeys always be greener than your punchlines!"
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "The Ultimate Playlist",
    "section": "",
    "text": "In this project, we harness Spotify’s rich dataset to answer a simple but compelling question: what makes a song not just good, but universally loved? By tapping into two distinct data exports, one detailing track popularity metrics and another cataloguing song attributes (tempo, key, danceability, and energy)—we can move beyond subjective taste and build an evidence‑backed “ultimate playlist.”\nThis project is also insipired by the “All Rise” playlist, by Mr Barney Stinson, as the Ultimate Playtlist helps define a great aggregation of music into a single playlist based on what we love to hear most.\nBelow, we will begin with the two datasets that will be used from this project.\n\n\n\n\nCode\nload_songs &lt;- function() {\n  library(readr)\n  dir_path  &lt;- \"data/mp03\"\n  file_name &lt;- \"songs.csv\"\n  file_path &lt;- file.path(dir_path, file_name)\n\n  #checking dupolicate so that github will not block it\n  if (!dir.exists(dir_path)) {\n    dir.create(dir_path, recursive = TRUE)\n  }\n  \n  # download\n  if (!file.exists(file_path)) {\n    download.file(\n      \"https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\",\n      destfile = file_path,\n      mode = \"wb\"\n    )\n  }\n  \n  # reading in dataset\n  file_path |&gt;\n    read_csv(show_col_types = FALSE)\n}\n\nsongs_df &lt;- load_songs()\nhead(songs_df)\n\n\n# A tibble: 6 × 19\n  id      name  artists duration_ms release_date  year acousticness danceability\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 6KbQ3u… Sing… ['Carl…      158648 1928          1928        0.995        0.708\n2 6KuQTI… Fant… ['Robe…      282133 1928          1928        0.994        0.379\n3 6L63VW… Chap… ['Sewe…      104300 1928          1928        0.604        0.749\n4 6M94Fk… Beba… ['Fran…      180760 9/25/28       1928        0.995        0.781\n5 6N6tiF… Polo… ['Fréd…      687733 1928          1928        0.99         0.21 \n6 6NxAf7… Sche… ['Feli…      352600 1928          1928        0.995        0.424\n# ℹ 11 more variables: energy &lt;dbl&gt;, instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;,\n#   loudness &lt;dbl&gt;, speechiness &lt;dbl&gt;, tempo &lt;dbl&gt;, valence &lt;dbl&gt;, mode &lt;dbl&gt;,\n#   key &lt;dbl&gt;, popularity &lt;dbl&gt;, explicit &lt;dbl&gt;\n\n\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(tidyr)\nlibrary(stringr)\n\nclean_artist_string &lt;- function(x) {\n  x |&gt;\n    str_replace_all(\"\\\\['\", \"\") |&gt;\n    str_replace_all(\"'\\\\]\", \"\") |&gt;\n    str_replace_all(\"[ ]?'\", \"\") |&gt;\n    str_replace_all(\"[ ]*,[ ]*\", \",\")\n}\n\nsongs_df |&gt;\n  separate_longer_delim(artists, \",\") |&gt;\n  mutate(artist = clean_artist_string(artists)) |&gt;\n  select(-artists)\n\n\n# A tibble: 226,813 × 19\n   id      name  duration_ms release_date  year acousticness danceability energy\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n 1 6KbQ3u… Sing…      158648 1928          1928        0.995        0.708 0.195 \n 2 6KuQTI… Fant…      282133 1928          1928        0.994        0.379 0.0135\n 3 6KuQTI… Fant…      282133 1928          1928        0.994        0.379 0.0135\n 4 6L63VW… Chap…      104300 1928          1928        0.604        0.749 0.22  \n 5 6M94Fk… Beba…      180760 9/25/28       1928        0.995        0.781 0.13  \n 6 6N6tiF… Polo…      687733 1928          1928        0.99         0.21  0.204 \n 7 6N6tiF… Polo…      687733 1928          1928        0.99         0.21  0.204 \n 8 6NxAf7… Sche…      352600 1928          1928        0.995        0.424 0.12  \n 9 6NxAf7… Sche…      352600 1928          1928        0.995        0.424 0.12  \n10 6O0puP… Vals…      136627 1928          1928        0.956        0.444 0.197 \n# ℹ 226,803 more rows\n# ℹ 11 more variables: instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, loudness &lt;dbl&gt;,\n#   speechiness &lt;dbl&gt;, tempo &lt;dbl&gt;, valence &lt;dbl&gt;, mode &lt;dbl&gt;, key &lt;dbl&gt;,\n#   popularity &lt;dbl&gt;, explicit &lt;dbl&gt;, artist &lt;chr&gt;\n\n\nThanks to the github user, gabminamedez, we have a master file of a catalogue of songs that contains their respective propertiers such as name, release_date, dancebility, energy and much more that will be integral for our analysis.\n\n\nCode\nload_playlists &lt;- function(n = 10) {\n  base_url &lt;- \"https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/\"\n  dir_path &lt;- \"data/mp03/playlists\"\n  if (!dir.exists(dir_path)) dir.create(dir_path, recursive = TRUE)\n\n  playlists &lt;- list()\n\n  for (i in 0:(n - 1)) {\n    start &lt;- i * 1000\n    end &lt;- start + 999\n    file_name &lt;- sprintf(\"mpd.slice.%d-%d.json\", start, end)\n    file_url &lt;- paste0(base_url, file_name)\n    file_path &lt;- file.path(dir_path, file_name)\n\n    if (!file.exists(file_path)) {\n      message(\"Downloading: \", file_name)\n      result &lt;- tryCatch({\n        download.file(file_url, file_path, mode = \"wb\", quiet = TRUE)\n        TRUE\n      }, error = function(e) {\n        message(\"Failed to download \", file_name)\n        FALSE\n      })\n\n      if (!result) next\n    }\n\n    if (file.exists(file_path)) {\n      json_data &lt;- tryCatch({\n        jsonlite::fromJSON(file_path)\n      }, error = function(e) {\n        message(\"Failed to parse \", file_name)\n        NULL\n      })\n\n      if (!is.null(json_data)) {\n        playlists[[length(playlists) + 1]] &lt;- json_data$playlists\n      }\n    }\n  }\n\n  return(playlists)\n}\n\nif (file.exists(\"data/processed_playlists.rds\")) {\n  playlists &lt;- readRDS(\"data/processed_playlists.rds\")\n} else {\n  playlists &lt;- load_playlists(n = 10)\n  saveRDS(playlists, \"data/processed_playlists.rds\")\n}\n\n\nplaylists &lt;- load_playlists(n = 10)  \n\n\nDownloading: mpd.slice.2000-2999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.2000-2999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.2000-2999.json\n\n\nDownloading: mpd.slice.4000-4999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.4000-4999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.4000-4999.json\n\n\nDownloading: mpd.slice.5000-5999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.5000-5999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.5000-5999.json\n\n\nDownloading: mpd.slice.6000-6999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.6000-6999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.6000-6999.json\n\n\nDownloading: mpd.slice.8000-8999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.8000-8999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.8000-8999.json\n\n\nDownloading: mpd.slice.9000-9999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.9000-9999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.9000-9999.json\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\nstrip_spotify_prefix &lt;- function(x) {\n  str_extract(x, \".*:.*:(.*)\", group = 1)\n}\nplaylist_df &lt;- playlists[[1]] |&gt;\n  \n  # 1. Keep just the playlist info + the tracks column\n  mutate(\n    playlist_name      = name,\n    playlist_id        = strip_spotify_prefix(pid),\n    playlist_followers = num_followers\n  ) |&gt;\n  select(playlist_name, playlist_id, playlist_followers, tracks) |&gt;\n  unnest(tracks) |&gt;\n  \n  # These are more of the columns that were specifid we needed for this project\n  mutate(\n    playlist_position = row_number(),                 \n    artist_name       = artist_name,                   \n    artist_id         = strip_spotify_prefix(artist_uri),\n    track_name        = track_name,\n    track_id          = strip_spotify_prefix(track_uri),\n    album_name        = album_name,\n    album_id          = strip_spotify_prefix(album_uri),\n    duration          = duration_ms                    \n  ) |&gt;\n\n  select(\n    playlist_name,\n    playlist_id,\n    playlist_position,\n    playlist_followers,\n    artist_name,\n    artist_id,\n    track_name,\n    track_id,\n    album_name,\n    album_id,\n    duration\n  )\n\n# Look at the first few rows\nhead(playlist_df)\n\n\n# A tibble: 6 × 11\n  playlist_name playlist_id playlist_position playlist_followers artist_name    \n  &lt;chr&gt;         &lt;chr&gt;                   &lt;int&gt;              &lt;int&gt; &lt;chr&gt;          \n1 Throwbacks    &lt;NA&gt;                        1                  1 Missy Elliott  \n2 Throwbacks    &lt;NA&gt;                        2                  1 Britney Spears \n3 Throwbacks    &lt;NA&gt;                        3                  1 Beyoncé        \n4 Throwbacks    &lt;NA&gt;                        4                  1 Justin Timberl…\n5 Throwbacks    &lt;NA&gt;                        5                  1 Shaggy         \n6 Throwbacks    &lt;NA&gt;                        6                  1 Usher          \n# ℹ 6 more variables: artist_id &lt;chr&gt;, track_name &lt;chr&gt;, track_id &lt;chr&gt;,\n#   album_name &lt;chr&gt;, album_id &lt;chr&gt;, duration &lt;int&gt;\n\n\n\n\n\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(knitr)\nplaylist_df |&gt;\n  summarise(\n    `Distinct Tracks`  = n_distinct(track_id),\n    `Distinct Artists` = n_distinct(artist_id)\n  ) |&gt;\n  kable(\n    caption = \"Unique Number of Artisits & Tracks\",\n    align   = \"c\"\n  )\n\n\n\nUnique Number of Artisits & Tracks\n\n\nDistinct Tracks\nDistinct Artists\n\n\n\n\n34443\n9754\n\n\n\n\n\nThe playlist dataset contains 34,443 distinct tracks and 9,754 distinct artists. Quite a large range!\n\n\n\n\n\nCode\nplaylist_df |&gt;\n  count(\n    track_name,\n    artist_name,\n    album_name,\n    name = \"Appearances\"        \n  ) |&gt;\n  arrange(desc(Appearances)) |&gt;\n  head() |&gt;\n  rename(\n    `Track Name`                     = track_name,\n    `Artist Name`                    = artist_name,\n    `Album Name`                     = album_name,\n    `Number of Playlist Appearances` = Appearances\n  ) |&gt;\n  kable(\n    caption = \"Most Popular Tracks in Playlist Dataset\",\n    align   = \"c\"\n  )\n\n\n\nMost Popular Tracks in Playlist Dataset\n\n\n\n\n\n\n\n\nTrack Name\nArtist Name\nAlbum Name\nNumber of Playlist Appearances\n\n\n\n\nOne Dance\nDrake\nViews\n55\n\n\nHUMBLE.\nKendrick Lamar\nDAMN.\n52\n\n\nBroccoli (feat. Lil Yachty)\nDRAM\nBig Baby DRAM\n50\n\n\nCloser\nThe Chainsmokers\nCloser\n46\n\n\nCongratulations\nPost Malone\nStoney\n44\n\n\nDon’t Let Me Down\nThe Chainsmokers\nThe Chainsmokers- Japan Special Edition\n42\n\n\n\n\n\nIn the table above, we display the top songs in the playlist dataset. “One Dance” by Drake, takes top spot as it appears in 55 different playlists.\n\n\n\n\n\nCode\nplaylist_df |&gt;\n  anti_join(songs_df, by = c(\"track_id\" = \"id\")) |&gt;     \n  count(\n    track_name,\n    artist_name,\n    album_name,\n    name = \"Appearances\"                                \n  ) |&gt;\n  arrange(desc(Appearances)) |&gt;                         \n  slice_head(n = 1) |&gt;                                  \n  rename(                                               \n    `Track Name`                     = track_name,\n    `Artist Name`                    = artist_name,\n    `Album Name`                     = album_name,\n    `Number of Playlist Appearances` = Appearances\n  ) |&gt;\n  kable(\n    caption = \"The Most Popular Track that is not in both Datasets\",\n    align   = \"c\"\n  )\n\n\n\nThe Most Popular Track that is not in both Datasets\n\n\n\n\n\n\n\n\nTrack Name\nArtist Name\nAlbum Name\nNumber of Playlist Appearances\n\n\n\n\nOne Dance\nDrake\nViews\n55\n\n\n\n\n\nThe song “One Dance” by Drake, does not appear in the songs characteristics data “songs_df”.\n\n\n\n\n\nCode\ntop_song &lt;- songs_df |&gt;\n  arrange(desc(danceability)) |&gt;\n  slice_head(n = 1)\n\ntop_song_id &lt;- top_song$id\nappearance_count &lt;- playlist_df |&gt;\n  filter(track_id == top_song_id) |&gt;\n  nrow()\ntop_song_table &lt;- tibble(\n  track_name            = top_song$name,\n  danceability_score    = top_song$danceability,\n  playlist_appearances  = appearance_count\n)\n\ntop_song_table\n\n\n# A tibble: 1 × 3\n  track_name        danceability_score playlist_appearances\n  &lt;chr&gt;                          &lt;dbl&gt;                &lt;int&gt;\n1 Funky Cold Medina              0.988                    1\n\n\nThe most “danceable” track, is Funky Cold Medina.\n\n\n\n\n\nCode\nplaylist_df |&gt;\n  group_by(playlist_name) |&gt;\n  summarise(\n    mean_duration_ms   = mean(duration),\n    median_duration_ms = median(duration),\n    artist_name        = first(artist_name),\n    .groups            = \"drop\"\n  ) |&gt;\n  arrange(desc(mean_duration_ms)) |&gt;\n  slice_head(n = 1) |&gt;\n  mutate(\n    `Mean duration (sec)`   = mean_duration_ms   / 1000,\n    `Median duration (sec)` = median_duration_ms / 1000\n  ) |&gt;\n  select(\n    playlist_name,\n    artist_name,\n    `Mean duration (sec)`,\n    `Median duration (sec)`\n  ) |&gt;\n  rename(\n    `Playlist Name` = playlist_name,\n    `Artist Name`   = artist_name\n  ) |&gt;\n  kable(\n    caption = \"Transit Service With Most UPT In 2023\",\n    align   = \"c\"\n  )\n\n\n\nTransit Service With Most UPT In 2023\n\n\n\n\n\n\n\n\nPlaylist Name\nArtist Name\nMean duration (sec)\nMedian duration (sec)\n\n\n\n\nclassical\nPyotr Ilyich Tchaikovsky\n411.1487\n341.42\n\n\n\n\n\nThe playlist that has the longest average duration is 411 seconds (mean). The playlist is called “Classical” and is by “Pyotr Ilyich Tchaikovsky”\n\n\n\n\n\nCode\nplaylist_df |&gt;\n  slice_max(playlist_followers, n = 1, with_ties = FALSE) |&gt;   # most‑followed playlist\n  mutate(`Duration (sec)` = duration / 1000) |&gt;                # ms → sec\n  select(\n    playlist_name,\n    playlist_position,\n    playlist_followers,\n    artist_name,\n    album_name,\n    `Duration (sec)`\n  ) |&gt;\n  rename(\n    `Playlist Name`     = playlist_name,\n    `Playlist Position` = playlist_position,\n    `Playlist Followers`= playlist_followers,\n    `Artist Name`       = artist_name,\n    `Album Name`        = album_name\n  ) |&gt;\n  kable(\n    caption = \"Most popular playlist on Spotify\",\n    align   = \"c\"\n  )\n\n\n\nMost popular playlist on Spotify\n\n\n\n\n\n\n\n\n\n\nPlaylist Name\nPlaylist Position\nPlaylist Followers\nArtist Name\nAlbum Name\nDuration (sec)\n\n\n\n\nTangled\n51259\n1038\nMandy Moore\nTangled\n152.333\n\n\n\n\n\nThe most popular playlist on Spotify is “Tangled”, with 1038 followers.\n\n\n\n\nInner-joiing both the song characteristics & playlist information datasets. Dataframe will be called inner_joined_data.\n\n\nCode\ninner_joined_data &lt;- songs_df |&gt;\n  # rename the song‐data 'id' column so it matches\n  rename(track_id = id) |&gt;\n  # now do the join by track_id\n  inner_join(playlist_df, by = \"track_id\")\n\nhead(inner_joined_data)\n\n\n# A tibble: 6 × 29\n  track_id             name  artists duration_ms release_date  year acousticness\n  &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 1J5kO1Iyuo9GUkKpVOL… \"Mis… ['Sonn…      564320 1957          1957        0.766\n2 5F0vhf0bfnmtMW2aWsx… \"Unc… ['Sonn…      235400 1965          1965        0.282\n3 5jOO4HOAqTR9KC3HfHG… \"Let… ['Jeff…      214427 8/15/66       1966        0.161\n4 6QmMCtfu6uHG1TbWHe0… \"Rai… ['John…      310800 1970          1970        0.806\n5 5FnlmowF3elDo9Psgw1… \"Old… ['Aret…      220201 1972          1972        0.365\n6 43l4wsDQHRa0lZwwuzS… \"Nev… ['Bob …      171773 1/17/74       1974        0.583\n# ℹ 22 more variables: danceability &lt;dbl&gt;, energy &lt;dbl&gt;,\n#   instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, loudness &lt;dbl&gt;, speechiness &lt;dbl&gt;,\n#   tempo &lt;dbl&gt;, valence &lt;dbl&gt;, mode &lt;dbl&gt;, key &lt;dbl&gt;, popularity &lt;dbl&gt;,\n#   explicit &lt;dbl&gt;, playlist_name &lt;chr&gt;, playlist_id &lt;chr&gt;,\n#   playlist_position &lt;int&gt;, playlist_followers &lt;int&gt;, artist_name &lt;chr&gt;,\n#   artist_id &lt;chr&gt;, track_name &lt;chr&gt;, album_name &lt;chr&gt;, album_id &lt;chr&gt;,\n#   duration &lt;int&gt;\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\n\ntrack_stats &lt;- inner_joined_data |&gt;\n  count(track_id, popularity, name = \"play_count\")\n\n#Pearson correlation between play count and popularity\ncorr_coef &lt;- cor(track_stats$play_count, track_stats$popularity)\nprint(paste(\"Pearson correlation:\", round(corr_coef, 3)))\n\n\n[1] \"Pearson correlation: 0.488\"\n\n\nCode\n#Scatterplot\ntrack_stats |&gt;\n  ggplot(aes(x = play_count, y = popularity)) +\n  geom_point(alpha = 0.4, size = 2, color = \"#2C3E50\") +\n  geom_smooth(method = \"lm\", color = \"#E74C3C\", se = FALSE) +\n  scale_x_log10() +\n  labs(\n    title    = \"Spotify Popularity vs. # of Playlist Appearances\",\n    subtitle = paste0(\"Pearson r = \", round(corr_coef, 2)),\n    x        = \"Playlist Appearances\",\n    y        = \"Spotify Popularity \"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCode\npop_cutoff &lt;- 70\n\n\nWe conducted a Pearson correlation analysis to examine whether a song’s popularity score is related to its playlist position. The resulting correlation coefficient of 0.49 indicates a moderate positive relationship: in general, more‑popular songs tend to appear earlier in playlists. However, the correlation is far from perfect, so highly popular tracks can still show up infrequently—or later—on certain playlists.\n\n\n\n\n\nCode\nyear_popularity &lt;- inner_joined_data |&gt;\n  filter(year &gt;= 1950) |&gt;\n  group_by(year) |&gt;\n  summarise(\n    median_popularity = mean(popularity),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(median_popularity))\n\nyear_popularity |&gt;\n  slice_head(n = 1)\n\n\n# A tibble: 1 × 2\n   year median_popularity\n  &lt;dbl&gt;             &lt;dbl&gt;\n1  2017              71.3\n\n\nCode\n# e.g. year == 2019\n\n# 3. Plot (ticks every 3 years on the x‑axis)\nyear_popularity |&gt;\n  ggplot(aes(x = year, y = median_popularity)) +\n  geom_point(size = 3, color = \"#2C3E50\") +\n  geom_line(color = \"#E74C3C\", size = 1) +\n  scale_x_continuous(\n    breaks = seq(1950, max(year_popularity$year), by = 3)\n  ) +\n  labs(\n    title = \"Mean Spotify Popularity by Release Year (1950+)\",\n    x     = \"Release Year\",\n    y     = \"Mean Popularity (0–100)\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nre f"
  },
  {
    "objectID": "mp03.html#initial-data-exploration",
    "href": "mp03.html#initial-data-exploration",
    "title": "The Ultimate Playlist",
    "section": "",
    "text": "Code\nlibrary(dplyr)\nlibrary(knitr)\nplaylist_df |&gt;\n  summarise(\n    `Distinct Tracks`  = n_distinct(track_id),\n    `Distinct Artists` = n_distinct(artist_id)\n  ) |&gt;\n  kable(\n    caption = \"Unique Number of Artisits & Tracks\",\n    align   = \"c\"\n  )\n\n\n\nUnique Number of Artisits & Tracks\n\n\nDistinct Tracks\nDistinct Artists\n\n\n\n\n34443\n9754\n\n\n\n\n\nThe playlist dataset contains 34,443 distinct tracks and 9,754 distinct artists. Quite a large range!\n\n\n\n\n\nCode\nplaylist_df |&gt;\n  count(\n    track_name,\n    artist_name,\n    album_name,\n    name = \"Appearances\"        \n  ) |&gt;\n  arrange(desc(Appearances)) |&gt;\n  head() |&gt;\n  rename(\n    `Track Name`                     = track_name,\n    `Artist Name`                    = artist_name,\n    `Album Name`                     = album_name,\n    `Number of Playlist Appearances` = Appearances\n  ) |&gt;\n  kable(\n    caption = \"Most Popular Tracks in Playlist Dataset\",\n    align   = \"c\"\n  )\n\n\n\nMost Popular Tracks in Playlist Dataset\n\n\n\n\n\n\n\n\nTrack Name\nArtist Name\nAlbum Name\nNumber of Playlist Appearances\n\n\n\n\nOne Dance\nDrake\nViews\n55\n\n\nHUMBLE.\nKendrick Lamar\nDAMN.\n52\n\n\nBroccoli (feat. Lil Yachty)\nDRAM\nBig Baby DRAM\n50\n\n\nCloser\nThe Chainsmokers\nCloser\n46\n\n\nCongratulations\nPost Malone\nStoney\n44\n\n\nDon’t Let Me Down\nThe Chainsmokers\nThe Chainsmokers- Japan Special Edition\n42\n\n\n\n\n\nIn the table above, we display the top songs in the playlist dataset. “One Dance” by Drake, takes top spot as it appears in 55 different playlists.\n\n\n\n\n\nCode\nplaylist_df |&gt;\n  anti_join(songs_df, by = c(\"track_id\" = \"id\")) |&gt;     \n  count(\n    track_name,\n    artist_name,\n    album_name,\n    name = \"Appearances\"                                \n  ) |&gt;\n  arrange(desc(Appearances)) |&gt;                         \n  slice_head(n = 1) |&gt;                                  \n  rename(                                               \n    `Track Name`                     = track_name,\n    `Artist Name`                    = artist_name,\n    `Album Name`                     = album_name,\n    `Number of Playlist Appearances` = Appearances\n  ) |&gt;\n  kable(\n    caption = \"The Most Popular Track that is not in both Datasets\",\n    align   = \"c\"\n  )\n\n\n\nThe Most Popular Track that is not in both Datasets\n\n\n\n\n\n\n\n\nTrack Name\nArtist Name\nAlbum Name\nNumber of Playlist Appearances\n\n\n\n\nOne Dance\nDrake\nViews\n55\n\n\n\n\n\nThe song “One Dance” by Drake, does not appear in the songs characteristics data “songs_df”.\n\n\n\n\n\nCode\ntop_song &lt;- songs_df |&gt;\n  arrange(desc(danceability)) |&gt;\n  slice_head(n = 1)\n\ntop_song_id &lt;- top_song$id\nappearance_count &lt;- playlist_df |&gt;\n  filter(track_id == top_song_id) |&gt;\n  nrow()\ntop_song_table &lt;- tibble(\n  track_name            = top_song$name,\n  danceability_score    = top_song$danceability,\n  playlist_appearances  = appearance_count\n)\n\ntop_song_table\n\n\n# A tibble: 1 × 3\n  track_name        danceability_score playlist_appearances\n  &lt;chr&gt;                          &lt;dbl&gt;                &lt;int&gt;\n1 Funky Cold Medina              0.988                    1\n\n\nThe most “danceable” track, is Funky Cold Medina.\n\n\n\n\n\nCode\nplaylist_df |&gt;\n  group_by(playlist_name) |&gt;\n  summarise(\n    mean_duration_ms   = mean(duration),\n    median_duration_ms = median(duration),\n    artist_name        = first(artist_name),\n    .groups            = \"drop\"\n  ) |&gt;\n  arrange(desc(mean_duration_ms)) |&gt;\n  slice_head(n = 1) |&gt;\n  mutate(\n    `Mean duration (sec)`   = mean_duration_ms   / 1000,\n    `Median duration (sec)` = median_duration_ms / 1000\n  ) |&gt;\n  select(\n    playlist_name,\n    artist_name,\n    `Mean duration (sec)`,\n    `Median duration (sec)`\n  ) |&gt;\n  rename(\n    `Playlist Name` = playlist_name,\n    `Artist Name`   = artist_name\n  ) |&gt;\n  kable(\n    caption = \"Transit Service With Most UPT In 2023\",\n    align   = \"c\"\n  )\n\n\n\nTransit Service With Most UPT In 2023\n\n\n\n\n\n\n\n\nPlaylist Name\nArtist Name\nMean duration (sec)\nMedian duration (sec)\n\n\n\n\nclassical\nPyotr Ilyich Tchaikovsky\n411.1487\n341.42\n\n\n\n\n\nThe playlist that has the longest average duration is 411 seconds (mean). The playlist is called “Classical” and is by “Pyotr Ilyich Tchaikovsky”\n\n\n\n\n\nCode\nplaylist_df |&gt;\n  slice_max(playlist_followers, n = 1, with_ties = FALSE) |&gt;   # most‑followed playlist\n  mutate(`Duration (sec)` = duration / 1000) |&gt;                # ms → sec\n  select(\n    playlist_name,\n    playlist_position,\n    playlist_followers,\n    artist_name,\n    album_name,\n    `Duration (sec)`\n  ) |&gt;\n  rename(\n    `Playlist Name`     = playlist_name,\n    `Playlist Position` = playlist_position,\n    `Playlist Followers`= playlist_followers,\n    `Artist Name`       = artist_name,\n    `Album Name`        = album_name\n  ) |&gt;\n  kable(\n    caption = \"Most popular playlist on Spotify\",\n    align   = \"c\"\n  )\n\n\n\nMost popular playlist on Spotify\n\n\n\n\n\n\n\n\n\n\nPlaylist Name\nPlaylist Position\nPlaylist Followers\nArtist Name\nAlbum Name\nDuration (sec)\n\n\n\n\nTangled\n51259\n1038\nMandy Moore\nTangled\n152.333\n\n\n\n\n\nThe most popular playlist on Spotify is “Tangled”, with 1038 followers."
  },
  {
    "objectID": "mp03.html#introduction",
    "href": "mp03.html#introduction",
    "title": "The Ultimate Playlist",
    "section": "",
    "text": "In this project, we harness Spotify’s rich dataset to answer a simple but compelling question: what makes a song not just good, but universally loved? By tapping into two distinct data exports, one detailing track popularity metrics and another cataloguing song attributes (tempo, key, danceability, and energy)—we can move beyond subjective taste and build an evidence‑backed “ultimate playlist.”\nThis project is also insipired by the “All Rise” playlist, by Mr Barney Stinson, as the Ultimate Playtlist helps define a great aggregation of music into a single playlist based on what we love to hear most.\nBelow, we will begin with the two datasets that will be used from this project."
  },
  {
    "objectID": "mp03.html#data",
    "href": "mp03.html#data",
    "title": "The Ultimate Playlist",
    "section": "",
    "text": "Code\nload_songs &lt;- function() {\n  library(readr)\n  dir_path  &lt;- \"data/mp03\"\n  file_name &lt;- \"songs.csv\"\n  file_path &lt;- file.path(dir_path, file_name)\n\n  #checking dupolicate so that github will not block it\n  if (!dir.exists(dir_path)) {\n    dir.create(dir_path, recursive = TRUE)\n  }\n  \n  # download\n  if (!file.exists(file_path)) {\n    download.file(\n      \"https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\",\n      destfile = file_path,\n      mode = \"wb\"\n    )\n  }\n  \n  # reading in dataset\n  file_path |&gt;\n    read_csv(show_col_types = FALSE)\n}\n\nsongs_df &lt;- load_songs()\nhead(songs_df)\n\n\n# A tibble: 6 × 19\n  id      name  artists duration_ms release_date  year acousticness danceability\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 6KbQ3u… Sing… ['Carl…      158648 1928          1928        0.995        0.708\n2 6KuQTI… Fant… ['Robe…      282133 1928          1928        0.994        0.379\n3 6L63VW… Chap… ['Sewe…      104300 1928          1928        0.604        0.749\n4 6M94Fk… Beba… ['Fran…      180760 9/25/28       1928        0.995        0.781\n5 6N6tiF… Polo… ['Fréd…      687733 1928          1928        0.99         0.21 \n6 6NxAf7… Sche… ['Feli…      352600 1928          1928        0.995        0.424\n# ℹ 11 more variables: energy &lt;dbl&gt;, instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;,\n#   loudness &lt;dbl&gt;, speechiness &lt;dbl&gt;, tempo &lt;dbl&gt;, valence &lt;dbl&gt;, mode &lt;dbl&gt;,\n#   key &lt;dbl&gt;, popularity &lt;dbl&gt;, explicit &lt;dbl&gt;\n\n\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(tidyr)\nlibrary(stringr)\n\nclean_artist_string &lt;- function(x) {\n  x |&gt;\n    str_replace_all(\"\\\\['\", \"\") |&gt;\n    str_replace_all(\"'\\\\]\", \"\") |&gt;\n    str_replace_all(\"[ ]?'\", \"\") |&gt;\n    str_replace_all(\"[ ]*,[ ]*\", \",\")\n}\n\nsongs_df |&gt;\n  separate_longer_delim(artists, \",\") |&gt;\n  mutate(artist = clean_artist_string(artists)) |&gt;\n  select(-artists)\n\n\n# A tibble: 226,813 × 19\n   id      name  duration_ms release_date  year acousticness danceability energy\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n 1 6KbQ3u… Sing…      158648 1928          1928        0.995        0.708 0.195 \n 2 6KuQTI… Fant…      282133 1928          1928        0.994        0.379 0.0135\n 3 6KuQTI… Fant…      282133 1928          1928        0.994        0.379 0.0135\n 4 6L63VW… Chap…      104300 1928          1928        0.604        0.749 0.22  \n 5 6M94Fk… Beba…      180760 9/25/28       1928        0.995        0.781 0.13  \n 6 6N6tiF… Polo…      687733 1928          1928        0.99         0.21  0.204 \n 7 6N6tiF… Polo…      687733 1928          1928        0.99         0.21  0.204 \n 8 6NxAf7… Sche…      352600 1928          1928        0.995        0.424 0.12  \n 9 6NxAf7… Sche…      352600 1928          1928        0.995        0.424 0.12  \n10 6O0puP… Vals…      136627 1928          1928        0.956        0.444 0.197 \n# ℹ 226,803 more rows\n# ℹ 11 more variables: instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, loudness &lt;dbl&gt;,\n#   speechiness &lt;dbl&gt;, tempo &lt;dbl&gt;, valence &lt;dbl&gt;, mode &lt;dbl&gt;, key &lt;dbl&gt;,\n#   popularity &lt;dbl&gt;, explicit &lt;dbl&gt;, artist &lt;chr&gt;\n\n\nThanks to the github user, gabminamedez, we have a master file of a catalogue of songs that contains their respective propertiers such as name, release_date, dancebility, energy and much more that will be integral for our analysis.\n\n\nCode\nload_playlists &lt;- function(n = 10) {\n  base_url &lt;- \"https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/\"\n  dir_path &lt;- \"data/mp03/playlists\"\n  if (!dir.exists(dir_path)) dir.create(dir_path, recursive = TRUE)\n\n  playlists &lt;- list()\n\n  for (i in 0:(n - 1)) {\n    start &lt;- i * 1000\n    end &lt;- start + 999\n    file_name &lt;- sprintf(\"mpd.slice.%d-%d.json\", start, end)\n    file_url &lt;- paste0(base_url, file_name)\n    file_path &lt;- file.path(dir_path, file_name)\n\n    if (!file.exists(file_path)) {\n      message(\"Downloading: \", file_name)\n      result &lt;- tryCatch({\n        download.file(file_url, file_path, mode = \"wb\", quiet = TRUE)\n        TRUE\n      }, error = function(e) {\n        message(\"Failed to download \", file_name)\n        FALSE\n      })\n\n      if (!result) next\n    }\n\n    if (file.exists(file_path)) {\n      json_data &lt;- tryCatch({\n        jsonlite::fromJSON(file_path)\n      }, error = function(e) {\n        message(\"Failed to parse \", file_name)\n        NULL\n      })\n\n      if (!is.null(json_data)) {\n        playlists[[length(playlists) + 1]] &lt;- json_data$playlists\n      }\n    }\n  }\n\n  return(playlists)\n}\n\nif (file.exists(\"data/processed_playlists.rds\")) {\n  playlists &lt;- readRDS(\"data/processed_playlists.rds\")\n} else {\n  playlists &lt;- load_playlists(n = 10)\n  saveRDS(playlists, \"data/processed_playlists.rds\")\n}\n\n\nplaylists &lt;- load_playlists(n = 10)  \n\n\nDownloading: mpd.slice.2000-2999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.2000-2999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.2000-2999.json\n\n\nDownloading: mpd.slice.4000-4999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.4000-4999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.4000-4999.json\n\n\nDownloading: mpd.slice.5000-5999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.5000-5999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.5000-5999.json\n\n\nDownloading: mpd.slice.6000-6999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.6000-6999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.6000-6999.json\n\n\nDownloading: mpd.slice.8000-8999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.8000-8999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.8000-8999.json\n\n\nDownloading: mpd.slice.9000-9999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.9000-9999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.9000-9999.json\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\nstrip_spotify_prefix &lt;- function(x) {\n  str_extract(x, \".*:.*:(.*)\", group = 1)\n}\nplaylist_df &lt;- playlists[[1]] |&gt;\n  \n  # 1. Keep just the playlist info + the tracks column\n  mutate(\n    playlist_name      = name,\n    playlist_id        = strip_spotify_prefix(pid),\n    playlist_followers = num_followers\n  ) |&gt;\n  select(playlist_name, playlist_id, playlist_followers, tracks) |&gt;\n  unnest(tracks) |&gt;\n  \n  # These are more of the columns that were specifid we needed for this project\n  mutate(\n    playlist_position = row_number(),                 \n    artist_name       = artist_name,                   \n    artist_id         = strip_spotify_prefix(artist_uri),\n    track_name        = track_name,\n    track_id          = strip_spotify_prefix(track_uri),\n    album_name        = album_name,\n    album_id          = strip_spotify_prefix(album_uri),\n    duration          = duration_ms                    \n  ) |&gt;\n\n  select(\n    playlist_name,\n    playlist_id,\n    playlist_position,\n    playlist_followers,\n    artist_name,\n    artist_id,\n    track_name,\n    track_id,\n    album_name,\n    album_id,\n    duration\n  )\n\n# Look at the first few rows\nhead(playlist_df)\n\n\n# A tibble: 6 × 11\n  playlist_name playlist_id playlist_position playlist_followers artist_name    \n  &lt;chr&gt;         &lt;chr&gt;                   &lt;int&gt;              &lt;int&gt; &lt;chr&gt;          \n1 Throwbacks    &lt;NA&gt;                        1                  1 Missy Elliott  \n2 Throwbacks    &lt;NA&gt;                        2                  1 Britney Spears \n3 Throwbacks    &lt;NA&gt;                        3                  1 Beyoncé        \n4 Throwbacks    &lt;NA&gt;                        4                  1 Justin Timberl…\n5 Throwbacks    &lt;NA&gt;                        5                  1 Shaggy         \n6 Throwbacks    &lt;NA&gt;                        6                  1 Usher          \n# ℹ 6 more variables: artist_id &lt;chr&gt;, track_name &lt;chr&gt;, track_id &lt;chr&gt;,\n#   album_name &lt;chr&gt;, album_id &lt;chr&gt;, duration &lt;int&gt;"
  },
  {
    "objectID": "test1.html",
    "href": "test1.html",
    "title": "The Ultimate Playlist",
    "section": "",
    "text": "In this project, we harness Spotify’s rich dataset to answer a simple but compelling question: what makes a song not just good, but universally loved? By tapping into two distinct data exports, one detailing track popularity metrics and another cataloguing song attributes (tempo, key, danceability, and energy)—we can move beyond subjective taste and build an evidence‑backed “ultimate playlist.”\nThis project is also insipired by the “All Rise” playlist, by Mr Barney Stinson, as the Ultimate Playtlist helps define a great aggregation of music into a single playlist based on what we love to hear most.\nBelow, we will begin with the two datasets that will be used from this project.\n\n\n\n\nCode\nload_songs &lt;- function() {\n  library(readr)\n  dir_path  &lt;- \"data/mp03\"\n  file_name &lt;- \"songs.csv\"\n  file_path &lt;- file.path(dir_path, file_name)\n\n  #checking dupolicate so that github will not block it\n  if (!dir.exists(dir_path)) {\n    dir.create(dir_path, recursive = TRUE)\n  }\n  \n  # download\n  if (!file.exists(file_path)) {\n    download.file(\n      \"https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\",\n      destfile = file_path,\n      mode = \"wb\"\n    )\n  }\n  \n  # reading in dataset\n  file_path |&gt;\n    read_csv(show_col_types = FALSE)\n}\n\nsongs_df &lt;- load_songs()\nhead(songs_df)\n\n\n# A tibble: 6 × 19\n  id      name  artists duration_ms release_date  year acousticness danceability\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 6KbQ3u… Sing… ['Carl…      158648 1928          1928        0.995        0.708\n2 6KuQTI… Fant… ['Robe…      282133 1928          1928        0.994        0.379\n3 6L63VW… Chap… ['Sewe…      104300 1928          1928        0.604        0.749\n4 6M94Fk… Beba… ['Fran…      180760 9/25/28       1928        0.995        0.781\n5 6N6tiF… Polo… ['Fréd…      687733 1928          1928        0.99         0.21 \n6 6NxAf7… Sche… ['Feli…      352600 1928          1928        0.995        0.424\n# ℹ 11 more variables: energy &lt;dbl&gt;, instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;,\n#   loudness &lt;dbl&gt;, speechiness &lt;dbl&gt;, tempo &lt;dbl&gt;, valence &lt;dbl&gt;, mode &lt;dbl&gt;,\n#   key &lt;dbl&gt;, popularity &lt;dbl&gt;, explicit &lt;dbl&gt;"
  },
  {
    "objectID": "test1.html#data",
    "href": "test1.html#data",
    "title": "The Ultimate Playlist",
    "section": "",
    "text": "Code\nload_songs &lt;- function() {\n  library(readr)\n  dir_path  &lt;- \"data/mp03\"\n  file_name &lt;- \"songs.csv\"\n  file_path &lt;- file.path(dir_path, file_name)\n\n  #checking dupolicate so that github will not block it\n  if (!dir.exists(dir_path)) {\n    dir.create(dir_path, recursive = TRUE)\n  }\n  \n  # download\n  if (!file.exists(file_path)) {\n    download.file(\n      \"https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\",\n      destfile = file_path,\n      mode = \"wb\"\n    )\n  }\n  \n  # reading in dataset\n  file_path |&gt;\n    read_csv(show_col_types = FALSE)\n}\n\nsongs_df &lt;- load_songs()\nhead(songs_df)\n\n\n# A tibble: 6 × 19\n  id      name  artists duration_ms release_date  year acousticness danceability\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 6KbQ3u… Sing… ['Carl…      158648 1928          1928        0.995        0.708\n2 6KuQTI… Fant… ['Robe…      282133 1928          1928        0.994        0.379\n3 6L63VW… Chap… ['Sewe…      104300 1928          1928        0.604        0.749\n4 6M94Fk… Beba… ['Fran…      180760 9/25/28       1928        0.995        0.781\n5 6N6tiF… Polo… ['Fréd…      687733 1928          1928        0.99         0.21 \n6 6NxAf7… Sche… ['Feli…      352600 1928          1928        0.995        0.424\n# ℹ 11 more variables: energy &lt;dbl&gt;, instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;,\n#   loudness &lt;dbl&gt;, speechiness &lt;dbl&gt;, tempo &lt;dbl&gt;, valence &lt;dbl&gt;, mode &lt;dbl&gt;,\n#   key &lt;dbl&gt;, popularity &lt;dbl&gt;, explicit &lt;dbl&gt;"
  },
  {
    "objectID": "mptest.html",
    "href": "mptest.html",
    "title": "The Ultimate Playlist",
    "section": "",
    "text": "In this project, we harness Spotify’s rich dataset to answer a simple but compelling question: what makes a song not just good, but universally loved? By tapping into two distinct data exports, one detailing track popularity metrics and another cataloguing song attributes (tempo, key, danceability, and energy)—we can move beyond subjective taste and build an evidence‑backed “ultimate playlist.”\nThis project is also insipired by the “All Rise” playlist, by Mr Barney Stinson, as the Ultimate Playtlist helps define a great aggregation of music into a single playlist based on what we love to hear most.\nBelow, we will begin with the two datasets that will be used from this project.\n\n\n\n\nCode\nload_songs &lt;- function() {\n  library(readr)\n  dir_path  &lt;- \"data/mp03\"\n  file_name &lt;- \"songs.csv\"\n  file_path &lt;- file.path(dir_path, file_name)\n\n  #checking dupolicate so that github will not block it\n  if (!dir.exists(dir_path)) {\n    dir.create(dir_path, recursive = TRUE)\n  }\n  \n  # download\n  if (!file.exists(file_path)) {\n    download.file(\n      \"https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\",\n      destfile = file_path,\n      mode = \"wb\"\n    )\n  }\n  \n  # reading in dataset\n  file_path |&gt;\n    read_csv(show_col_types = FALSE)\n}\n\nsongs_df &lt;- load_songs()\nhead(songs_df)\n\n\n# A tibble: 6 × 19\n  id      name  artists duration_ms release_date  year acousticness danceability\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 6KbQ3u… Sing… ['Carl…      158648 1928          1928        0.995        0.708\n2 6KuQTI… Fant… ['Robe…      282133 1928          1928        0.994        0.379\n3 6L63VW… Chap… ['Sewe…      104300 1928          1928        0.604        0.749\n4 6M94Fk… Beba… ['Fran…      180760 9/25/28       1928        0.995        0.781\n5 6N6tiF… Polo… ['Fréd…      687733 1928          1928        0.99         0.21 \n6 6NxAf7… Sche… ['Feli…      352600 1928          1928        0.995        0.424\n# ℹ 11 more variables: energy &lt;dbl&gt;, instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;,\n#   loudness &lt;dbl&gt;, speechiness &lt;dbl&gt;, tempo &lt;dbl&gt;, valence &lt;dbl&gt;, mode &lt;dbl&gt;,\n#   key &lt;dbl&gt;, popularity &lt;dbl&gt;, explicit &lt;dbl&gt;\n\n\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(tidyr)\nlibrary(stringr)\n\nclean_artist_string &lt;- function(x) {\n  x |&gt;\n    str_replace_all(\"\\\\['\", \"\") |&gt;\n    str_replace_all(\"'\\\\]\", \"\") |&gt;\n    str_replace_all(\"[ ]?'\", \"\") |&gt;\n    str_replace_all(\"[ ]*,[ ]*\", \",\")\n}\n\nsongs_df |&gt;\n  separate_longer_delim(artists, \",\") |&gt;\n  mutate(artist = clean_artist_string(artists)) |&gt;\n  select(-artists)\n\n\n# A tibble: 226,813 × 19\n   id      name  duration_ms release_date  year acousticness danceability energy\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n 1 6KbQ3u… Sing…      158648 1928          1928        0.995        0.708 0.195 \n 2 6KuQTI… Fant…      282133 1928          1928        0.994        0.379 0.0135\n 3 6KuQTI… Fant…      282133 1928          1928        0.994        0.379 0.0135\n 4 6L63VW… Chap…      104300 1928          1928        0.604        0.749 0.22  \n 5 6M94Fk… Beba…      180760 9/25/28       1928        0.995        0.781 0.13  \n 6 6N6tiF… Polo…      687733 1928          1928        0.99         0.21  0.204 \n 7 6N6tiF… Polo…      687733 1928          1928        0.99         0.21  0.204 \n 8 6NxAf7… Sche…      352600 1928          1928        0.995        0.424 0.12  \n 9 6NxAf7… Sche…      352600 1928          1928        0.995        0.424 0.12  \n10 6O0puP… Vals…      136627 1928          1928        0.956        0.444 0.197 \n# ℹ 226,803 more rows\n# ℹ 11 more variables: instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, loudness &lt;dbl&gt;,\n#   speechiness &lt;dbl&gt;, tempo &lt;dbl&gt;, valence &lt;dbl&gt;, mode &lt;dbl&gt;, key &lt;dbl&gt;,\n#   popularity &lt;dbl&gt;, explicit &lt;dbl&gt;, artist &lt;chr&gt;\n\n\nThanks to the github user, gabminamedez, we have a master file of a catalogue of songs that contains their respective propertiers such as name, release_date, dancebility, energy and much more that will be integral for our analysis.\n\n\nCode\nload_playlists &lt;- function(n = 10) {\n  base_url &lt;- \"https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/\"\n  dir_path &lt;- \"data/mp03/playlists\"\n  if (!dir.exists(dir_path)) dir.create(dir_path, recursive = TRUE)\n\n  playlists &lt;- list()\n\n  for (i in 0:(n - 1)) {\n    start &lt;- i * 1000\n    end &lt;- start + 999\n    file_name &lt;- sprintf(\"mpd.slice.%d-%d.json\", start, end)\n    file_url &lt;- paste0(base_url, file_name)\n    file_path &lt;- file.path(dir_path, file_name)\n\n    if (!file.exists(file_path)) {\n      message(\"Downloading: \", file_name)\n      result &lt;- tryCatch({\n        download.file(file_url, file_path, mode = \"wb\", quiet = TRUE)\n        TRUE\n      }, error = function(e) {\n        message(\"Failed to download \", file_name)\n        FALSE\n      })\n\n      if (!result) next\n    }\n\n    if (file.exists(file_path)) {\n      json_data &lt;- tryCatch({\n        jsonlite::fromJSON(file_path)\n      }, error = function(e) {\n        message(\"Failed to parse \", file_name)\n        NULL\n      })\n\n      if (!is.null(json_data)) {\n        playlists[[length(playlists) + 1]] &lt;- json_data$playlists\n      }\n    }\n  }\n\n  return(playlists)\n}\n\nif (file.exists(\"data/processed_playlists.rds\")) {\n  playlists &lt;- readRDS(\"data/processed_playlists.rds\")\n} else {\n  playlists &lt;- load_playlists(n = 10)\n  saveRDS(playlists, \"data/processed_playlists.rds\")\n}\n\n\nplaylists &lt;- load_playlists(n = 10)  \n\n\nDownloading: mpd.slice.2000-2999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.2000-2999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.2000-2999.json\n\n\nDownloading: mpd.slice.4000-4999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.4000-4999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.4000-4999.json\n\n\nDownloading: mpd.slice.5000-5999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.5000-5999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.5000-5999.json\n\n\nDownloading: mpd.slice.6000-6999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.6000-6999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.6000-6999.json\n\n\nDownloading: mpd.slice.8000-8999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.8000-8999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.8000-8999.json\n\n\nDownloading: mpd.slice.9000-9999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.9000-9999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.9000-9999.json\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\nstrip_spotify_prefix &lt;- function(x) {\n  # Drop everything up to the last colon\n  str_extract(x, \".*:.*:(.*)\", group = 1)\n}\n\n# Turn the first slice of playlists into one flat table\nrectangular_tracks_df &lt;- playlists[[1]] |&gt;\n  \n  # 1. Keep just the playlist info + the tracks column\n  mutate(\n    playlist_name      = name,\n    playlist_id        = strip_spotify_prefix(pid),\n    playlist_followers = num_followers\n  ) |&gt;\n  select(playlist_name, playlist_id, playlist_followers, tracks) |&gt;\n  \n  # 2. Unnest so each track is its own row\n  unnest(tracks) |&gt;\n  \n  # 3. Clean up and make new columns for each detail\n  mutate(\n    playlist_position = row_number(),                  # track order\n    artist_name       = artist_name,                   # from the JSON\n    artist_id         = strip_spotify_prefix(artist_uri),\n    track_name        = track_name,\n    track_id          = strip_spotify_prefix(track_uri),\n    album_name        = album_name,\n    album_id          = strip_spotify_prefix(album_uri),\n    duration          = duration_ms                    # in milliseconds\n  ) |&gt;\n  \n  # 4. Pick columns in the exact order you want\n  select(\n    playlist_name,\n    playlist_id,\n    playlist_position,\n    playlist_followers,\n    artist_name,\n    artist_id,\n    track_name,\n    track_id,\n    album_name,\n    album_id,\n    duration\n  )\n\n# Look at the first few rows\nhead(rectangular_tracks_df)\n\n\n# A tibble: 6 × 11\n  playlist_name playlist_id playlist_position playlist_followers artist_name    \n  &lt;chr&gt;         &lt;chr&gt;                   &lt;int&gt;              &lt;int&gt; &lt;chr&gt;          \n1 Throwbacks    &lt;NA&gt;                        1                  1 Missy Elliott  \n2 Throwbacks    &lt;NA&gt;                        2                  1 Britney Spears \n3 Throwbacks    &lt;NA&gt;                        3                  1 Beyoncé        \n4 Throwbacks    &lt;NA&gt;                        4                  1 Justin Timberl…\n5 Throwbacks    &lt;NA&gt;                        5                  1 Shaggy         \n6 Throwbacks    &lt;NA&gt;                        6                  1 Usher          \n# ℹ 6 more variables: artist_id &lt;chr&gt;, track_name &lt;chr&gt;, track_id &lt;chr&gt;,\n#   album_name &lt;chr&gt;, album_id &lt;chr&gt;, duration &lt;int&gt;\n\n\n\n\n\nHow many distinct tracks and artists are represented in the playlist data?\n\n\nCode\nlibrary(dplyr)\n\nrectangular_tracks_df |&gt;\n  summarise(\n    distinct_tracks  = n_distinct(track_id),\n    distinct_artists = n_distinct(artist_id)\n  )\n\n\n# A tibble: 1 × 2\n  distinct_tracks distinct_artists\n            &lt;int&gt;            &lt;int&gt;\n1           34443             9754\n\n\nWhat are the 5 most popular tracks in the playlist data?\n\n\nCode\nlibrary(dplyr)\nmost_pop_tracks &lt;- rectangular_tracks_df |&gt;\n\n  count(\n    track_name,\n    artist_name,\n    album_name,\n    name = \"The number of appearances in playlists\"\n  ) |&gt;\n  arrange(desc(`The number of appearances in playlists`))\n\nhead(most_pop_tracks)\n\n\n# A tibble: 6 × 4\n  track_name                  artist_name      album_name The number of appear…¹\n  &lt;chr&gt;                       &lt;chr&gt;            &lt;chr&gt;                       &lt;int&gt;\n1 One Dance                   Drake            Views                          55\n2 HUMBLE.                     Kendrick Lamar   DAMN.                          52\n3 Broccoli (feat. Lil Yachty) DRAM             Big Baby …                     50\n4 Closer                      The Chainsmokers Closer                         46\n5 Congratulations             Post Malone      Stoney                         44\n6 Don't Let Me Down           The Chainsmokers The Chain…                     42\n# ℹ abbreviated name: ¹​`The number of appearances in playlists`\n\n\nWhat is the most popular track in the playlist data that does not have a corresponding entry in the song characteristics data?\n\n\nCode\nrectangular_tracks_df |&gt;\n  # 1. Filter out any tracks that *do* appear in songs_df\n  anti_join(songs_df, by = c(\"track_id\" = \"id\")) |&gt;\n  \n  # 2. Count by track name, artist, and album\n  count(\n    track_name,\n    artist_name,\n    album_name,\n    name = \"count\"\n  ) |&gt;\n  \n  # 3. Order by descending count\n  arrange(desc(count)) |&gt;\n  \n  # 4. Take just the top one\n  slice_head(n = 1)\n\n\n# A tibble: 1 × 4\n  track_name artist_name album_name count\n  &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;      &lt;int&gt;\n1 One Dance  Drake       Views         55\n\n\nAccording to the song characteristics data, what is the most “danceable” track? How often does it appear in a playlist?\n\n\nCode\nlibrary(dplyr)\n\n# 1. Find the single most danceable song\ntop_song &lt;- songs_df |&gt;\n  arrange(desc(danceability)) |&gt;\n  head(1)\n\n# Pull out its ID, name, and danceability score\ntop_id    &lt;- top_song$id\ntop_name  &lt;- top_song$name\ntop_score &lt;- top_song$danceability\n\n# 2. Count how many times that song appears in the playlists\nappearance_count &lt;- rectangular_tracks_df |&gt;\n  filter(track_id == top_id) |&gt;\n  nrow()\n\n# 3. Show the results\nprint(paste(\"Most danceable track:\", top_name))\n\n\n[1] \"Most danceable track: Funky Cold Medina\"\n\n\nCode\nprint(paste(\"Danceability score:\", top_score))\n\n\n[1] \"Danceability score: 0.988\"\n\n\nCode\nprint(paste(\"Number of playlist appearances:\", appearance_count))\n\n\n[1] \"Number of playlist appearances: 1\"\n\n\nWhich playlist has the longest average track length?\n\n\nCode\n# Find the playlist with the longest mean track duration,\n# and show both mean and median durations (in seconds).\nlongest_playlist &lt;- rectangular_tracks_df |&gt;\n  group_by(playlist_name) |&gt;\n  summarise(\n    mean_duration_ms   = mean(duration),\n    median_duration_ms = median(duration),\n    artist_name        = first(artist_name),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(mean_duration_ms)) |&gt;\n  slice_head(n = 1) |&gt;\n  # Convert from milliseconds to seconds\n  mutate(\n    mean_duration_sec   = mean_duration_ms   / 1000,\n    median_duration_sec = median_duration_ms / 1000\n  ) |&gt;\n  # Select and rename the columns we want\n  select(\n    playlist_name,\n    artist_name,\n    `Mean duration (sec)`   = mean_duration_sec,\n    `Median duration (sec)` = median_duration_sec\n  )\n\nprint(longest_playlist)\n\n\n# A tibble: 1 × 4\n  playlist_name artist_name         `Mean duration (sec)` Median duration (sec…¹\n  &lt;chr&gt;         &lt;chr&gt;                               &lt;dbl&gt;                  &lt;dbl&gt;\n1 classical     Pyotr Ilyich Tchai…                  411.                   341.\n# ℹ abbreviated name: ¹​`Median duration (sec)`\n\n\nWhat is the most popular playlist on Spotify?\n\n\nCode\nmost_popular_playlist &lt;- rectangular_tracks_df|&gt;\n  slice_max(playlist_followers) |&gt; \nslice_head(n = 1)\nmost_popular_playlist\n\n\n# A tibble: 1 × 11\n  playlist_name playlist_id playlist_position playlist_followers artist_name\n  &lt;chr&gt;         &lt;chr&gt;                   &lt;int&gt;              &lt;int&gt; &lt;chr&gt;      \n1 Tangled       &lt;NA&gt;                    51259               1038 Mandy Moore\n# ℹ 6 more variables: artist_id &lt;chr&gt;, track_name &lt;chr&gt;, track_id &lt;chr&gt;,\n#   album_name &lt;chr&gt;, album_id &lt;chr&gt;, duration &lt;int&gt;\n\n\nTask 5: Visually Identifying Characteristics of Popular Songs\ninner join the two datasets\n\n\nCode\ninner_joined_data &lt;- songs_df |&gt;\n  # rename the song‐data 'id' column so it matches\n  rename(track_id = id) |&gt;\n  # now do the join by track_id\n  inner_join(rectangular_tracks_df, by = \"track_id\")\n\nhead(inner_joined_data)\n\n\n# A tibble: 6 × 29\n  track_id             name  artists duration_ms release_date  year acousticness\n  &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 1J5kO1Iyuo9GUkKpVOL… \"Mis… ['Sonn…      564320 1957          1957        0.766\n2 5F0vhf0bfnmtMW2aWsx… \"Unc… ['Sonn…      235400 1965          1965        0.282\n3 5jOO4HOAqTR9KC3HfHG… \"Let… ['Jeff…      214427 8/15/66       1966        0.161\n4 6QmMCtfu6uHG1TbWHe0… \"Rai… ['John…      310800 1970          1970        0.806\n5 5FnlmowF3elDo9Psgw1… \"Old… ['Aret…      220201 1972          1972        0.365\n6 43l4wsDQHRa0lZwwuzS… \"Nev… ['Bob …      171773 1/17/74       1974        0.583\n# ℹ 22 more variables: danceability &lt;dbl&gt;, energy &lt;dbl&gt;,\n#   instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, loudness &lt;dbl&gt;, speechiness &lt;dbl&gt;,\n#   tempo &lt;dbl&gt;, valence &lt;dbl&gt;, mode &lt;dbl&gt;, key &lt;dbl&gt;, popularity &lt;dbl&gt;,\n#   explicit &lt;dbl&gt;, playlist_name &lt;chr&gt;, playlist_id &lt;chr&gt;,\n#   playlist_position &lt;int&gt;, playlist_followers &lt;int&gt;, artist_name &lt;chr&gt;,\n#   artist_id &lt;chr&gt;, track_name &lt;chr&gt;, album_name &lt;chr&gt;, album_id &lt;chr&gt;,\n#   duration &lt;int&gt;\n\n\nNEED TO FIX Is the popularity column correlated with the number of playlist appearances? If so, to what degree?\n\n\nCode\nlibrary(ggplot2)\n\n# 1. Count how many times each track appears in all playlists\ntrack_counts &lt;- rectangular_tracks_df |&gt;\n  count(track_id, name = \"play_count\")\n\n# 2. Join that to your song characteristics (make sure track_id lines up)\njoined &lt;- songs_df |&gt;\n  rename(track_id = id) |&gt;\n  inner_join(track_counts, by = \"track_id\")\n\n# 3. Compute Pearson correlation\ncorr_coef &lt;- cor(joined$popularity, joined$play_count)\nprint(paste(\"Pearson correlation:\", round(corr_coef, 3)))\n\n\n[1] \"Pearson correlation: 0.488\"\n\n\nCode\n# e.g. “Pearson correlation: 0.42”\n\n# 4. Scatterplot: Popularity vs. Playlist Appearances\njoined |&gt;\n  ggplot(aes(x = play_count, y = popularity)) +\n  geom_point(alpha = 0.4, size = 2, color = \"#2C3E50\") +\n  geom_smooth(method = \"lm\", color = \"#E74C3C\", se = FALSE) +\n  scale_x_log10() +\n  labs(\n    title = \"Spotify Popularity vs. # of Playlist Appearances\",\n    subtitle = paste0(\"Pearson r = \", round(corr_coef, 2)),\n    x = \"Playlist Appearances (log₁₀ scale)\",\n    y = \"Spotify Popularity (0–100)\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCode\n# 5. Choose a “popular” cutoff and inspect borderline cases\npop_cutoff &lt;- 70\n\n\nfix In what year were the most popular songs released?\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# 1. Filter to songs from 1950 onward,\n#    then compute the median popularity by year\nyear_popularity &lt;- songs_df |&gt;\n  filter(year &gt;= 1950) |&gt;\n  group_by(year) |&gt;\n  summarise(\n    median_popularity = mean(popularity),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(median_popularity))\n\nhead(year_popularity)\n\n\n# A tibble: 6 × 2\n   year median_popularity\n  &lt;dbl&gt;             &lt;dbl&gt;\n1  2019              69.7\n2  2018              67.3\n3  2017              64.9\n4  2020              63.1\n5  2016              61.4\n6  2015              59.5\n\n\nCode\n# 3. Plot year vs. median popularity\nyear_popularity |&gt;\n  ggplot(aes(x = year, y = median_popularity)) +\n  geom_point(size = 3, color = \"#2C3E50\") +\n  geom_line(color = \"#E74C3C\", size = 1) +\n  scale_x_continuous(breaks = seq(1950, max(year_popularity$year), by = 4)) +\n  labs(\n    title    = \"Median Spotify Popularity by Release Year (1950+)\",\n    x        = \"Release Year\",\n    y        = \"Median Popularity (0–100)\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 1. Compute mean danceability for each year\nyear_dance &lt;- songs_df |&gt;\n  group_by(year) |&gt;\n  summarise(\n    mean_danceability = mean(danceability),\n    .groups = \"drop\"\n  )\n\n# 2. Find the year where mean danceability is highest\npeak_year &lt;- year_dance |&gt;\n  arrange(desc(mean_danceability)) |&gt;\n  slice_head(n = 1)\n\nprint(peak_year)\n\n\n# A tibble: 1 × 2\n   year mean_danceability\n  &lt;dbl&gt;             &lt;dbl&gt;\n1  2020             0.673\n\n\nCode\n# This tells you the single year with the highest average danceability.\n\n# 3. Plot mean danceability by year as a line graph\nyear_dance |&gt;\n  ggplot(aes(x = year, y = mean_danceability)) +\n  geom_line(size = 1) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Trend Of Danceability by Release Year\",\n    x     = \"Release Year\",\n    y     = \"Danceability\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n\nWhich decade is most represented on user playlists? (The integer division (%/%) operator may be useful for computing decades from years.)\n\n\nCode\nlibrary(dplyr)\n\n# 1. Using the existing inner_joined_data (which has 'year'), compute each track’s decade\n# 2. Count how many playlist appearances come from each decade\n# 3. Find the decade with the highest count\n\nmost_represented_decade &lt;- inner_joined_data |&gt;\n  mutate(decade = (year %/% 10) * 10) |&gt;\n  count(decade, name = \"playlist_count\") |&gt;\n  arrange(desc(playlist_count)) |&gt;\n  slice_head(n = 5)\n\n# Show the result\nprint(most_represented_decade)\n\n\n# A tibble: 5 × 2\n  decade playlist_count\n   &lt;dbl&gt;          &lt;int&gt;\n1   2010          17022\n2   2000           5899\n3   1990           2582\n4   1980           1447\n5   1970           1431\n\n\nCreate a plot of key frequency among songs. Because musical keys exist in a ‘cycle’, your plot should use polar (circular) coordinates.\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# 1. Count how many songs fall into each key\nkey_counts &lt;- inner_joined_data |&gt;\n  count(key, name = \"freq\") |&gt;\n  arrange(freq)\n\n# 2. Make a circular bar chart of key frequencies\nggplot(key_counts, aes(x = factor(key), y = freq)) +\n  geom_col(fill = \"#FF6347\", width = 0.7) +         # tomato‐red bars\n  coord_polar(theta = \"x\", start = 0) +             # switch to polar coords\n  labs(\n    title = \"Frequency of Musical Keys in Joined Data\",\n    x     = \"Key (0 = C, …, 11 = B)\",\n    y     = \"Number of Songs\"\n  ) +\n  theme_light(base_size = 14) +                     # clean background\n  theme(\n    plot.title          = element_text(face = \"bold\", hjust = 0.5),\n    axis.text.x         = element_text(size = 12),\n    panel.grid.major.y  = element_line(color = \"grey80\"),\n    panel.grid.minor    = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nWhat are the most popular track lengths? (Are short tracks, long tracks, or something in between most commonly included in user playlists?)\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# 1. Bin track lengths into categories (in minutes)\nlength_bins &lt;- inner_joined_data |&gt;\n  mutate(\n    length_min = duration / 60000,\n    length_bin = case_when(\n      length_min &lt; 2 ~ \"&lt; 2 min\",\n      length_min &lt; 4 ~ \"2–4 min\",\n      length_min &lt; 6 ~ \"4–6 min\",\n      length_min &lt; 8 ~ \"6–8 min\",\n      TRUE           ~ \"8+ min\"\n    ),\n    # ensure the factor levels stay in logical order\n    length_bin = factor(\n      length_bin,\n      levels = c(\"&lt; 2 min\", \"2–4 min\", \"4–6 min\", \"6–8 min\", \"8+ min\")\n    )\n  )\n\n# 2. Count how often each length category appears in playlists\nlength_freq &lt;- length_bins |&gt;\n  count(length_bin, name = \"freq\") |&gt;\n  arrange(desc(freq))\n\nprint(length_freq)\n\n\n# A tibble: 5 × 2\n  length_bin  freq\n  &lt;fct&gt;      &lt;int&gt;\n1 2–4 min    17791\n2 4–6 min    10403\n3 6–8 min      557\n4 8+ min       225\n5 &lt; 2 min      207\n\n\nCode\n# A tibble showing which length bin is most common\n\n# 3. Plot the distribution as a bar chart\nlength_freq |&gt;\n  ggplot(aes(x = length_bin, y = freq)) +\n  geom_col(fill = \"#1ABC9C\", width = 0.7) +\n  labs(\n    title = \"Track Lengths in User Playlists\",\n    x     = \"Track Length\",\n    y     = \"Number of Appearances\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    axis.text.x  = element_text(size = 12),\n    axis.title   = element_text(size = 14),\n    plot.title   = element_text(size = 16, face = \"bold\", hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\nPose and visually answer at least two more other exploratory questions. Do higher‑energy songs tend to be more popular on Spotify?\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\ninner_joined_data |&gt;\n  # 1. Plot energy on the x-axis and popularity on the y-axis\n  ggplot(aes(x = energy, y = popularity)) +\n  \n  # 2. Add semi‑transparent points\n  geom_point(alpha = 0.4, size = 2) +\n  \n  # 3. Overlay a linear trend line (no shaded SE band)\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#E74C3C\") +\n  \n  # 4. Labels and clean theme\n  labs(\n    title = \"Spotify Popularity vs. Song Energy\",\n    x     = \"Energy (0–1)\",\n    y     = \"Spotify Popularity (0–100)\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nDo more acoustic songs tend to be more or less popular on Spotify?\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\ninner_joined_data |&gt;\n  # 1. Scatter acousticness vs. popularity\n  ggplot(aes(x = acousticness, y = popularity)) +\n  geom_point(alpha = 0.3, size = 1.5, color = \"#2C3E50\") +\n  \n  # 2. Add a linear‐fit line in red\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#E74C3C\") +\n  \n  # 3. Labels and clean theme\n  labs(\n    title = \"Spotify Popularity vs. Acousticness\",\n    x     = \"Acousticness (0–1)\",\n    y     = \"Popularity (0–100)\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    axis.title = element_text(size = 12)\n  )\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTask 6 What other songs commonly appear on playlists along side this song? I choise Fashion Killa by A$AP Rocky\n\n\nCode\nlibrary(dplyr)\n\n# 1. Get all playlist IDs that contain “Fashion Killa”\nanchor_playlists &lt;- inner_joined_data |&gt;\n  filter(track_name == \"Fashion Killa\") |&gt;\n  pull(playlist_id) |&gt;\n  unique()\n\n# 2. Within those playlists, count every other track *and* carry along its popularity\ncooccur_songs &lt;- inner_joined_data |&gt;\n  filter(\n    playlist_id %in% anchor_playlists,\n    track_name   != \"Fashion Killa\"\n  ) |&gt;\n  count(\n    track_name,\n    artist_name,\n    popularity,               # include popularity here\n    name = \"cooccurrence_count\"\n  ) |&gt;\n  arrange(desc(cooccurrence_count))\n\n# 3. Show the top 20\nhead(cooccur_songs, 20)\n\n\n# A tibble: 20 × 4\n   track_name                          artist_name popularity cooccurrence_count\n   &lt;chr&gt;                               &lt;chr&gt;            &lt;dbl&gt;              &lt;int&gt;\n 1 HUMBLE.                             Kendrick L…         83                 52\n 2 Closer                              The Chains…         84                 46\n 3 Congratulations                     Post Malone         83                 44\n 4 Bounce Back                         Big Sean            75                 39\n 5 Jumpman                             Drake               74                 39\n 6 iSpy (feat. Lil Yachty)             KYLE                77                 39\n 7 Bad and Boujee (feat. Lil Uzi Vert) Migos               76                 38\n 8 Mask Off                            Future              81                 38\n 9 XO TOUR Llif3                       Lil Uzi Ve…         84                 37\n10 White Iverson                       Post Malone         79                 36\n11 Caroline                            Aminé               76                 35\n12 goosebumps                          Travis Sco…         92                 35\n13 No Role Modelz                      J. Cole             82                 34\n14 Location                            Khalid              81                 33\n15 Trap Queen                          Fetty Wap           75                 33\n16 Work from Home                      Fifth Harm…         78                 33\n17 Ni**as In Paris                     JAY Z               61                 32\n18 Black Beatles                       Rae Sremmu…         75                 31\n19 No Problem (feat. Lil Wayne & 2 Ch… Chance The…         73                 31\n20 F**kin' Problems                    A$AP Rocky          76                 30\n\n\nWhat other songs are in the same key2 and have a similar tempo? (This makes it easy for a skilled DJ to transition from one song to the next.)\n\n\nCode\nlibrary(dplyr)\nlibrary(dplyr)\n\n# 1. Get Fashion Killa’s key and tempo\nfk_info &lt;- inner_joined_data |&gt;\n  filter(track_name == \"Fashion Killa\") |&gt;\n  slice_head(n = 1) |&gt;\n  select(fk_key = key, fk_tempo = tempo)\n\nfk_key   &lt;- fk_info$fk_key\nfk_tempo &lt;- fk_info$fk_tempo\n\n# 2. Find other tracks in that same key & within ±5 BPM, and include popularity\nsimilar_songs &lt;- inner_joined_data |&gt;\n  filter(\n    key == fk_key,\n    abs(tempo - fk_tempo) &lt;= 5,\n    track_name != \"Fashion Killa\"\n  ) |&gt;\n  distinct(\n    track_name,\n    artist_name,\n    album_name,\n    tempo,\n    popularity\n  ) |&gt;\n  arrange(abs(tempo - fk_tempo)) |&gt;\n  slice_head(n = 30)\n\n# 3. View them\nsimilar_songs\n\n\n# A tibble: 30 × 5\n   track_name                            artist_name album_name tempo popularity\n   &lt;chr&gt;                                 &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 See My Tears                          Machine Gu… Lace Up     140.         52\n 2 One I Want (feat. PARTYNEXTDOOR)      Majid Jord… The Space…  140.         59\n 3 guidance                              Travis Sco… Birds In …  140.         62\n 4 I'm a Thug                            Trick Daddy THUGS ARE…  140.         58\n 5 Spaceman - Carnage Festival Trap Rem… Hardwell    Spaceman    140.         53\n 6 F*ck Up Some Commas                   Future      DS2         140.         67\n 7 I Get The Bag (feat. Migos)           Gucci Mane  Mr. Davis   140.         73\n 8 Revenge                               XXXTENTACI… 17          140.         76\n 9 Work Hard, Play Hard                  Wiz Khalifa O.N.I.F.C.  140.         62\n10 One Day They'll Know (ODESZA Remix)   Pretty Lig… A Color M…  140.         56\n# ℹ 20 more rows\n\n\nWhat other songs were released by the same artist?\n\n\nCode\nlibrary(dplyr)\n\n# 1. Find the artist of your anchor song\nanchor_artist &lt;- inner_joined_data |&gt;\n  filter(track_name == \"Fashion Killa\") |&gt;\n  pull(artist_name) |&gt;\n  unique()\n\n# 2. Get all the other tracks by that same artist\nother_songs &lt;- inner_joined_data |&gt;\n  filter(\n    artist_name == anchor_artist,        # same artist\n    track_name   != \"Fashion Killa\"      # not the anchor itself\n  ) |&gt;\n  distinct(track_name, album_name) |&gt;    # one row per song\n  arrange(track_name)                    # sort alphabetically\n\n# 3. View the result\nother_songs\n\n\n# A tibble: 26 × 2\n   track_name       album_name                     \n   &lt;chr&gt;            &lt;chr&gt;                          \n 1 1Train           LONG.LIVE.A$AP (Deluxe Version)\n 2 Angels           LONG.LIVE.A$AP (Deluxe Version)\n 3 Better Things    AT.LONG.LAST.A$AP              \n 4 Canal St.        AT.LONG.LAST.A$AP              \n 5 Electric Body    AT.LONG.LAST.A$AP              \n 6 Everyday         AT.LONG.LAST.A$AP              \n 7 Excuse Me        AT.LONG.LAST.A$AP              \n 8 F**kin' Problems LONG.LIVE.A$AP (Deluxe Version)\n 9 Ghetto Symphony  LONG.LIVE.A$AP (Deluxe Version)\n10 Goldie           LONG.LIVE.A$AP (Deluxe Version)\n# ℹ 16 more rows\n\n\nWhat other songs were released in the same year and have similar levels of acousticness, danceability, etc.?\n\n\nCode\nlibrary(dplyr)\n\n# 1. Get Fashion Killa’s stats once\nanchor &lt;- inner_joined_data |&gt;\n  filter(track_name == \"Fashion Killa\") |&gt;\n  slice_head(n = 1)\n\nyear0 &lt;- anchor$year\nac0   &lt;- anchor$acousticness\nda0   &lt;- anchor$danceability\nen0   &lt;- anchor$energy\nli0   &lt;- anchor$liveness\n\n# 2. Pull all OTHER tracks from that same year, dedupe\ncandidates &lt;- inner_joined_data |&gt;\n  filter(\n    year      == year0,\n    track_name != \"Fashion Killa\"\n  ) |&gt;\n  distinct(track_id, .keep_all = TRUE)\n\n# 3. Compute similarity and pick top 10, selecting song title and popularity\nrecommendations &lt;- candidates |&gt;\n  mutate(\n    diff = abs(acousticness - ac0) +\n           abs(danceability - da0) +\n           abs(energy      - en0) +\n           abs(liveness    - li0)\n  ) |&gt;\n  arrange(diff) |&gt;\n  select(\n    track_name,     # song title\n    artist_name,\n    popularity,     # include popularity now\n    acousticness,\n    danceability,\n    energy,\n    liveness,\n    diff\n  ) |&gt;\n  slice_head(n = 30)\n\nprint(recommendations)\n\n\n# A tibble: 30 × 8\n   track_name   artist_name popularity acousticness danceability energy liveness\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Goldie       A$AP Rocky          66        0.225        0.697  0.848    0.493\n 2 El Inmigran… Calibre 50          50        0.403        0.797  0.758    0.623\n 3 Love Game    Eminem              55        0.359        0.766  0.941    0.585\n 4 La Hummer Y… Voz De Man…         63        0.287        0.788  0.785    0.259\n 5 Tunnel Visi… Justin Tim…         52        0.275        0.572  0.771    0.473\n 6 Guap         Big Sean            54        0.311        0.65   0.723    0.412\n 7 Freak Hoe    Speaker Kn…         54        0.1          0.794  0.871    0.384\n 8 Better Than… One Direct…         64        0.114        0.627  0.839    0.534\n 9 You're Not … Blood Oran…         60        0.189        0.74   0.798    0.305\n10 Daydreamin'  Ariana Gra…         49        0.233        0.675  0.643    0.561\n# ℹ 20 more rows\n# ℹ 1 more variable: diff &lt;dbl&gt;\n\n\nWhat other songs were released by singers with “A$AP” in their name?\n\n\nCode\n# 1. Filter for artists with \"A$AP\" in their name (excluding Fashion Killa)\n# 2. Select the song info + popularity\n# 3. Remove duplicates and sort by popularity descending\n\nasap_songs &lt;- inner_joined_data |&gt;\n  filter(\n    str_detect(artist_name, \"A\\\\$AP\"),     # keep only A$AP artists\n    track_name != \"Fashion Killa\"          # drop the anchor track\n  ) |&gt;\n  distinct(\n    artist_name,\n    track_name,\n    album_name,\n    popularity                              # include popularity now\n  ) |&gt;\n  arrange(desc(popularity))                # most popular first\n\n# 4. View the result\nasap_songs\n\n\n# A tibble: 38 × 4\n   artist_name track_name                         album_name          popularity\n   &lt;chr&gt;       &lt;chr&gt;                              &lt;chr&gt;                    &lt;dbl&gt;\n 1 A$AP Rocky  F**kin' Problems                   LONG.LIVE.A$AP (De…         76\n 2 A$AP Rocky  L$D                                AT.LONG.LAST.A$AP           76\n 3 A$AP Ferg   Plain Jane                         Still Striving              76\n 4 A$AP Rocky  Everyday                           AT.LONG.LAST.A$AP           73\n 5 A$AP Ferg   Work REMIX                         Trap Lord                   70\n 6 A$AP Mob    Yamborghini High                   Yamborghini High            69\n 7 A$AP Rocky  Wild for the Night                 LONG.LIVE.A$AP (De…         68\n 8 A$AP Rocky  Lord Pretty Flacko Jodye 2 (LPFJ2) AT.LONG.LAST.A$AP           68\n 9 A$AP Rocky  Goldie                             LONG.LIVE.A$AP (De…         66\n10 A$AP Rocky  Canal St.                          AT.LONG.LAST.A$AP           66\n# ℹ 28 more rows\n\n\nTask 7 Ultimate Playlist"
  },
  {
    "objectID": "mptest.html#data",
    "href": "mptest.html#data",
    "title": "The Ultimate Playlist",
    "section": "",
    "text": "Code\nload_songs &lt;- function() {\n  library(readr)\n  dir_path  &lt;- \"data/mp03\"\n  file_name &lt;- \"songs.csv\"\n  file_path &lt;- file.path(dir_path, file_name)\n\n  #checking dupolicate so that github will not block it\n  if (!dir.exists(dir_path)) {\n    dir.create(dir_path, recursive = TRUE)\n  }\n  \n  # download\n  if (!file.exists(file_path)) {\n    download.file(\n      \"https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\",\n      destfile = file_path,\n      mode = \"wb\"\n    )\n  }\n  \n  # reading in dataset\n  file_path |&gt;\n    read_csv(show_col_types = FALSE)\n}\n\nsongs_df &lt;- load_songs()\nhead(songs_df)\n\n\n# A tibble: 6 × 19\n  id      name  artists duration_ms release_date  year acousticness danceability\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 6KbQ3u… Sing… ['Carl…      158648 1928          1928        0.995        0.708\n2 6KuQTI… Fant… ['Robe…      282133 1928          1928        0.994        0.379\n3 6L63VW… Chap… ['Sewe…      104300 1928          1928        0.604        0.749\n4 6M94Fk… Beba… ['Fran…      180760 9/25/28       1928        0.995        0.781\n5 6N6tiF… Polo… ['Fréd…      687733 1928          1928        0.99         0.21 \n6 6NxAf7… Sche… ['Feli…      352600 1928          1928        0.995        0.424\n# ℹ 11 more variables: energy &lt;dbl&gt;, instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;,\n#   loudness &lt;dbl&gt;, speechiness &lt;dbl&gt;, tempo &lt;dbl&gt;, valence &lt;dbl&gt;, mode &lt;dbl&gt;,\n#   key &lt;dbl&gt;, popularity &lt;dbl&gt;, explicit &lt;dbl&gt;\n\n\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(tidyr)\nlibrary(stringr)\n\nclean_artist_string &lt;- function(x) {\n  x |&gt;\n    str_replace_all(\"\\\\['\", \"\") |&gt;\n    str_replace_all(\"'\\\\]\", \"\") |&gt;\n    str_replace_all(\"[ ]?'\", \"\") |&gt;\n    str_replace_all(\"[ ]*,[ ]*\", \",\")\n}\n\nsongs_df |&gt;\n  separate_longer_delim(artists, \",\") |&gt;\n  mutate(artist = clean_artist_string(artists)) |&gt;\n  select(-artists)\n\n\n# A tibble: 226,813 × 19\n   id      name  duration_ms release_date  year acousticness danceability energy\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n 1 6KbQ3u… Sing…      158648 1928          1928        0.995        0.708 0.195 \n 2 6KuQTI… Fant…      282133 1928          1928        0.994        0.379 0.0135\n 3 6KuQTI… Fant…      282133 1928          1928        0.994        0.379 0.0135\n 4 6L63VW… Chap…      104300 1928          1928        0.604        0.749 0.22  \n 5 6M94Fk… Beba…      180760 9/25/28       1928        0.995        0.781 0.13  \n 6 6N6tiF… Polo…      687733 1928          1928        0.99         0.21  0.204 \n 7 6N6tiF… Polo…      687733 1928          1928        0.99         0.21  0.204 \n 8 6NxAf7… Sche…      352600 1928          1928        0.995        0.424 0.12  \n 9 6NxAf7… Sche…      352600 1928          1928        0.995        0.424 0.12  \n10 6O0puP… Vals…      136627 1928          1928        0.956        0.444 0.197 \n# ℹ 226,803 more rows\n# ℹ 11 more variables: instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, loudness &lt;dbl&gt;,\n#   speechiness &lt;dbl&gt;, tempo &lt;dbl&gt;, valence &lt;dbl&gt;, mode &lt;dbl&gt;, key &lt;dbl&gt;,\n#   popularity &lt;dbl&gt;, explicit &lt;dbl&gt;, artist &lt;chr&gt;\n\n\nThanks to the github user, gabminamedez, we have a master file of a catalogue of songs that contains their respective propertiers such as name, release_date, dancebility, energy and much more that will be integral for our analysis.\n\n\nCode\nload_playlists &lt;- function(n = 10) {\n  base_url &lt;- \"https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/\"\n  dir_path &lt;- \"data/mp03/playlists\"\n  if (!dir.exists(dir_path)) dir.create(dir_path, recursive = TRUE)\n\n  playlists &lt;- list()\n\n  for (i in 0:(n - 1)) {\n    start &lt;- i * 1000\n    end &lt;- start + 999\n    file_name &lt;- sprintf(\"mpd.slice.%d-%d.json\", start, end)\n    file_url &lt;- paste0(base_url, file_name)\n    file_path &lt;- file.path(dir_path, file_name)\n\n    if (!file.exists(file_path)) {\n      message(\"Downloading: \", file_name)\n      result &lt;- tryCatch({\n        download.file(file_url, file_path, mode = \"wb\", quiet = TRUE)\n        TRUE\n      }, error = function(e) {\n        message(\"Failed to download \", file_name)\n        FALSE\n      })\n\n      if (!result) next\n    }\n\n    if (file.exists(file_path)) {\n      json_data &lt;- tryCatch({\n        jsonlite::fromJSON(file_path)\n      }, error = function(e) {\n        message(\"Failed to parse \", file_name)\n        NULL\n      })\n\n      if (!is.null(json_data)) {\n        playlists[[length(playlists) + 1]] &lt;- json_data$playlists\n      }\n    }\n  }\n\n  return(playlists)\n}\n\nif (file.exists(\"data/processed_playlists.rds\")) {\n  playlists &lt;- readRDS(\"data/processed_playlists.rds\")\n} else {\n  playlists &lt;- load_playlists(n = 10)\n  saveRDS(playlists, \"data/processed_playlists.rds\")\n}\n\n\nplaylists &lt;- load_playlists(n = 10)  \n\n\nDownloading: mpd.slice.2000-2999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.2000-2999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.2000-2999.json\n\n\nDownloading: mpd.slice.4000-4999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.4000-4999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.4000-4999.json\n\n\nDownloading: mpd.slice.5000-5999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.5000-5999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.5000-5999.json\n\n\nDownloading: mpd.slice.6000-6999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.6000-6999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.6000-6999.json\n\n\nDownloading: mpd.slice.8000-8999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.8000-8999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.8000-8999.json\n\n\nDownloading: mpd.slice.9000-9999.json\n\n\nWarning in download.file(file_url, file_path, mode = \"wb\", quiet = TRUE):\ncannot open URL\n'https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/mpd.slice.9000-9999.json':\nHTTP status was '404 Not Found'\n\n\nFailed to download mpd.slice.9000-9999.json\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\nstrip_spotify_prefix &lt;- function(x) {\n  # Drop everything up to the last colon\n  str_extract(x, \".*:.*:(.*)\", group = 1)\n}\n\n# Turn the first slice of playlists into one flat table\nrectangular_tracks_df &lt;- playlists[[1]] |&gt;\n  \n  # 1. Keep just the playlist info + the tracks column\n  mutate(\n    playlist_name      = name,\n    playlist_id        = strip_spotify_prefix(pid),\n    playlist_followers = num_followers\n  ) |&gt;\n  select(playlist_name, playlist_id, playlist_followers, tracks) |&gt;\n  \n  # 2. Unnest so each track is its own row\n  unnest(tracks) |&gt;\n  \n  # 3. Clean up and make new columns for each detail\n  mutate(\n    playlist_position = row_number(),                  # track order\n    artist_name       = artist_name,                   # from the JSON\n    artist_id         = strip_spotify_prefix(artist_uri),\n    track_name        = track_name,\n    track_id          = strip_spotify_prefix(track_uri),\n    album_name        = album_name,\n    album_id          = strip_spotify_prefix(album_uri),\n    duration          = duration_ms                    # in milliseconds\n  ) |&gt;\n  \n  # 4. Pick columns in the exact order you want\n  select(\n    playlist_name,\n    playlist_id,\n    playlist_position,\n    playlist_followers,\n    artist_name,\n    artist_id,\n    track_name,\n    track_id,\n    album_name,\n    album_id,\n    duration\n  )\n\n# Look at the first few rows\nhead(rectangular_tracks_df)\n\n\n# A tibble: 6 × 11\n  playlist_name playlist_id playlist_position playlist_followers artist_name    \n  &lt;chr&gt;         &lt;chr&gt;                   &lt;int&gt;              &lt;int&gt; &lt;chr&gt;          \n1 Throwbacks    &lt;NA&gt;                        1                  1 Missy Elliott  \n2 Throwbacks    &lt;NA&gt;                        2                  1 Britney Spears \n3 Throwbacks    &lt;NA&gt;                        3                  1 Beyoncé        \n4 Throwbacks    &lt;NA&gt;                        4                  1 Justin Timberl…\n5 Throwbacks    &lt;NA&gt;                        5                  1 Shaggy         \n6 Throwbacks    &lt;NA&gt;                        6                  1 Usher          \n# ℹ 6 more variables: artist_id &lt;chr&gt;, track_name &lt;chr&gt;, track_id &lt;chr&gt;,\n#   album_name &lt;chr&gt;, album_id &lt;chr&gt;, duration &lt;int&gt;"
  },
  {
    "objectID": "mptest.html#initial-data-exploration",
    "href": "mptest.html#initial-data-exploration",
    "title": "The Ultimate Playlist",
    "section": "",
    "text": "How many distinct tracks and artists are represented in the playlist data?\n\n\nCode\nlibrary(dplyr)\n\nrectangular_tracks_df |&gt;\n  summarise(\n    distinct_tracks  = n_distinct(track_id),\n    distinct_artists = n_distinct(artist_id)\n  )\n\n\n# A tibble: 1 × 2\n  distinct_tracks distinct_artists\n            &lt;int&gt;            &lt;int&gt;\n1           34443             9754\n\n\nWhat are the 5 most popular tracks in the playlist data?\n\n\nCode\nlibrary(dplyr)\nmost_pop_tracks &lt;- rectangular_tracks_df |&gt;\n\n  count(\n    track_name,\n    artist_name,\n    album_name,\n    name = \"The number of appearances in playlists\"\n  ) |&gt;\n  arrange(desc(`The number of appearances in playlists`))\n\nhead(most_pop_tracks)\n\n\n# A tibble: 6 × 4\n  track_name                  artist_name      album_name The number of appear…¹\n  &lt;chr&gt;                       &lt;chr&gt;            &lt;chr&gt;                       &lt;int&gt;\n1 One Dance                   Drake            Views                          55\n2 HUMBLE.                     Kendrick Lamar   DAMN.                          52\n3 Broccoli (feat. Lil Yachty) DRAM             Big Baby …                     50\n4 Closer                      The Chainsmokers Closer                         46\n5 Congratulations             Post Malone      Stoney                         44\n6 Don't Let Me Down           The Chainsmokers The Chain…                     42\n# ℹ abbreviated name: ¹​`The number of appearances in playlists`\n\n\nWhat is the most popular track in the playlist data that does not have a corresponding entry in the song characteristics data?\n\n\nCode\nrectangular_tracks_df |&gt;\n  # 1. Filter out any tracks that *do* appear in songs_df\n  anti_join(songs_df, by = c(\"track_id\" = \"id\")) |&gt;\n  \n  # 2. Count by track name, artist, and album\n  count(\n    track_name,\n    artist_name,\n    album_name,\n    name = \"count\"\n  ) |&gt;\n  \n  # 3. Order by descending count\n  arrange(desc(count)) |&gt;\n  \n  # 4. Take just the top one\n  slice_head(n = 1)\n\n\n# A tibble: 1 × 4\n  track_name artist_name album_name count\n  &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;      &lt;int&gt;\n1 One Dance  Drake       Views         55\n\n\nAccording to the song characteristics data, what is the most “danceable” track? How often does it appear in a playlist?\n\n\nCode\nlibrary(dplyr)\n\n# 1. Find the single most danceable song\ntop_song &lt;- songs_df |&gt;\n  arrange(desc(danceability)) |&gt;\n  head(1)\n\n# Pull out its ID, name, and danceability score\ntop_id    &lt;- top_song$id\ntop_name  &lt;- top_song$name\ntop_score &lt;- top_song$danceability\n\n# 2. Count how many times that song appears in the playlists\nappearance_count &lt;- rectangular_tracks_df |&gt;\n  filter(track_id == top_id) |&gt;\n  nrow()\n\n# 3. Show the results\nprint(paste(\"Most danceable track:\", top_name))\n\n\n[1] \"Most danceable track: Funky Cold Medina\"\n\n\nCode\nprint(paste(\"Danceability score:\", top_score))\n\n\n[1] \"Danceability score: 0.988\"\n\n\nCode\nprint(paste(\"Number of playlist appearances:\", appearance_count))\n\n\n[1] \"Number of playlist appearances: 1\"\n\n\nWhich playlist has the longest average track length?\n\n\nCode\n# Find the playlist with the longest mean track duration,\n# and show both mean and median durations (in seconds).\nlongest_playlist &lt;- rectangular_tracks_df |&gt;\n  group_by(playlist_name) |&gt;\n  summarise(\n    mean_duration_ms   = mean(duration),\n    median_duration_ms = median(duration),\n    artist_name        = first(artist_name),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(mean_duration_ms)) |&gt;\n  slice_head(n = 1) |&gt;\n  # Convert from milliseconds to seconds\n  mutate(\n    mean_duration_sec   = mean_duration_ms   / 1000,\n    median_duration_sec = median_duration_ms / 1000\n  ) |&gt;\n  # Select and rename the columns we want\n  select(\n    playlist_name,\n    artist_name,\n    `Mean duration (sec)`   = mean_duration_sec,\n    `Median duration (sec)` = median_duration_sec\n  )\n\nprint(longest_playlist)\n\n\n# A tibble: 1 × 4\n  playlist_name artist_name         `Mean duration (sec)` Median duration (sec…¹\n  &lt;chr&gt;         &lt;chr&gt;                               &lt;dbl&gt;                  &lt;dbl&gt;\n1 classical     Pyotr Ilyich Tchai…                  411.                   341.\n# ℹ abbreviated name: ¹​`Median duration (sec)`\n\n\nWhat is the most popular playlist on Spotify?\n\n\nCode\nmost_popular_playlist &lt;- rectangular_tracks_df|&gt;\n  slice_max(playlist_followers) |&gt; \nslice_head(n = 1)\nmost_popular_playlist\n\n\n# A tibble: 1 × 11\n  playlist_name playlist_id playlist_position playlist_followers artist_name\n  &lt;chr&gt;         &lt;chr&gt;                   &lt;int&gt;              &lt;int&gt; &lt;chr&gt;      \n1 Tangled       &lt;NA&gt;                    51259               1038 Mandy Moore\n# ℹ 6 more variables: artist_id &lt;chr&gt;, track_name &lt;chr&gt;, track_id &lt;chr&gt;,\n#   album_name &lt;chr&gt;, album_id &lt;chr&gt;, duration &lt;int&gt;\n\n\nTask 5: Visually Identifying Characteristics of Popular Songs\ninner join the two datasets\n\n\nCode\ninner_joined_data &lt;- songs_df |&gt;\n  # rename the song‐data 'id' column so it matches\n  rename(track_id = id) |&gt;\n  # now do the join by track_id\n  inner_join(rectangular_tracks_df, by = \"track_id\")\n\nhead(inner_joined_data)\n\n\n# A tibble: 6 × 29\n  track_id             name  artists duration_ms release_date  year acousticness\n  &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 1J5kO1Iyuo9GUkKpVOL… \"Mis… ['Sonn…      564320 1957          1957        0.766\n2 5F0vhf0bfnmtMW2aWsx… \"Unc… ['Sonn…      235400 1965          1965        0.282\n3 5jOO4HOAqTR9KC3HfHG… \"Let… ['Jeff…      214427 8/15/66       1966        0.161\n4 6QmMCtfu6uHG1TbWHe0… \"Rai… ['John…      310800 1970          1970        0.806\n5 5FnlmowF3elDo9Psgw1… \"Old… ['Aret…      220201 1972          1972        0.365\n6 43l4wsDQHRa0lZwwuzS… \"Nev… ['Bob …      171773 1/17/74       1974        0.583\n# ℹ 22 more variables: danceability &lt;dbl&gt;, energy &lt;dbl&gt;,\n#   instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, loudness &lt;dbl&gt;, speechiness &lt;dbl&gt;,\n#   tempo &lt;dbl&gt;, valence &lt;dbl&gt;, mode &lt;dbl&gt;, key &lt;dbl&gt;, popularity &lt;dbl&gt;,\n#   explicit &lt;dbl&gt;, playlist_name &lt;chr&gt;, playlist_id &lt;chr&gt;,\n#   playlist_position &lt;int&gt;, playlist_followers &lt;int&gt;, artist_name &lt;chr&gt;,\n#   artist_id &lt;chr&gt;, track_name &lt;chr&gt;, album_name &lt;chr&gt;, album_id &lt;chr&gt;,\n#   duration &lt;int&gt;\n\n\nNEED TO FIX Is the popularity column correlated with the number of playlist appearances? If so, to what degree?\n\n\nCode\nlibrary(ggplot2)\n\n# 1. Count how many times each track appears in all playlists\ntrack_counts &lt;- rectangular_tracks_df |&gt;\n  count(track_id, name = \"play_count\")\n\n# 2. Join that to your song characteristics (make sure track_id lines up)\njoined &lt;- songs_df |&gt;\n  rename(track_id = id) |&gt;\n  inner_join(track_counts, by = \"track_id\")\n\n# 3. Compute Pearson correlation\ncorr_coef &lt;- cor(joined$popularity, joined$play_count)\nprint(paste(\"Pearson correlation:\", round(corr_coef, 3)))\n\n\n[1] \"Pearson correlation: 0.488\"\n\n\nCode\n# e.g. “Pearson correlation: 0.42”\n\n# 4. Scatterplot: Popularity vs. Playlist Appearances\njoined |&gt;\n  ggplot(aes(x = play_count, y = popularity)) +\n  geom_point(alpha = 0.4, size = 2, color = \"#2C3E50\") +\n  geom_smooth(method = \"lm\", color = \"#E74C3C\", se = FALSE) +\n  scale_x_log10() +\n  labs(\n    title = \"Spotify Popularity vs. # of Playlist Appearances\",\n    subtitle = paste0(\"Pearson r = \", round(corr_coef, 2)),\n    x = \"Playlist Appearances (log₁₀ scale)\",\n    y = \"Spotify Popularity (0–100)\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCode\n# 5. Choose a “popular” cutoff and inspect borderline cases\npop_cutoff &lt;- 70\n\n\nfix In what year were the most popular songs released?\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# 1. Filter to songs from 1950 onward,\n#    then compute the median popularity by year\nyear_popularity &lt;- songs_df |&gt;\n  filter(year &gt;= 1950) |&gt;\n  group_by(year) |&gt;\n  summarise(\n    median_popularity = mean(popularity),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(median_popularity))\n\nhead(year_popularity)\n\n\n# A tibble: 6 × 2\n   year median_popularity\n  &lt;dbl&gt;             &lt;dbl&gt;\n1  2019              69.7\n2  2018              67.3\n3  2017              64.9\n4  2020              63.1\n5  2016              61.4\n6  2015              59.5\n\n\nCode\n# 3. Plot year vs. median popularity\nyear_popularity |&gt;\n  ggplot(aes(x = year, y = median_popularity)) +\n  geom_point(size = 3, color = \"#2C3E50\") +\n  geom_line(color = \"#E74C3C\", size = 1) +\n  scale_x_continuous(breaks = seq(1950, max(year_popularity$year), by = 4)) +\n  labs(\n    title    = \"Median Spotify Popularity by Release Year (1950+)\",\n    x        = \"Release Year\",\n    y        = \"Median Popularity (0–100)\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 1. Compute mean danceability for each year\nyear_dance &lt;- songs_df |&gt;\n  group_by(year) |&gt;\n  summarise(\n    mean_danceability = mean(danceability),\n    .groups = \"drop\"\n  )\n\n# 2. Find the year where mean danceability is highest\npeak_year &lt;- year_dance |&gt;\n  arrange(desc(mean_danceability)) |&gt;\n  slice_head(n = 1)\n\nprint(peak_year)\n\n\n# A tibble: 1 × 2\n   year mean_danceability\n  &lt;dbl&gt;             &lt;dbl&gt;\n1  2020             0.673\n\n\nCode\n# This tells you the single year with the highest average danceability.\n\n# 3. Plot mean danceability by year as a line graph\nyear_dance |&gt;\n  ggplot(aes(x = year, y = mean_danceability)) +\n  geom_line(size = 1) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Trend Of Danceability by Release Year\",\n    x     = \"Release Year\",\n    y     = \"Danceability\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n\nWhich decade is most represented on user playlists? (The integer division (%/%) operator may be useful for computing decades from years.)\n\n\nCode\nlibrary(dplyr)\n\n# 1. Using the existing inner_joined_data (which has 'year'), compute each track’s decade\n# 2. Count how many playlist appearances come from each decade\n# 3. Find the decade with the highest count\n\nmost_represented_decade &lt;- inner_joined_data |&gt;\n  mutate(decade = (year %/% 10) * 10) |&gt;\n  count(decade, name = \"playlist_count\") |&gt;\n  arrange(desc(playlist_count)) |&gt;\n  slice_head(n = 5)\n\n# Show the result\nprint(most_represented_decade)\n\n\n# A tibble: 5 × 2\n  decade playlist_count\n   &lt;dbl&gt;          &lt;int&gt;\n1   2010          17022\n2   2000           5899\n3   1990           2582\n4   1980           1447\n5   1970           1431\n\n\nCreate a plot of key frequency among songs. Because musical keys exist in a ‘cycle’, your plot should use polar (circular) coordinates.\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# 1. Count how many songs fall into each key\nkey_counts &lt;- inner_joined_data |&gt;\n  count(key, name = \"freq\") |&gt;\n  arrange(freq)\n\n# 2. Make a circular bar chart of key frequencies\nggplot(key_counts, aes(x = factor(key), y = freq)) +\n  geom_col(fill = \"#FF6347\", width = 0.7) +         # tomato‐red bars\n  coord_polar(theta = \"x\", start = 0) +             # switch to polar coords\n  labs(\n    title = \"Frequency of Musical Keys in Joined Data\",\n    x     = \"Key (0 = C, …, 11 = B)\",\n    y     = \"Number of Songs\"\n  ) +\n  theme_light(base_size = 14) +                     # clean background\n  theme(\n    plot.title          = element_text(face = \"bold\", hjust = 0.5),\n    axis.text.x         = element_text(size = 12),\n    panel.grid.major.y  = element_line(color = \"grey80\"),\n    panel.grid.minor    = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nWhat are the most popular track lengths? (Are short tracks, long tracks, or something in between most commonly included in user playlists?)\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# 1. Bin track lengths into categories (in minutes)\nlength_bins &lt;- inner_joined_data |&gt;\n  mutate(\n    length_min = duration / 60000,\n    length_bin = case_when(\n      length_min &lt; 2 ~ \"&lt; 2 min\",\n      length_min &lt; 4 ~ \"2–4 min\",\n      length_min &lt; 6 ~ \"4–6 min\",\n      length_min &lt; 8 ~ \"6–8 min\",\n      TRUE           ~ \"8+ min\"\n    ),\n    # ensure the factor levels stay in logical order\n    length_bin = factor(\n      length_bin,\n      levels = c(\"&lt; 2 min\", \"2–4 min\", \"4–6 min\", \"6–8 min\", \"8+ min\")\n    )\n  )\n\n# 2. Count how often each length category appears in playlists\nlength_freq &lt;- length_bins |&gt;\n  count(length_bin, name = \"freq\") |&gt;\n  arrange(desc(freq))\n\nprint(length_freq)\n\n\n# A tibble: 5 × 2\n  length_bin  freq\n  &lt;fct&gt;      &lt;int&gt;\n1 2–4 min    17791\n2 4–6 min    10403\n3 6–8 min      557\n4 8+ min       225\n5 &lt; 2 min      207\n\n\nCode\n# A tibble showing which length bin is most common\n\n# 3. Plot the distribution as a bar chart\nlength_freq |&gt;\n  ggplot(aes(x = length_bin, y = freq)) +\n  geom_col(fill = \"#1ABC9C\", width = 0.7) +\n  labs(\n    title = \"Track Lengths in User Playlists\",\n    x     = \"Track Length\",\n    y     = \"Number of Appearances\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    axis.text.x  = element_text(size = 12),\n    axis.title   = element_text(size = 14),\n    plot.title   = element_text(size = 16, face = \"bold\", hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\nPose and visually answer at least two more other exploratory questions. Do higher‑energy songs tend to be more popular on Spotify?\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\ninner_joined_data |&gt;\n  # 1. Plot energy on the x-axis and popularity on the y-axis\n  ggplot(aes(x = energy, y = popularity)) +\n  \n  # 2. Add semi‑transparent points\n  geom_point(alpha = 0.4, size = 2) +\n  \n  # 3. Overlay a linear trend line (no shaded SE band)\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#E74C3C\") +\n  \n  # 4. Labels and clean theme\n  labs(\n    title = \"Spotify Popularity vs. Song Energy\",\n    x     = \"Energy (0–1)\",\n    y     = \"Spotify Popularity (0–100)\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nDo more acoustic songs tend to be more or less popular on Spotify?\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\ninner_joined_data |&gt;\n  # 1. Scatter acousticness vs. popularity\n  ggplot(aes(x = acousticness, y = popularity)) +\n  geom_point(alpha = 0.3, size = 1.5, color = \"#2C3E50\") +\n  \n  # 2. Add a linear‐fit line in red\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#E74C3C\") +\n  \n  # 3. Labels and clean theme\n  labs(\n    title = \"Spotify Popularity vs. Acousticness\",\n    x     = \"Acousticness (0–1)\",\n    y     = \"Popularity (0–100)\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    axis.title = element_text(size = 12)\n  )\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTask 6 What other songs commonly appear on playlists along side this song? I choise Fashion Killa by A$AP Rocky\n\n\nCode\nlibrary(dplyr)\n\n# 1. Get all playlist IDs that contain “Fashion Killa”\nanchor_playlists &lt;- inner_joined_data |&gt;\n  filter(track_name == \"Fashion Killa\") |&gt;\n  pull(playlist_id) |&gt;\n  unique()\n\n# 2. Within those playlists, count every other track *and* carry along its popularity\ncooccur_songs &lt;- inner_joined_data |&gt;\n  filter(\n    playlist_id %in% anchor_playlists,\n    track_name   != \"Fashion Killa\"\n  ) |&gt;\n  count(\n    track_name,\n    artist_name,\n    popularity,               # include popularity here\n    name = \"cooccurrence_count\"\n  ) |&gt;\n  arrange(desc(cooccurrence_count))\n\n# 3. Show the top 20\nhead(cooccur_songs, 20)\n\n\n# A tibble: 20 × 4\n   track_name                          artist_name popularity cooccurrence_count\n   &lt;chr&gt;                               &lt;chr&gt;            &lt;dbl&gt;              &lt;int&gt;\n 1 HUMBLE.                             Kendrick L…         83                 52\n 2 Closer                              The Chains…         84                 46\n 3 Congratulations                     Post Malone         83                 44\n 4 Bounce Back                         Big Sean            75                 39\n 5 Jumpman                             Drake               74                 39\n 6 iSpy (feat. Lil Yachty)             KYLE                77                 39\n 7 Bad and Boujee (feat. Lil Uzi Vert) Migos               76                 38\n 8 Mask Off                            Future              81                 38\n 9 XO TOUR Llif3                       Lil Uzi Ve…         84                 37\n10 White Iverson                       Post Malone         79                 36\n11 Caroline                            Aminé               76                 35\n12 goosebumps                          Travis Sco…         92                 35\n13 No Role Modelz                      J. Cole             82                 34\n14 Location                            Khalid              81                 33\n15 Trap Queen                          Fetty Wap           75                 33\n16 Work from Home                      Fifth Harm…         78                 33\n17 Ni**as In Paris                     JAY Z               61                 32\n18 Black Beatles                       Rae Sremmu…         75                 31\n19 No Problem (feat. Lil Wayne & 2 Ch… Chance The…         73                 31\n20 F**kin' Problems                    A$AP Rocky          76                 30\n\n\nWhat other songs are in the same key2 and have a similar tempo? (This makes it easy for a skilled DJ to transition from one song to the next.)\n\n\nCode\nlibrary(dplyr)\nlibrary(dplyr)\n\n# 1. Get Fashion Killa’s key and tempo\nfk_info &lt;- inner_joined_data |&gt;\n  filter(track_name == \"Fashion Killa\") |&gt;\n  slice_head(n = 1) |&gt;\n  select(fk_key = key, fk_tempo = tempo)\n\nfk_key   &lt;- fk_info$fk_key\nfk_tempo &lt;- fk_info$fk_tempo\n\n# 2. Find other tracks in that same key & within ±5 BPM, and include popularity\nsimilar_songs &lt;- inner_joined_data |&gt;\n  filter(\n    key == fk_key,\n    abs(tempo - fk_tempo) &lt;= 5,\n    track_name != \"Fashion Killa\"\n  ) |&gt;\n  distinct(\n    track_name,\n    artist_name,\n    album_name,\n    tempo,\n    popularity\n  ) |&gt;\n  arrange(abs(tempo - fk_tempo)) |&gt;\n  slice_head(n = 30)\n\n# 3. View them\nsimilar_songs\n\n\n# A tibble: 30 × 5\n   track_name                            artist_name album_name tempo popularity\n   &lt;chr&gt;                                 &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 See My Tears                          Machine Gu… Lace Up     140.         52\n 2 One I Want (feat. PARTYNEXTDOOR)      Majid Jord… The Space…  140.         59\n 3 guidance                              Travis Sco… Birds In …  140.         62\n 4 I'm a Thug                            Trick Daddy THUGS ARE…  140.         58\n 5 Spaceman - Carnage Festival Trap Rem… Hardwell    Spaceman    140.         53\n 6 F*ck Up Some Commas                   Future      DS2         140.         67\n 7 I Get The Bag (feat. Migos)           Gucci Mane  Mr. Davis   140.         73\n 8 Revenge                               XXXTENTACI… 17          140.         76\n 9 Work Hard, Play Hard                  Wiz Khalifa O.N.I.F.C.  140.         62\n10 One Day They'll Know (ODESZA Remix)   Pretty Lig… A Color M…  140.         56\n# ℹ 20 more rows\n\n\nWhat other songs were released by the same artist?\n\n\nCode\nlibrary(dplyr)\n\n# 1. Find the artist of your anchor song\nanchor_artist &lt;- inner_joined_data |&gt;\n  filter(track_name == \"Fashion Killa\") |&gt;\n  pull(artist_name) |&gt;\n  unique()\n\n# 2. Get all the other tracks by that same artist\nother_songs &lt;- inner_joined_data |&gt;\n  filter(\n    artist_name == anchor_artist,        # same artist\n    track_name   != \"Fashion Killa\"      # not the anchor itself\n  ) |&gt;\n  distinct(track_name, album_name) |&gt;    # one row per song\n  arrange(track_name)                    # sort alphabetically\n\n# 3. View the result\nother_songs\n\n\n# A tibble: 26 × 2\n   track_name       album_name                     \n   &lt;chr&gt;            &lt;chr&gt;                          \n 1 1Train           LONG.LIVE.A$AP (Deluxe Version)\n 2 Angels           LONG.LIVE.A$AP (Deluxe Version)\n 3 Better Things    AT.LONG.LAST.A$AP              \n 4 Canal St.        AT.LONG.LAST.A$AP              \n 5 Electric Body    AT.LONG.LAST.A$AP              \n 6 Everyday         AT.LONG.LAST.A$AP              \n 7 Excuse Me        AT.LONG.LAST.A$AP              \n 8 F**kin' Problems LONG.LIVE.A$AP (Deluxe Version)\n 9 Ghetto Symphony  LONG.LIVE.A$AP (Deluxe Version)\n10 Goldie           LONG.LIVE.A$AP (Deluxe Version)\n# ℹ 16 more rows\n\n\nWhat other songs were released in the same year and have similar levels of acousticness, danceability, etc.?\n\n\nCode\nlibrary(dplyr)\n\n# 1. Get Fashion Killa’s stats once\nanchor &lt;- inner_joined_data |&gt;\n  filter(track_name == \"Fashion Killa\") |&gt;\n  slice_head(n = 1)\n\nyear0 &lt;- anchor$year\nac0   &lt;- anchor$acousticness\nda0   &lt;- anchor$danceability\nen0   &lt;- anchor$energy\nli0   &lt;- anchor$liveness\n\n# 2. Pull all OTHER tracks from that same year, dedupe\ncandidates &lt;- inner_joined_data |&gt;\n  filter(\n    year      == year0,\n    track_name != \"Fashion Killa\"\n  ) |&gt;\n  distinct(track_id, .keep_all = TRUE)\n\n# 3. Compute similarity and pick top 10, selecting song title and popularity\nrecommendations &lt;- candidates |&gt;\n  mutate(\n    diff = abs(acousticness - ac0) +\n           abs(danceability - da0) +\n           abs(energy      - en0) +\n           abs(liveness    - li0)\n  ) |&gt;\n  arrange(diff) |&gt;\n  select(\n    track_name,     # song title\n    artist_name,\n    popularity,     # include popularity now\n    acousticness,\n    danceability,\n    energy,\n    liveness,\n    diff\n  ) |&gt;\n  slice_head(n = 30)\n\nprint(recommendations)\n\n\n# A tibble: 30 × 8\n   track_name   artist_name popularity acousticness danceability energy liveness\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 Goldie       A$AP Rocky          66        0.225        0.697  0.848    0.493\n 2 El Inmigran… Calibre 50          50        0.403        0.797  0.758    0.623\n 3 Love Game    Eminem              55        0.359        0.766  0.941    0.585\n 4 La Hummer Y… Voz De Man…         63        0.287        0.788  0.785    0.259\n 5 Tunnel Visi… Justin Tim…         52        0.275        0.572  0.771    0.473\n 6 Guap         Big Sean            54        0.311        0.65   0.723    0.412\n 7 Freak Hoe    Speaker Kn…         54        0.1          0.794  0.871    0.384\n 8 Better Than… One Direct…         64        0.114        0.627  0.839    0.534\n 9 You're Not … Blood Oran…         60        0.189        0.74   0.798    0.305\n10 Daydreamin'  Ariana Gra…         49        0.233        0.675  0.643    0.561\n# ℹ 20 more rows\n# ℹ 1 more variable: diff &lt;dbl&gt;\n\n\nWhat other songs were released by singers with “A$AP” in their name?\n\n\nCode\n# 1. Filter for artists with \"A$AP\" in their name (excluding Fashion Killa)\n# 2. Select the song info + popularity\n# 3. Remove duplicates and sort by popularity descending\n\nasap_songs &lt;- inner_joined_data |&gt;\n  filter(\n    str_detect(artist_name, \"A\\\\$AP\"),     # keep only A$AP artists\n    track_name != \"Fashion Killa\"          # drop the anchor track\n  ) |&gt;\n  distinct(\n    artist_name,\n    track_name,\n    album_name,\n    popularity                              # include popularity now\n  ) |&gt;\n  arrange(desc(popularity))                # most popular first\n\n# 4. View the result\nasap_songs\n\n\n# A tibble: 38 × 4\n   artist_name track_name                         album_name          popularity\n   &lt;chr&gt;       &lt;chr&gt;                              &lt;chr&gt;                    &lt;dbl&gt;\n 1 A$AP Rocky  F**kin' Problems                   LONG.LIVE.A$AP (De…         76\n 2 A$AP Rocky  L$D                                AT.LONG.LAST.A$AP           76\n 3 A$AP Ferg   Plain Jane                         Still Striving              76\n 4 A$AP Rocky  Everyday                           AT.LONG.LAST.A$AP           73\n 5 A$AP Ferg   Work REMIX                         Trap Lord                   70\n 6 A$AP Mob    Yamborghini High                   Yamborghini High            69\n 7 A$AP Rocky  Wild for the Night                 LONG.LIVE.A$AP (De…         68\n 8 A$AP Rocky  Lord Pretty Flacko Jodye 2 (LPFJ2) AT.LONG.LAST.A$AP           68\n 9 A$AP Rocky  Goldie                             LONG.LIVE.A$AP (De…         66\n10 A$AP Rocky  Canal St.                          AT.LONG.LAST.A$AP           66\n# ℹ 28 more rows\n\n\nTask 7 Ultimate Playlist"
  },
  {
    "objectID": "mp03.html#task-5-visually-identifying-characteristics-of-popular-songs",
    "href": "mp03.html#task-5-visually-identifying-characteristics-of-popular-songs",
    "title": "The Ultimate Playlist",
    "section": "",
    "text": "Inner-joiing both the song characteristics & playlist information datasets. Dataframe will be called inner_joined_data.\n\n\nCode\ninner_joined_data &lt;- songs_df |&gt;\n  # rename the song‐data 'id' column so it matches\n  rename(track_id = id) |&gt;\n  # now do the join by track_id\n  inner_join(playlist_df, by = \"track_id\")\n\nhead(inner_joined_data)\n\n\n# A tibble: 6 × 29\n  track_id             name  artists duration_ms release_date  year acousticness\n  &lt;chr&gt;                &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 1J5kO1Iyuo9GUkKpVOL… \"Mis… ['Sonn…      564320 1957          1957        0.766\n2 5F0vhf0bfnmtMW2aWsx… \"Unc… ['Sonn…      235400 1965          1965        0.282\n3 5jOO4HOAqTR9KC3HfHG… \"Let… ['Jeff…      214427 8/15/66       1966        0.161\n4 6QmMCtfu6uHG1TbWHe0… \"Rai… ['John…      310800 1970          1970        0.806\n5 5FnlmowF3elDo9Psgw1… \"Old… ['Aret…      220201 1972          1972        0.365\n6 43l4wsDQHRa0lZwwuzS… \"Nev… ['Bob …      171773 1/17/74       1974        0.583\n# ℹ 22 more variables: danceability &lt;dbl&gt;, energy &lt;dbl&gt;,\n#   instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, loudness &lt;dbl&gt;, speechiness &lt;dbl&gt;,\n#   tempo &lt;dbl&gt;, valence &lt;dbl&gt;, mode &lt;dbl&gt;, key &lt;dbl&gt;, popularity &lt;dbl&gt;,\n#   explicit &lt;dbl&gt;, playlist_name &lt;chr&gt;, playlist_id &lt;chr&gt;,\n#   playlist_position &lt;int&gt;, playlist_followers &lt;int&gt;, artist_name &lt;chr&gt;,\n#   artist_id &lt;chr&gt;, track_name &lt;chr&gt;, album_name &lt;chr&gt;, album_id &lt;chr&gt;,\n#   duration &lt;int&gt;\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\n\ntrack_stats &lt;- inner_joined_data |&gt;\n  count(track_id, popularity, name = \"play_count\")\n\n#Pearson correlation between play count and popularity\ncorr_coef &lt;- cor(track_stats$play_count, track_stats$popularity)\nprint(paste(\"Pearson correlation:\", round(corr_coef, 3)))\n\n\n[1] \"Pearson correlation: 0.488\"\n\n\nCode\n#Scatterplot\ntrack_stats |&gt;\n  ggplot(aes(x = play_count, y = popularity)) +\n  geom_point(alpha = 0.4, size = 2, color = \"#2C3E50\") +\n  geom_smooth(method = \"lm\", color = \"#E74C3C\", se = FALSE) +\n  scale_x_log10() +\n  labs(\n    title    = \"Spotify Popularity vs. # of Playlist Appearances\",\n    subtitle = paste0(\"Pearson r = \", round(corr_coef, 2)),\n    x        = \"Playlist Appearances\",\n    y        = \"Spotify Popularity \"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCode\npop_cutoff &lt;- 70\n\n\nWe conducted a Pearson correlation analysis to examine whether a song’s popularity score is related to its playlist position. The resulting correlation coefficient of 0.49 indicates a moderate positive relationship: in general, more‑popular songs tend to appear earlier in playlists. However, the correlation is far from perfect, so highly popular tracks can still show up infrequently—or later—on certain playlists.\n\n\n\n\n\nCode\nyear_popularity &lt;- inner_joined_data |&gt;\n  filter(year &gt;= 1950) |&gt;\n  group_by(year) |&gt;\n  summarise(\n    median_popularity = mean(popularity),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(median_popularity))\n\nyear_popularity |&gt;\n  slice_head(n = 1)\n\n\n# A tibble: 1 × 2\n   year median_popularity\n  &lt;dbl&gt;             &lt;dbl&gt;\n1  2017              71.3\n\n\nCode\n# e.g. year == 2019\n\n# 3. Plot (ticks every 3 years on the x‑axis)\nyear_popularity |&gt;\n  ggplot(aes(x = year, y = median_popularity)) +\n  geom_point(size = 3, color = \"#2C3E50\") +\n  geom_line(color = \"#E74C3C\", size = 1) +\n  scale_x_continuous(\n    breaks = seq(1950, max(year_popularity$year), by = 3)\n  ) +\n  labs(\n    title = \"Mean Spotify Popularity by Release Year (1950+)\",\n    x     = \"Release Year\",\n    y     = \"Mean Popularity (0–100)\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nre f"
  }
]